{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook loads the data from trajectory_analysis_script.py for whichever data set is needed. The point is to look at the density of the tracks/localizations per area of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_t/qlzp5l894v16vmr8m_81x5x80000gn/T/ipykernel_84989/734256252.py:34: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg')\n"
     ]
    }
   ],
   "source": [
    "from src.SMT_Analysis_BP.databases.trajectory_analysis_script import *\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.colors\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "from scipy.stats import gaussian_kde\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from src.SMT_Analysis_BP.helpers.plotting_functions import *\n",
    "from src.SMT_Analysis_BP.helpers.import_functions import *\n",
    "from src.SMT_Analysis_BP.helpers.Analysis_functions import *\n",
    "from src.SMT_Analysis_BP.helpers.diff_mw import *\n",
    "import mpl_toolkits.mplot3d.art3d as art3d\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import matplotlib as mpl\n",
    "from src.SMT_Analysis_BP.helpers.scalebars import *\n",
    "from src.SMT_Analysis_BP.helpers.Convert_csv_mat import *\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from scipy import stats, ndimage,io\n",
    "import csv  \n",
    "from sklearn.cluster import OPTICS\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')  \n",
    "from sklearn.cluster import DBSCAN\n",
    "from cgitb import small\n",
    "import src.SMT_Analysis_BP.helpers.smallestenclosingcircle as smallestenclosingcircle\n",
    "import math\n",
    "import src.SMT_Analysis_BP.helpers.guassian_fit as gaussian_fit\n",
    "import src.SMT_Analysis_BP.helpers.fbm_utility as fbm_utility\n",
    "import src.SMT_Analysis_BP.helpers.simulate_foci as simulate_foci\n",
    "import src.SMT_Analysis_BP.helpers.simulate_cells as simulate_cells\n",
    "import src.SMT_Analysis_BP.helpers.SMT_converters as smt\n",
    "import matplotlib.patches as mpatches\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals = {\"olympus_pixel_size\":130,\"confocal_pixel_size\":79}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore',module=\"DeprecationWarning\")\n",
    "#ignore deprecated warnings\n",
    "#ignore stacklevel=2 warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Label Track index Track ID Number of spots in track Number of gaps  \\\n",
      "0        Label       Index       ID                  N spots         N gaps   \n",
      "1          NaN         NaN      NaN                      NaN            NaN   \n",
      "2      Track_0           0        0                       11              0   \n",
      "3      Track_1           1        1                       14              0   \n",
      "4      Track_2           2        2                        8              0   \n",
      "..         ...         ...      ...                      ...            ...   \n",
      "434  Track_432         432      432                        3              0   \n",
      "435  Track_433         433      433                        2              0   \n",
      "436  Track_434         434      434                        2              0   \n",
      "437  Track_435         435      435                        2              0   \n",
      "438  Track_436         436      436                        2              0   \n",
      "\n",
      "    Number of split events Number of merge events Number of complex points  \\\n",
      "0                 N splits               N merges                N complex   \n",
      "1                      NaN                    NaN                      NaN   \n",
      "2                        0                      0                        0   \n",
      "3                        0                      0                        0   \n",
      "4                        0                      0                        0   \n",
      "..                     ...                    ...                      ...   \n",
      "434                      0                      0                        0   \n",
      "435                      0                      0                        0   \n",
      "436                      0                      0                        0   \n",
      "437                      0                      0                        0   \n",
      "438                      0                      0                        0   \n",
      "\n",
      "    Longest gap Track duration  ...      Track min speed   Track median speed  \\\n",
      "0      Lgst gap       Duration  ...            Min speed           Med. speed   \n",
      "1           NaN        (frame)  ...        (pixel/frame)        (pixel/frame)   \n",
      "2             0           10.0  ...  0.29629444547142475   0.6409386521054724   \n",
      "3             0           13.0  ...  0.03442074198558806  0.19179080884192168   \n",
      "4             0            7.0  ...   0.2572520526217642   0.9657161472913045   \n",
      "..          ...            ...  ...                  ...                  ...   \n",
      "434           0            2.0  ...    0.563414847436883   0.9528033098626754   \n",
      "435           0            1.0  ...  0.13467385387969746  0.13467385387969746   \n",
      "436           0            1.0  ...  0.15232274356174777  0.15232274356174777   \n",
      "437           0            1.0  ...   0.6473665845140782   0.6473665845140782   \n",
      "438           0            1.0  ...  0.44201518952863317  0.44201518952863317   \n",
      "\n",
      "         Track std speed  Track mean quality Total distance traveled  \\\n",
      "0              Std speed              Mean Q             Total dist.   \n",
      "1          (pixel/frame)           (quality)                 (pixel)   \n",
      "2     0.2504983162627309    84.0211559642445       6.124387005787577   \n",
      "3     0.1875739929754913   86.02050045558384       3.285142892790709   \n",
      "4     0.3449050331602079   64.12969493865967       6.538358714508822   \n",
      "..                   ...                 ...                     ...   \n",
      "434  0.27533922229708097   58.29638799031576      1.5162181572995583   \n",
      "435                  NaN      57.37060546875     0.13467385387969746   \n",
      "436                  NaN   46.75994873046875     0.15232274356174777   \n",
      "437                  NaN  43.537147521972656      0.6473665845140782   \n",
      "438                  NaN   48.93546676635742     0.44201518952863317   \n",
      "\n",
      "    Max distance traveled    Confinement ratio Mean straight line speed  \\\n",
      "0               Max dist.           Cfn. ratio              Mn. v. line   \n",
      "1                 (pixel)                  NaN            (pixel/frame)   \n",
      "2      0.8821373248975675  0.03901318387774616        0.023893183639527   \n",
      "3     0.48292730895977926   0.0837756350825501     0.021170379398497495   \n",
      "4      1.3835121345232053  0.21159930112937195      0.19764459064617218   \n",
      "..                    ...                  ...                      ...   \n",
      "434    0.9528033098626754   0.2577425884104858      0.19539699622868265   \n",
      "435   0.13467385387969746                  1.0      0.13467385387969746   \n",
      "436   0.15232274356174777                  1.0      0.15232274356174777   \n",
      "437    0.6473665845140782                  1.0       0.6473665845140782   \n",
      "438   0.44201518952863317                  1.0      0.44201518952863317   \n",
      "\n",
      "    Linearity of forward progression Mean directional change rate  \n",
      "0                        Fwd. progr.                   Mn. Î³ rate  \n",
      "1                                NaN                  (rad/frame)  \n",
      "2                0.03901318387774615           2.3997415239413815  \n",
      "3                0.08377563508255009           1.8494508706708614  \n",
      "4                0.21159930112937192           2.4486102185065777  \n",
      "..                               ...                          ...  \n",
      "434               0.2577425884104858            3.096392558033099  \n",
      "435                              1.0                          NaN  \n",
      "436                              1.0                          NaN  \n",
      "437                              1.0                          NaN  \n",
      "438                              1.0                          NaN  \n",
      "\n",
      "[439 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "#import this .csv file as a pandas dataframe\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/ll_ez/Analysis_lowpsf/tracks.csv',header=1)\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the integrated intensities of the spots found in experimental conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random testing\n",
    "paths = ['/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/Analysis',\n",
    "         '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/ll_ez/Analysis',\n",
    "         '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/ll_m9/Analysis',\n",
    "         '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/15/rp_ez_hex5/Analysis']\n",
    "names = ['rpoc_ez','ll_ez','ll_m9','rp_ez_hex5']\n",
    "num_points_path = []\n",
    "integrated_intensity_path = []\n",
    "\n",
    "for path in paths:\n",
    "    #find all files in path that end with spots.csv\n",
    "    files = [f for f in os.listdir(path) if f.endswith('spots.csv')]\n",
    "    #read the files with loadtext\n",
    "    data = [np.loadtxt(os.path.join(path,f),delimiter=',',skiprows=1) for f in files]\n",
    "    num_points = np.zeros(len(data))\n",
    "    integrated_intensity = []\n",
    "    for i in range(len(data)):\n",
    "        num_points[i] = len(data[i])\n",
    "        integrated_intensity.append(data[i][:,4])\n",
    "    num_points_path.append(num_points)\n",
    "    integrated_intensity_all = np.concatenate(integrated_intensity)\n",
    "    integrated_intensity_path.append(integrated_intensity_all)\n",
    "#plot the integrated intensity\n",
    "for i in range(len(integrated_intensity_path)):\n",
    "    plt.hist(integrated_intensity_path[i],bins=50,alpha = 0.5,density=True,label=names[i])\n",
    "plt.xlim(4000,9000)\n",
    "plt.xlabel('Integrated Intensity')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('Integrated Intensity of Spots')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#plot the CDF of the integrated intensity\n",
    "for i in range(len(integrated_intensity_path)):\n",
    "    plt.hist(integrated_intensity_path[i],bins=50,alpha = 0.5,density=True,label=names[i],cumulative=True,histtype='step')\n",
    "plt.xlim(4000,9000)\n",
    "plt.xlabel('Integrated Intensity')\n",
    "plt.ylabel('Cumulative Probability Density')\n",
    "plt.title('Integrated Intensity of Spots CDF')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#do the KS test on the first 2 paths and print out which names are being compared\n",
    "print(\"KS test between {} and {}: {}\".format(names[0],names[1],stats.ks_2samp(integrated_intensity_path[0],integrated_intensity_path[1],method='asymp')))\n",
    "print(\"KS test between {} and {}: {}\".format(names[0],names[2],stats.ks_2samp(integrated_intensity_path[0],integrated_intensity_path[2],method='asymp')))\n",
    "print(\"KS test between {} and {}: {}\".format(names[1],names[2],stats.ks_2samp(integrated_intensity_path[1],integrated_intensity_path[2])))\n",
    "print(\"KS test between {} and {}: {}\".format(names[0],names[3],stats.ks_2samp(integrated_intensity_path[0],integrated_intensity_path[3])))\n",
    "print(\"KS test between {} and {}: {}\".format(names[1],names[3],stats.ks_2samp(integrated_intensity_path[1],integrated_intensity_path[3])))\n",
    "print(\"KS test between {} and {}: {}\".format(names[2],names[3],stats.ks_2samp(integrated_intensity_path[2],integrated_intensity_path[3])))\n",
    "\n",
    "#compare the rpoc_ez to itself\n",
    "print(\"KS test between {} and {}: {}\".format(names[0],names[0],stats.ks_2samp(integrated_intensity_path[0],integrated_intensity_path[0])))\n",
    "#print the length of the data\n",
    "print(\"Length of rpoc_ez: {}\".format(len(integrated_intensity_path[0])))\n",
    "print(\"Length of ll_ez: {}\".format(len(integrated_intensity_path[1])))\n",
    "print(\"Length of ll_m9: {}\".format(len(integrated_intensity_path[2])))\n",
    "print(\"Length of rp_ez_hex5: {}\".format(len(integrated_intensity_path[3])))\n",
    "\n",
    "\n",
    "#test the poisition dependence \n",
    "\n",
    "###conclustion is that the integrated intensities are the same for all 3 paths. So the intensity of each single localization looks identical to others. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DO the intensity check on simulated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"/Users/baljyot/Documents/2022-2023/PhD_Thesis/sim_data/h_0.2_track_200_diff_001_at_100100/Analysis/\",\n",
    "         \"/Users/baljyot/Documents/2022-2023/PhD_Thesis/sim_data/h_0.35_track_200_diff_001_random/Analysis/\",\n",
    "         \"/Users/baljyot/Documents/2022-2023/PhD_Thesis/sim_data/h_0.35_track_200_diff_1_random/Analysis/\",\n",
    "         \"/Users/baljyot/Documents/2022-2023/PhD_Thesis/sim_data/h_0.2_track_100_diff_01_random/Analysis/\",\n",
    "         \"/Users/baljyot/Documents/2022-2023/PhD_Thesis/sim_data/testing/Analysis/\"\n",
    "         ]\n",
    "names = ['h_0.2_track_200_diff_001_at_100100','h_0.35_track_200_diff_001_random','h_0.35_track_200_diff_1_random','h_0.2_track_100_diff_01_random','testing']\n",
    "num_points_path = []\n",
    "integrated_intensity_path = []\n",
    "\n",
    "for path in paths:\n",
    "    #find all files in path that end with spots.csv\n",
    "    files = [f for f in os.listdir(path) if f.endswith('spots.csv')]\n",
    "    #read the files with pandas\n",
    "    data = [pd.read_csv(os.path.join(path,f),delimiter=',') for f in files]\n",
    "    num_points_path.append([len(each[\"TOTAL_INTENSITY_CH1\"][3:]) for each in data])\n",
    "    integrated_intensity_path.append([each[\"TOTAL_INTENSITY_CH1\"][3:] for each in data])\n",
    "#plot the integrated intensity\n",
    "for i in range(len(integrated_intensity_path)):\n",
    "    plt.hist(np.array(integrated_intensity_path[i][0],dtype=float),bins=20,alpha = 0.5,density=True,label=names[i])\n",
    "#plt.xlim(4000,9000)\n",
    "plt.xlabel('Integrated Intensity')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('Integrated Intensity of Spots')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#plot the CDF of the integrated intensity\n",
    "for i in range(len(integrated_intensity_path)):\n",
    "    plt.hist(np.array(integrated_intensity_path[i][0],dtype=float),bins=20,alpha = 0.5,density=True,label=names[i],cumulative=True,histtype='step')\n",
    "#plt.xlim(4000,9000)\n",
    "plt.xlabel('Integrated Intensity')\n",
    "plt.ylabel('Cumulative Probability Density')\n",
    "plt.title('Integrated Intensity of Spots CDF')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"/Users/baljyot/Desktop/Steph_Bead_Data/tracked_data/\"\n",
    "         ]\n",
    "names = [\"Steph's Beads\"]\n",
    "num_points_path = []\n",
    "integrated_intensity_path = []\n",
    "\n",
    "for path in paths:\n",
    "    #find all files in path that end with spots.csv\n",
    "    files = [f for f in os.listdir(path) if f.endswith('spots.csv')]\n",
    "    #read the files with pandas\n",
    "    data = [pd.read_csv(os.path.join(path,f),delimiter=',') for f in files]\n",
    "    num_points_path.append([len(each[\"TOTAL_INTENSITY_CH1\"][3:]) for each in data])\n",
    "    integrated_intensity_path.append([each[\"TOTAL_INTENSITY_CH1\"][3:] for each in data])\n",
    "#plot the integrated intensity\n",
    "for i in range(len(integrated_intensity_path)):\n",
    "    plt.hist(np.array(integrated_intensity_path[i][0],dtype=float),bins=20,alpha = 0.5,density=True,label=names[i])\n",
    "#plt.xlim(4000,9000)\n",
    "plt.xlabel('Integrated Intensity')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('Integrated Intensity of Spots')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#plot the CDF of the integrated intensity\n",
    "for i in range(len(integrated_intensity_path)):\n",
    "    plt.hist(np.array(integrated_intensity_path[i][0],dtype=float),bins=20,alpha = 0.5,density=True,label=names[i],cumulative=True,histtype='step')\n",
    "#plt.xlim(4000,9000)\n",
    "plt.xlabel('Integrated Intensity')\n",
    "plt.ylabel('Cumulative Probability Density')\n",
    "plt.title('Integrated Intensity of Spots CDF')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following makes a \"true\" simulation of tracks and then subsamples them to determine how many subsamples one needs to recapture the total set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = 500\n",
    "\n",
    "cell_parms = {\n",
    "    \"diffusion_coefficients\":np.array(list(0.1*np.ones(100)) + list(1*np.ones(300)) + list(0.01*np.ones(100))),\n",
    "    \"initials\":np.random.uniform(100,130,(tracks,2)),\n",
    "    \"num_tracks\":tracks,\n",
    "    \"track_type\":\"fbm\",\n",
    "    \"hursts\":np.array(list(0.2*np.ones(100)) + list(0.35*np.ones(300)) + list(0.35*np.ones(100))),\n",
    "    \"dims\":(200,200),\n",
    "    \"movie_frames\":500\n",
    "}      \n",
    "\n",
    "#sim_params\n",
    "sim_params = {\n",
    "    \"track_length_mean\":15,\n",
    "    \"track_distribution\":\"exponential\",\n",
    "    \"exposure_time\":20,#ms same as frame_time\n",
    "    \"base_noise\":140,\n",
    "    \"point_intensity\":20,\n",
    "    \"psf_sigma\":0.82,\n",
    "    \"frame_time\":20,#ms\n",
    "    \"pixel_size\":globals[\"olympus_pixel_size\"],\n",
    "    \"axial_function\":\"ones\", #\"ones\" or \"exponential\"\n",
    "}\n",
    "samples = [20,40,60,80,100,120,140,160,180,200,300,400]\n",
    "cell_sim = simulate_cells.Simulate_cells(cell_parms,sim_params)\n",
    "samples,full_sim = cell_sim.sample_cells(subsample_sizes = samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data for the full simulation\n",
    "\n",
    "simulate_cells.make_directory_structure(\"/Users/baljyot/Documents/2022-2023/PhD_Thesis/sim_data/subsampling_sim/h_0.2(100).0.35(300).0.35(100)_track_500_diff_1(300).1(100).01(100)_random(100_130)/\",\"test_seg\",full_sim[\"map\"],\"mean\",1,\n",
    "                                        data=full_sim,parameters={\"cell_parms\":cell_parms,\"sim_params\":sim_params})\n",
    "sample_lengths = [20,40,60,80,100,120,140,160,180,200,300,400]\n",
    "#for each sample size, save the data\n",
    "for i in range(len(samples)):\n",
    "    simulate_cells.make_directory_structure(\"/Users/baljyot/Documents/2022-2023/PhD_Thesis/sim_data/subsampling_sim/h_0.2(100).0.35(300).0.35(100)_track_500_diff_1(300).1(100).01(100)_random(100_130)/subsamples/{0}_samples/\".format(sample_lengths[i]),\"test_seg\",samples[i][\"map\"],\"mean\",1,\n",
    "                                        data=samples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_dir = \"/Users/baljyot/Documents/2022-2023/PhD_Thesis/sim_data/subsampling_sim/h_0.2(150).0.35(350)_track_500_diff_1(350).1(150)_random(100_130)\"\n",
    "#for the sampling sim find read the data for the full simulation\n",
    "full_sim_tracks = open_pickle_data(base_dir)\n",
    "#for each sample size, load the true data\n",
    "#in the directory find the directory labeled \"subsamples\" which stores the data for each sample size with names like \"20_samples\"\n",
    "#use the open_pickle_data function to load the data for each sample size\n",
    "\n",
    "sample_files = os.listdir(base_dir+\"/subsamples/\")\n",
    "#remove the .DS_Store file\n",
    "sample_files.remove(\".DS_Store\")\n",
    "\n",
    "#add these names to the path to get the full path to the data\n",
    "sample_paths = [base_dir +\"/subsamples/{0}\".format(i) for i in sample_files]\n",
    "#load the data for each sample size\n",
    "sample_data_tracks = [open_pickle_data(i) for i in sample_paths]\n",
    "#now read the extracted data for each sample size\n",
    "sample_data = [read_data(i)[\"ALL\"] for i in sample_paths]\n",
    "#do msd_calc on the full simulation data\n",
    "full_sim_msd = msd_calc(full_sim_tracks,plot=False,save=False)\n",
    "#repeat for each sample size\n",
    "sample_data_msd = [msd_calc(i,plot=False,save=False) for i in sample_data]\n",
    "\n",
    "sample_data_track_msd = [msd_calc(i,plot=False,save=False) for i in sample_data_tracks]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the histograms of the msd for the full_sim and each sample size\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist(np.log10(np.array(list(full_sim_msd[\"track_diffusion\"].values()))*(1000./20.)*(130**2)*1e-6),bins=20,alpha=0.5,density=True)\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_xlabel(\"log10(diffusion coefficient (um^2/s))\")\n",
    "ax.set_title(\"Diffusion Coefficient for full simulation \\n (h=0.5,track=500 (150 for low, 350 for high)\")\n",
    "#make all the plots have the same x axis limits\n",
    "ax.set_xlim(-3,1)\n",
    "plt.show()\n",
    "\n",
    "#plot the histograms of the msd of each sample size\n",
    "for i in range(len(sample_data_msd)):\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.hist(np.log10(np.array(list(sample_data_msd[i][\"track_diffusion\"].values()))*(1000./20.)*(130**2)*1e-6),bins=20,alpha=0.5,density=True,label = \"extracted\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.set_xlabel(\"log10(diffusion coefficient (um^2/s))\")\n",
    "    ax.set_title(\"Histogram of diffusion coefficient for sample size {0}\".format(len(sample_data_msd[i][\"tavg_t1_msd\"].keys())))\n",
    "    #make all the plots have the same x axis limits\n",
    "    ax.set_xlim(-3,1)\n",
    "    #plt.show()\n",
    "\n",
    "    ax.hist(np.log10(np.array(list(sample_data_track_msd[i][\"track_diffusion\"].values()))*(1000./20.)*(130**2)*1e-6),bins=20,alpha=0.5,density=True,label = \"true\")\n",
    "    #ax.set_ylabel(\"Density\")\n",
    "    #ax.set_xlabel(\"log10(diffusion coefficient (um^2/s))\")\n",
    "    #ax.set_title(\"True Histogram of diffusion coefficient for sample size {0}\".format(len(sample_data_track_msd[i][\"tavg_t1_msd\"].keys())))\n",
    "    #make all the plots have the same x axis limits\n",
    "    #ax.set_xlim(-3,1)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#plot the cdf of the msd for the full simulation and each sample size as a line plot\n",
    "fig,ax = plt.subplots()\n",
    "for i in sample_data_msd:\n",
    "    ax.plot(np.sort(np.log10(np.array(list(i[\"track_diffusion\"].values()))*(1000./20.)*(130**2)*1e-6)),np.linspace(0,1,len(i[\"track_diffusion\"].keys()))*(1000./20.)*(130**2)*1e-6,label=\"sample size {0}\".format(len(i[\"track_diffusion\"].keys())))\n",
    "ax.plot(np.sort(np.log10(np.array(list(full_sim_msd[\"track_diffusion\"].values()))*(1000./20.)*(130**2)*1e-6)),np.linspace(0,1,len(full_sim_msd[\"track_diffusion\"].keys()))*(1000./20.)*(130**2)*1e-6,label=\"full sim\")\n",
    "ax.set_ylabel(\"CDF\")\n",
    "ax.set_xlabel(\"log10(Diffusion Coefficient (um^2/s))\")\n",
    "ax.set_title(\"CDF of diffusion coefficient \\n (h=0.5,track=500 (150 for low, 350 for high)\")\n",
    "#sort the legend by sample size (ascending)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "labels, handles = zip(*sorted(zip(labels, handles), key=lambda t: t[-3:]))\n",
    "ax.legend(handles, labels)\n",
    "plt.show()\n",
    "\n",
    "#for each sample find the KS test statistic and p value when comparing to the full simulation\n",
    "ks_stats = []\n",
    "for i in sample_data_msd:\n",
    "    ks_stats.append(stats.ks_2samp(list(i[\"track_diffusion\"].values()),list(full_sim_msd[\"track_diffusion\"].values())))\n",
    "#plot the KS test statistic and p value for each sample size\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter([len(i[\"track_diffusion\"].keys()) for i in sample_data_msd],np.array(ks_stats)[:,1],label=\"p value\")\n",
    "ax.set_ylabel(\"p value\")\n",
    "ax.set_xlabel(\"sample size\")\n",
    "ax.set_title(\"KS test p value for each sample size \\n (h=0.5,track=500 (150 for low, 350 for high)\")\n",
    "#plot a horizontal line at the p value of 0.05\n",
    "ax.axhline(y=0.05, color='r', linestyle='--')\n",
    "#make the ys axis log scale\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the histograms of the msd for the full_sim and each sample size\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist(list(full_sim_msd[\"track_alpha\"].values()),bins=50,alpha=0.5,density=True)\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_xlabel(\"alpha per track\")\n",
    "ax.set_title(\"Alphas for full simulation \\n (h=0.5,track=500 (150 for low, 350 for high)\")\n",
    "#make all the plots have the same x axis limits\n",
    "ax.set_xlim(-2,2)\n",
    "plt.show()\n",
    "\n",
    "#plot the histograms of the msd of each sample size\n",
    "for i in sample_data_msd:\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.hist(np.array(list(i[\"track_alpha\"].values())),bins=50,alpha=0.5,density=True)\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.set_xlabel(\"alpha per track\")\n",
    "    ax.set_title(\"Histogram of alphas for sample size {0}\".format(len(i[\"tavg_t1_msd\"].keys())))\n",
    "    #make all the plots have the same x axis limits\n",
    "    ax.set_xlim(-2,2)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_x = 175\n",
    "min_x = 25\n",
    "max_y = 175\n",
    "min_y = 25\n",
    "\n",
    "#find combinations of [x,y] values that are unique\n",
    "x = np.arange(min_x,max_x+1)\n",
    "y = np.arange(min_y,max_y+1)\n",
    "x,y = np.meshgrid(x,y)\n",
    "x = x.flatten()\n",
    "y = y.flatten()\n",
    "xy = np.array([x,y]).T\n",
    "xy = np.unique(xy,axis=0)\n",
    "\n",
    "#randomly select 100 of these combinations\n",
    "xy = xy[np.random.choice(len(xy),300,replace=False)]\n",
    "#make sure xy is an array\n",
    "xy = np.array(xy)\n",
    "#make the x and y values floats\n",
    "xy = xy.astype(float)\n",
    "print(xy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the cell creating with the condensate definitions for initials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_dict = {\n",
    "    \"initial_centers\":np.array([[100,100]]),\n",
    "    \"initial_scale\": np.array([2.5]),\n",
    "    \"diffusion_coefficient\": np.array([1e-4]),\n",
    "    \"hurst_exponent\": np.array([0.2])  \n",
    "}\n",
    "cell_parms = {\n",
    "    \"diffusion_coefficients\":np.array(list(0.0*np.ones(100))),# + list(0.1*np.ones(40)) + list(0.5*np.ones(20)) + list(1*np.ones(15))),\n",
    "    \"initials\": initial_dict,\n",
    "    \"num_tracks\":100,\n",
    "    \"track_type\":\"fbm\",\n",
    "    \"hursts\":np.array(list(0.2*np.ones(100))),# + list(0.2*np.ones(40)) + list(0.35*np.ones(20)) + list(0.35*np.ones(15))),\n",
    "    \"dims\":(200,200),\n",
    "    \"movie_frames\":500,\n",
    "    \"cell_space\":np.array([50,120,50,102])#np.array([80,120,90,102])\n",
    "}     \n",
    "global_sim_params = {\n",
    "    \"track_length_mean\":10,\n",
    "    \"track_distribution\":\"constant\",\n",
    "    \"exposure_time\":20,#ms same as frame_time\n",
    "    \"base_noise\":140,\n",
    "    \"point_intensity\":20,\n",
    "    \"psf_sigma\":1,\n",
    "    \"frame_time\":20,#ms\n",
    "    \"pixel_size\":globals[\"olympus_pixel_size\"],\n",
    "    \"axial_function\":\"exponential\",\n",
    "    \"density_dif\": 1\n",
    "}\n",
    "\n",
    "cell_sim = simulate_cells.Simulate_cells(cell_parms,global_sim_params)\n",
    "a = cell_sim.get_cell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the sim data\n",
    "simulate_cells.make_directory_structure(\"/Volumes/Baljyot_HD/Sim_Data/sim_data_condensate_axial/h_0.2_track_100_diff_0.0_condensate_none_large_ecoli/\",\"test_seg\",a[\"map\"],\"mean\",1,\n",
    "                                        data=a,parameters={\"cell_parms\":cell_parms,\"sim_params\":global_sim_params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell parameters\n",
    "cell_parms = {\n",
    "    \"diffusion_coefficients\":np.array(list(0.1*np.ones(100)) + list(0.5*np.ones(100))),\n",
    "    \"initials\":np.random.uniform(20,180,(200,2)),#np.ones((100,2))*np.array([100,100]),#xy,#np.ones((10,2))*np.array([100,100]),#np.random.uniform(50,150,(200,2)),\n",
    "    \"num_tracks\":200,\n",
    "    \"track_type\":\"fbm\",\n",
    "    \"hursts\":np.array(list(0.2*np.ones(100)) + list(0.5*np.ones(100))),\n",
    "    \"dims\":(200,200),\n",
    "    \"movie_frames\":500\n",
    "}      \n",
    "\n",
    "#sim_params\n",
    "sim_params = {\n",
    "    \"track_length_mean\":20,\n",
    "    \"track_distribution\":\"constant\",\n",
    "    \"exposure_time\":20,#ms same as frame_time\n",
    "    \"base_noise\":140,\n",
    "    \"point_intensity\":20,\n",
    "    \"psf_sigma\":130./130.,\n",
    "    \"frame_time\":20,#ms\n",
    "    \"pixel_size\":globals[\"olympus_pixel_size\"],\n",
    "    \"axial_function\":\"ones\"\n",
    "}\n",
    "cell_sim = simulate_cells.Simulate_cells(cell_parms,sim_params)\n",
    "a = cell_sim.get_cell()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the sim data\n",
    "simulate_cells.make_directory_structure(\"/Volumes/Baljyot_HD/Sim_Data/sim_data_condensate_axial/h_0.5_track_100_diff_0.001_condensate_none_large_ecoli\",\"test_seg\",a[\"map\"],\"mean\",1,\n",
    "                                        data=a,parameters={\"cell_parms\":cell_parms,\"sim_params\":sim_params})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all simulated maps saved:\n",
    "1) open the true simulated data\n",
    "2) open and prepare the extracted data\n",
    "3) compare them both\n",
    "4) plot all of these on a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define all the locations of the simulated maps\n",
    "sim_maps = glob.glob(\"/Volumes/Baljyot_HD/Sim_Data/sim_data/**\")\n",
    "#remove the subsampling_sim directory\n",
    "sim_maps = [i for i in sim_maps if \"subsampling_sim\" not in i]\n",
    "#find the unique name of each directory\n",
    "sim_names = [os.path.basename(os.path.normpath(i)) for i in sim_maps]\n",
    "\n",
    "# sim_maps = [i for i in sim_maps if \"h_0.5\" not in i]\n",
    "sim_maps = [i for i in sim_maps if \"h_0.2\" not in i]\n",
    "sim_maps = [i for i in sim_maps if \"h_0.35\" not in i]\n",
    "sim_maps = [i for i in sim_maps if \"diff_1\" not in i]\n",
    "sim_maps = [i for i in sim_maps if \"diff_001\" not in i]\n",
    "sim_maps = [i for i in sim_maps if \"diff_0001\" not in i]\n",
    "sim_maps = [i for i in sim_maps if \"diff_0.001\" not in i]\n",
    "# sim_maps = [i for i in sim_maps if \"random\" not in i]\n",
    "#sim_maps = [i for i in sim_maps if \"500\" not in i]\n",
    "sim_maps = [i for i in sim_maps if \"h_0_\" not in i]\n",
    "sim_maps = [i for i in sim_maps if \"track_5_\" not in i]\n",
    "sim_maps = [i for i in sim_maps if \"track_10_\" not in i]\n",
    "sim_maps = [i for i in sim_maps if \"len499\" not in i]\n",
    "sim_maps = [i for i in sim_maps if \"testing\" not in i]\n",
    "sim_maps = [i for i in sim_maps if \"h_0.2_track_200_diff_01_at_100100\" not in i]\n",
    "\n",
    "sim_names = [i for i in sim_maps if \"h_0.35\" not in i]\n",
    "sim_names = [i for i in sim_maps if \"h_0.2\" not in i]\n",
    "# sim_names = [i for i in sim_maps if \"h_0.5\" not in i]\n",
    "sim_names = [i for i in sim_names if \"diff_1\" not in i]\n",
    "sim_names = [i for i in sim_names if \"diff_001\" not in i]\n",
    "# sim_names = [i for i in sim_names if \"random\" not in i]\n",
    "#sim_names = [i for i in sim_names if \"500\" not in i]\n",
    "sim_names = [i for i in sim_names if \"h_0_\" not in i]\n",
    "sim_names = [i for i in sim_names if \"diff_0.001\" not in i]\n",
    "sim_names = [i for i in sim_names if \"diff_0001\" not in i]\n",
    "sim_names = [i for i in sim_names if \"track_5_\" not in i]\n",
    "sim_names = [i for i in sim_names if \"track_10_\" not in i]\n",
    "sim_names = [i for i in sim_names if \"len499\" not in i]\n",
    "sim_names = [i for i in sim_names if \"testing\" not in i]\n",
    "sim_names = [i for i in sim_names if \"h_0.2_track_200_diff_01_at_100100\" not in i]\n",
    "#define 3 symbols for each of the 3 different types of simulations\n",
    "symbols = [\"x\",\"o\",\"^\"]#for hurst=0.2,0.35,0.5\n",
    "#make a list of marker symbols for each simulation name.\n",
    "#assign the same symbol to the simulations with the same hurst value\n",
    "sim_markers = []\n",
    "for i in sim_names:\n",
    "    if \"h_0.2\" in i:\n",
    "        sim_markers.append(symbols[0])\n",
    "    elif \"h_0.35\" in i:\n",
    "        sim_markers.append(symbols[1])\n",
    "    elif \"h_0.5\" in i:\n",
    "        sim_markers.append(symbols[2])\n",
    "    else:\n",
    "        print(\"Error: Unknown hurst value\")\n",
    "        print(i)\n",
    "        #sim_markers.append(\"x\")\n",
    "        break\n",
    "\n",
    "#make a list of colors for each simulation name.\n",
    "sim_colors = []\n",
    "for i in sim_names:\n",
    "    if \"track_100\" in i:\n",
    "        sim_colors.append(\"red\")\n",
    "    if \"track_200\" in i:\n",
    "        sim_colors.append(\"blue\")\n",
    "    if \"track_500\" in i:\n",
    "        sim_colors.append(\"green\")\n",
    "print(sim_names)\n",
    "print(sim_colors)\n",
    "print(sim_markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_names = ['/Volumes/Baljyot_HD/Sim_Data/sim_data_condensate_axial/h_0.2_track_100_diff_0.0_condensate_none_large_ecoli']\n",
    "sim_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#radius of confinment fucntion\n",
    "def radius_of_confinement(t,r_sqr,D,loc_msd):\n",
    "    return (r_sqr**2)*(1.-np.exp(-4*D*t/(r_sqr**2))) + 4*(loc_msd**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#radius of confinment fucntion\n",
    "def radius_of_confinement_xy(t,r_sqr,D,loc_msd_x,loc_msd_y):\n",
    "    return (r_sqr**2)*(1.-np.exp(-4*D*t/(r_sqr**2))) + 4*(loc_msd_x**2) + 4*(loc_msd_y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#power law function\n",
    "def power_law(t,alpha,D,loc_msd_x,loc_msd_y):\n",
    "    return 4*(loc_msd_x**2) + 4*(loc_msd_y**2) + 4.*D*t**(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def open the pickeled data\n",
    "def open_pickle_data(path,which=\"tracks\"):\n",
    "    with open(path+\"/Track_dump.pkl\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    true_tracks = cell_sim._convert_track_dict_msd(data[which])\n",
    "    return true_tracks\n",
    "\n",
    "def msd_calc(track_dic,h=None,tau_lim=None,tick_space=2,save=False,cd=None,data_type=None,plot=True,msd_fit_lim=3,convert=None):\n",
    "    '''Docstring for msd_calc, this is just a fancy wrapper for the MSD_Tracks function in the Analysis_functions module that also does some plotting\n",
    "    Not very useful for anything other than plotting the MSD curves for a set of tracks.\n",
    "    MSD calculations can be done using this but it is obtuse and not recommended. See MSD_Tracks for a better way to do this.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    track_dic: dictionary\n",
    "        dictionary of tracks with the keys being the track number and the values being the track\n",
    "    h: float\n",
    "        True husrt value for the simulation, if None this does not get plotted\n",
    "    tau_lim: int\n",
    "        The maximum tau value to plot, if None then this is set to the maximum tau value. Only used if plot is True\n",
    "    tick_space: int\n",
    "        Total ticks for colorbar in Van hove Correlation Plot, only used if plot is True\n",
    "    save: bool\n",
    "        If True then the plot is saved to the specified directory\n",
    "    cd: str\n",
    "        The directory to save the plot to, only used if save is True\n",
    "    data_type: str\n",
    "        The type of data that is being plotted, only used if save is True. This is the name of the folder that the plot is saved to\n",
    "    plot: bool\n",
    "        If True then the plot is plotted\n",
    "    msd_fit_lim: int, array-like of length 2, or None, optional\n",
    "        The number of points to fit the line to for the alpha value\n",
    "        if array then the first value is the lower limit and the second value is the upper limit to fit for tau\n",
    "    convert: Default, None (takes values in um for the pixel->um conversion)\n",
    "        covert pixel to um\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Dict containing:\n",
    "    fit_ens: array\n",
    "        The fit parameters for the ensemble of tracks\n",
    "    track_alpha: dict\n",
    "        The alpha values for each track\n",
    "    tavg_t1_msd: dict\n",
    "        The time averaged msd for tau = 1\n",
    "    track_msds: dict\n",
    "        The msd curves for each track\n",
    "    track_alpha_linear_fit: dict\n",
    "        The linear fit parameters for each track\n",
    "    track_diffusion: dict\n",
    "        The diffusion coefficient for each track using a polynomial fit\n",
    "    track_diffusion_linear_fit: dict\n",
    "        The diffusion coefficient for each track using a linear fit\n",
    "    \n",
    "    \n",
    "\n",
    "    '''\n",
    "    #if save is True and cd is None then raise an error\n",
    "    if save:\n",
    "        if cd is None:\n",
    "            raise ValueError(\"cd must be specified if save is True\")\n",
    "        if data_type is None:\n",
    "            raise ValueError(\"data_type must be specified if save is True\")\n",
    "\n",
    "    msd_dict,ens_displacements = MSD_Tracks(track_dic,permutation=True,return_type=\"both\",verbose=True,conversion_factor=convert)\n",
    "    msd = msd_dict[\"msd_curves\"][0]\n",
    "    msd_error = msd_dict[\"msd_curves\"][1]\n",
    "    disp_per_track = msd_dict[\"displacements\"]\n",
    "    #update the disp_per_track dictionary to have the msd curve per track\n",
    "    track_msds = {}\n",
    "    for i,j in disp_per_track.items():\n",
    "        track_msds[i] = msd_avgerage_utility(j)[0]\n",
    "    #fit a line to the msd curves for the first n of the points and find the r2 value\n",
    "    try:\n",
    "        fit_num = 15\n",
    "        fit_num_lower = 0\n",
    "        if isinstance(msd_fit_lim,int):\n",
    "            fit_ens = np.polyfit(np.log(list(msd.keys())[:msd_fit_lim]),np.log(list(msd.values())[:msd_fit_lim]),1,cov=True)\n",
    "            \n",
    "        elif isinstance(msd_fit_lim,list|tuple|np.ndarray):\n",
    "            fit_ens = np.polyfit(np.log(list(msd.keys())[msd_fit_lim[0]:msd_fit_lim[1]]),np.log(list(msd.values())[msd_fit_lim[0]:msd_fit_lim[1]]),1,cov=True)\n",
    "            #fit_ens = np.polyfit(np.log(list(msd.keys())[-15:-6]),np.log(list(msd.values())[-15:-6]),1,cov=True)\n",
    "        #fit_ens = np.polyfit(np.log(list(msd.keys())[fit_num_lower:fit_num]),np.log(list(msd.values())[fit_num_lower:fit_num]),1,cov=True)\n",
    "        slope_error = np.sqrt(fit_ens[1])\n",
    "        #fit the first 12 time points to the radius_of_confinement function\n",
    "        fit_ens_con,pcov_fit_ens_con = curve_fit(radius_of_confinement_xy,0.02*(np.array(list(msd.keys()))[fit_num_lower:fit_num]),np.array(list(msd.values()))[fit_num_lower:fit_num],p0=[1,0.3,0.01,0.01],method='lm')\n",
    "        fit_ens_power_law,fit_ens_power_law_con = curve_fit(power_law,0.02*(np.array(list(msd.keys()))[fit_num_lower:fit_num]),np.array(list(msd.values()))[fit_num_lower:fit_num],p0=[1,1,0.03,0.03],method='lm')\n",
    "        print(fit_ens)\n",
    "        print(fit_ens_con,pcov_fit_ens_con)\n",
    "        print(fit_ens_power_law,fit_ens_power_law_con)\n",
    "        #plot this fit\n",
    "        if plot:\n",
    "            plt.errorbar(0.02*(np.array(list(msd.keys()))[fit_num_lower:fit_num]),np.array(list(msd.values()))[fit_num_lower:fit_num],yerr=np.array(list(msd_error.values())[fit_num_lower:fit_num])*1.96,fmt=\"o\",label=\"Ensemble MSD\")\n",
    "            plt.plot(0.02*(np.array(list(msd.keys()))[fit_num_lower:fit_num]),radius_of_confinement_xy(0.02*(np.array(list(msd.keys()))[fit_num_lower:fit_num]),*fit_ens_con),label=\"Radius of Confinement Fit\")\n",
    "            plt.legend()\n",
    "            plt.xlabel(\"Time (s)\")\n",
    "            plt.ylabel(\"MSD (um^2)\")\n",
    "            plt.yscale(\"log\")\n",
    "            plt.xscale(\"log\")\n",
    "            plt.show()\n",
    "            plt.errorbar(0.02*(np.array(list(msd.keys()))[fit_num_lower:fit_num]),np.array(list(msd.values()))[fit_num_lower:fit_num],yerr=np.array(list(msd_error.values())[fit_num_lower:fit_num])*1.96,fmt=\"o\",label=\"Ensemble MSD\")\n",
    "            plt.plot(0.02*(np.array(list(msd.keys()))[fit_num_lower:fit_num]),power_law(0.02*(np.array(list(msd.keys()))[fit_num_lower:fit_num]),*fit_ens_power_law),label=\"Power Law Fit\")\n",
    "            plt.legend()\n",
    "            plt.xlabel(\"Time (s)\")\n",
    "            plt.ylabel(\"MSD (um^2)\")\n",
    "            plt.yscale(\"log\")\n",
    "            plt.xscale(\"log\")\n",
    "            plt.title(\"Power_law\")\n",
    "            plt.show()\n",
    "\n",
    "    except:\n",
    "        fit_ens = None\n",
    "        slope_error = None\n",
    "\n",
    "    track_alphas = {}\n",
    "    track_alphas_linear_fit = {}\n",
    "    tavg_t1_msds = {}\n",
    "    track_diffusion = {}\n",
    "    track_diffusion_linear_fit = {}\n",
    "    loc_err = {}\n",
    "    d_app_loc_corr = {}\n",
    "    # for each track plot the msd_curve \n",
    "    for i,j in track_msds.items():\n",
    "        #make sure the length of the track is greater than 3 so that the fit can be done\n",
    "        if len(j.keys())<3:\n",
    "            continue\n",
    "        #set the alpha to be 0.1 so that the lines are transparent\n",
    "        #if plot:\n",
    "        #    plt.plot(j.keys(),j.values(),alpha=0.1)\n",
    "        #fit a line to the msd curves for the first 3 of the points and find the r2 value\n",
    "        if isinstance(msd_fit_lim,int):\n",
    "            #fit,pcov = curve_fit(fit_MSD_Linear,np.log(list(j.keys())[:msd_fit_lim]),np.log(list(j.values())[:msd_fit_lim]),p0=[1,1])\n",
    "            #repeat this with fitting the msd to a the function fit_MSD from Analysis_functions using curve_fit\n",
    "            fit_curve,pcov = curve_fit(fit_MSD,list(j.keys())[:msd_fit_lim],list(j.values())[:msd_fit_lim],p0=[1,1,0],maxfev=1000000)\n",
    "            #fit using the loc_error function\n",
    "            fit_curve_loc,pcov_loc = curve_fit(fit_MSD_loc_err,list(j.keys())[:msd_fit_lim],list(j.values())[:msd_fit_lim],p0=[1,1,1],maxfev=1000000)\n",
    "        elif isinstance(msd_fit_lim,list|tuple|np.ndarray):\n",
    "            #fit,pcov = curve_fit(fit_MSD_Linear,np.log(list(j.keys())[msd_fit_lim[0]:msd_fit_lim[1]]),np.log(list(j.values())[msd_fit_lim[0]:msd_fit_lim[1]]),p0=[1,1])\n",
    "            #repeat this with fitting the msd to a the function fit_MSD from Analysis_functions using curve_fit\n",
    "            fit_curve,pcov = curve_fit(fit_MSD,list(j.keys())[msd_fit_lim[0]:msd_fit_lim[1]],list(j.values())[msd_fit_lim[0]:msd_fit_lim[1]],p0=[1,1,0],maxfev=1000000)\n",
    "            #fit using the loc_error function\n",
    "            fit_curve_loc,pcov_loc = curve_fit(fit_MSD_loc_err,list(j.keys())[msd_fit_lim[0]:msd_fit_lim[1]],list(j.values())[msd_fit_lim[0]:msd_fit_lim[1]],p0=[0.045,0.2,1],maxfev=1000000)\n",
    "        #plot the fitted line\n",
    "        #if plot and fit_curve[1]<0:\n",
    "        #     plt.plot(list(j.keys())[:msd_fit_lim],fit_MSD(list(j.keys())[:msd_fit_lim],fit_curve[0],fit_curve[1]),alpha=0.1)\n",
    "        #     plt.plot(j.keys(),j.values(),alpha=0.1)\n",
    "        # plt.show()\n",
    "        if plot:\n",
    "            plt.plot(list(j.keys()),np.array(list(j.values())),alpha=0.1)\n",
    "        #add the slope of the fitted line to the track_alphas dictionary\n",
    "        #track_alphas_linear_fit[i] = fit[1]\n",
    "        track_alphas[i] = fit_curve[1]\n",
    "        #add the msd at tau=1 to the tavg_t1_msds dictionary, divide by 4 to get the correct value t is by default 1 since its tau=1\n",
    "        tavg_t1_msds[i] = j[1]/4.\n",
    "        track_diffusion[i] = fit_curve[0]/4.\n",
    "        #track_diffusion_linear_fit[i] = np.exp(fit[0])/4.\n",
    "        loc_err[i] = fit_curve_loc[2]\n",
    "        d_app_loc_corr[i] = fit_curve_loc[0]/4.\n",
    "        \n",
    "    if plot:\n",
    "        #plot the msd curves and the fitted line\n",
    "        plt.plot(list(msd.keys())[:fit_num],np.array(list(msd.values()))[:fit_num],label=\"MSD_ensemble\",linewidth=3,alpha=1,zorder=1)\n",
    "        if fit_ens != None:\n",
    "            plt.plot(list(msd.keys())[:fit_num],np.exp(fit_ens[0][1])*(np.array(list(msd.keys()))[:fit_num])**fit_ens[0][0],label=\"fit_ensemble\",linewidth=3,alpha=1,zorder=2)\n",
    "\n",
    "        plt.xscale(\"log\")\n",
    "        plt.yscale(\"log\")\n",
    "        #label the plot\n",
    "        plt.xlabel(\"lag time (au)\")\n",
    "        plt.ylabel(\"MSD (au)\")\n",
    "        plt.legend()\n",
    "        #annotate the plot with the slope of the fitted line with 2 decimal places (label the slope as alpha in greek)\n",
    "        #add the error in the slope as well\n",
    "        #plt.annotate(r\"$\\alpha$ = {:.2f} $\\pm$ {:.2f}\".format(fit_ens[0][0],slope_error[0][0]),xy=(0.05,0.7),xycoords=\"axes fraction\",fontweight=\"bold\",fontsize=16)\n",
    "        #annotate the true alpha value (hurst*2)\n",
    "        if h != None:\n",
    "            plt.annotate(r\"True $\\alpha$ = {:.2f}\".format(h*2),xy=(0.05,0.6),xycoords=\"axes fraction\",fontweight=\"bold\",fontsize=16)\n",
    "        if save == True:\n",
    "            plt.savefig(cd+\"/{0}_MSD_plot.png\".format(data_type))\n",
    "        plt.show()\n",
    "\n",
    "        #repeat on a linear-linear plot\n",
    "        #plot the msd curves and the fitted line\n",
    "        plt.errorbar(list(msd.keys())[:fit_num],np.array(list(msd.values()))[:fit_num],yerr=np.array(list(msd_error.values()))[:fit_num]*1.96,label=\"MSD_ensemble\",linewidth=3,alpha=1,zorder=1)\n",
    "        if fit_ens != None:\n",
    "            plt.plot(list(msd.keys())[:fit_num],np.exp(fit_ens[0][1])*(np.array(list(msd.keys()))[:fit_num])**fit_ens[0][0],label=\"fit_ensemble\",linewidth=3,alpha=1,zorder=2)\n",
    "\n",
    "        # plt.xscale(\"log\")\n",
    "        # plt.yscale(\"log\")\n",
    "        #label the plot\n",
    "        plt.xlabel(\"lag time (au)\")\n",
    "        plt.ylabel(\"MSD (au)\")\n",
    "        plt.legend()\n",
    "        #annotate the plot with the slope of the fitted line with 2 decimal places (label the slope as alpha in greek)\n",
    "        #add the error in the slope as well\n",
    "        #plt.annotate(r\"$\\alpha$ = {:.2f} $\\pm$ {:.2f}\".format(fit_ens[0][0],slope_error[0][0]),xy=(0.05,0.7),xycoords=\"axes fraction\",fontweight=\"bold\",fontsize=16)\n",
    "        #annotate the true alpha value (hurst*2)\n",
    "        if h != None:\n",
    "            plt.annotate(r\"True $\\alpha$ = {:.2f}\".format(h*2),xy=(0.05,0.6),xycoords=\"axes fraction\",fontweight=\"bold\",fontsize=16)\n",
    "        if save == True:\n",
    "            plt.savefig(cd+\"/{0}_MSD_plot.png\".format(data_type))\n",
    "        plt.show()\n",
    "\n",
    "        #on a new figure plot the histogram of the slopes of the fitted lines\n",
    "        plt.clf()\n",
    "        plt.hist(list(track_alphas.values()),bins=10)\n",
    "        #plot a vertical line at the mean of the track_alphas\n",
    "        plt.axvline(np.mean(list(track_alphas.values())),color=\"red\",label=\"mean\")\n",
    "        #annotate the plot with the mean of the track_alphas\n",
    "        #plt.annotate(r\"$\\alpha$ = {:.2f}\".format(np.mean(list(track_alphas.values()))),xy=(0.05,0.7),xycoords=\"axes fraction\",fontweight=\"bold\",fontsize=16)\n",
    "        plt.xlabel(r\"$\\alpha$\")\n",
    "        plt.ylabel(\"count\")\n",
    "        if save == True:\n",
    "            plt.savefig(cd+\"/{0}_alpha_hist.png\".format(data_type))\n",
    "        plt.show()\n",
    "\n",
    "        #repeat the above for the track_alphas_linear_fit\n",
    "        plt.clf()\n",
    "        plt.hist(list(track_alphas_linear_fit.values()),bins=10)\n",
    "        plt.axvline(np.mean(list(track_alphas_linear_fit.values())),color=\"red\",label=\"mean\")\n",
    "        #plt.annotate(r\"$\\alpha$ = {:.2f}\".format(np.mean(list(track_alphas_linear_fit.values()))),xy=(0.05,0.7),xycoords=\"axes fraction\",fontweight=\"bold\",fontsize=16)\n",
    "        plt.xlabel(r\"$\\alpha$\")\n",
    "        plt.ylabel(\"count\")\n",
    "        if save == True:\n",
    "            plt.savefig(cd+\"/{0}_alpha_hist_linear_fit.png\".format(data_type))\n",
    "        plt.show()\n",
    "\n",
    "    #the following plots the pdf of the displacements for each tau, right now it sometimes creates infinite loops so it is commented out TODO: fix this\n",
    "    if plot:\n",
    "        #make a figure and axes 2 subplots\n",
    "        fig,ax = plt.subplots(1,2,figsize=(20,10))\n",
    "\n",
    "        #get a collection of N different colours where N is the number of taus\n",
    "        colors = plt.cm.jet(np.linspace(0,1,len(ens_displacements.keys())))\n",
    "\n",
    "    #make a df to store the tau value and the fitted gmm mean for that tau\n",
    "    gmm_tau_df = pd.DataFrame(columns=[\"tau\",\"mean\",\"sigma\"])\n",
    "    #make a histogram of the displacements for each tau from ens_displacements\n",
    "    for i,j in ens_displacements.items():\n",
    "        #convert to r\n",
    "        j_r = np.sqrt(np.sum(np.array(j)**2,axis=1))\n",
    "        #if the tau is greater than the tau_lim then skip it\n",
    "        if (tau_lim != None):\n",
    "            if i > tau_lim: \n",
    "                continue\n",
    "        if plot:\n",
    "            #make the histogram normalized and transparent for the first subplot\n",
    "            ax[0].hist(np.ndarray.flatten(np.array(j)),bins=100,alpha=0.1,color=colors[i-1],density=True)#,stacked=True,weights=np.ones(len(np.ndarray.flatten(np.array(j))))/len(np.ndarray.flatten(np.array(j))))\n",
    "            #make the histogram normalized and transparent for the second subplot for abs displacements\n",
    "            ax[1].hist(np.abs(np.ndarray.flatten(np.array(j_r))),bins=100,alpha=0.1,color=colors[i-1],density=True)#,stacked=True,weights=np.ones(len(np.ndarray.flatten(np.array(j))))/len(np.ndarray.flatten(np.array(j))))\n",
    "            pass\n",
    "        #fit a gaussian to the histogram\n",
    "        mu,sigma = norm.fit(np.ndarray.flatten(np.array(j)))\n",
    "        #fit it for the abs displacements as well\n",
    "        mu_abs,sigma_abs = norm.fit(np.abs(np.ndarray.flatten(np.array(j))))\n",
    "        #store the tau and the mean of the gaussian in the df\n",
    "        gmm_tau_df = gmm_tau_df.append({\"tau\":i,\"mean\":mu_abs,\"sigma\":sigma_abs**2},ignore_index=True)\n",
    "        if plot:\n",
    "            #plot the gaussian\n",
    "            x = np.linspace(np.min(np.ndarray.flatten(np.array(j))),np.max(np.ndarray.flatten(np.array(j))),100)\n",
    "            x_abs = np.linspace(np.min(np.abs(np.ndarray.flatten(np.array(j_r)))),np.max(np.abs(np.ndarray.flatten(np.array(j_r)))),100)\n",
    "            ax[0].plot(x,norm.pdf(x,mu,sigma),linewidth=1,color=colors[i-1])\n",
    "            ax[1].plot(x_abs,norm.pdf(x_abs,mu_abs,sigma_abs),linewidth=1,color=colors[i-1])\n",
    "\n",
    "    if plot:\n",
    "        #label the plot, in greek the delta_x is P_delta_x\n",
    "        ax[0].set_xlabel(r\"$\\Delta r$ (au)\")\n",
    "        ax[0].set_ylabel(r\"$P_{\\Delta r}$ ($au^{-1}$)\")\n",
    "        ax[1].set_xlabel(r\"$|\\Delta r|$ (au)\")\n",
    "        ax[1].set_ylabel(r\"$P_{|\\Delta r|}$ ($au^{-1}$)\")\n",
    "        ax[0].set_yscale(\"log\")\n",
    "\n",
    "        v1 = np.linspace(np.min(np.array(list(ens_displacements.keys()),dtype=int)), np.max(np.array(list(ens_displacements.keys()),dtype=int)), tick_space, endpoint=True)\n",
    "        #rather than a legend, make a colorbar with the colors corresponding to the taus\n",
    "        cbar = plt.colorbar(plt.cm.ScalarMappable(cmap=plt.cm.jet),ticks=v1,ax=ax[0])\n",
    "        cbar.ax.set_yticklabels([\"{:4.2f}\".format(i) for i in v1])\n",
    "\n",
    "        cbar.set_label(\"tau (au)\")\n",
    "\n",
    "\n",
    "        #make the title\n",
    "        ax[0].set_title(\"PDF of displacements for each tau (van Hove correlation)\")\n",
    "        #ax[0].set_ylim(0,1.5)\n",
    "        if save == True:\n",
    "            plt.savefig(cd+\"/{0}_PDF_plot.png\".format(data_type))\n",
    "        plt.show()\n",
    "        #plot the mean of the gaussian for each tau\n",
    "        plt.errorbar(gmm_tau_df[\"tau\"],gmm_tau_df[\"mean\"],yerr=gmm_tau_df[\"sigma\"],fmt=\"o\")\n",
    "        plt.xlabel(\"tau (au)\")\n",
    "        plt.ylabel(\"mean of gaussian fit (au)\")\n",
    "        plt.title(\"Mean of gaussian fit for each tau\")\n",
    "        plt.show()\n",
    "\n",
    "    return {\"fit_ens\":fit_ens, \n",
    "            \"track_alpha\":track_alphas, \n",
    "            \"tavg_t1_msd\":tavg_t1_msds, \n",
    "            \"track_msds\":track_msds, \n",
    "            \"track_alpha_linear_fit\":track_alphas_linear_fit,\n",
    "            \"track_diffusion\":track_diffusion,\n",
    "            \"track_diffusion_linear_fit\":track_diffusion_linear_fit,\n",
    "            \"D_app_loc_corr\":d_app_loc_corr,\n",
    "            \"loc_err\":loc_err,\n",
    "            \"msd_curve_ens\":msd,\n",
    "            \"msd_curve_ens_err\":msd_error,\n",
    "            \"Displacements_per_track\": disp_per_track,\n",
    "            \"Track_msds\":track_msds,\n",
    "            \"ens_displacements\":ens_displacements}\n",
    "#load the extracted data\n",
    "def read_data(wd):\n",
    "    \n",
    "    rp_ez= run_analysis(wd=wd,\n",
    "                    t_string=\"test_seg\",sim=True)\n",
    "                    \n",
    "    rp_ez.read_parameters(minimum_percent_per_drop_in = 0.5, \n",
    "                        t_len_u = 500, \n",
    "                        t_len_l=1, \n",
    "                        minimum_tracks_per_drop = 3,\n",
    "                        frame_step=500,\n",
    "                        frame_total=500)\n",
    "\n",
    "    rp_ez.get_blob_parameters(threshold=1e-2,\n",
    "                            overlap=0,\n",
    "                            detection_name='bp',\n",
    "                            min_sigma=1/np.sqrt(2),\n",
    "                            max_sigma=6/np.sqrt(2),\n",
    "                            num_sigma=500,median = False)\n",
    "\n",
    "    rp_ez.get_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                        \"plot_fit\":False,\n",
    "                                        \"fitting_image\":\"Original\",\n",
    "                                        \"radius_func\":identity,\n",
    "                                        \"residual_func\":residuals_gaus2d,\n",
    "                                        \"sigma_range\":2,\n",
    "                                        \"centroid_range\":0.5,\n",
    "                                        \"height_range\":2})\n",
    "    tem = rp_ez.run_flow_sim(wd,\"test_seg\")\n",
    "    track_dict = rp_ez._convert_to_track_dict_bulk()\n",
    "    rp_ez.mat_path_dir = wd + \"/Analysis/\" + \"test_seg\" + \"MATLAB_dat/\"\n",
    "    #make a directory rp_ez.mat_path_dir if it doesn't exist and make two subdirectories nameed \"true\" and \"extracted\"\n",
    "    if not os.path.exists(rp_ez.mat_path_dir):\n",
    "        os.makedirs(rp_ez.mat_path_dir)\n",
    "        os.makedirs(rp_ez.mat_path_dir+\"/true\")\n",
    "        os.makedirs(rp_ez.mat_path_dir+\"/extracted\")\n",
    "    SMAUG_extracted = smt.convert_track_data_SMAUG_format(track_dict[\"ALL\"])\n",
    "    #save this to the extracted folder as a .mat file using scipy.io.savemat\n",
    "    io.savemat(rp_ez.mat_path_dir+\"/extracted/extracted_track_data.mat\",{\"trfile\":SMAUG_extracted})\n",
    "    true = open_pickle_data(sim_names[0])\n",
    "    #convert the extracted data to the SMT SMAUG format\n",
    "    SMAUG_true = smt.convert_track_data_SMAUG_format(true)\n",
    "    #save this to the true folder as a .mat file using scipy.io.savemat\n",
    "    io.savemat(rp_ez.mat_path_dir+\"/true/true_track_data.mat\",{\"trfile\":SMAUG_true})\n",
    "    return track_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = read_data(sim_names[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the localization error using trackmate fitting (sim a long track at the same point and look at the differences in the x,y positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_ez= run_analysis(wd='/Users/baljyot/Documents',\n",
    "                t_string=\"test_seg\",sim=True)\n",
    "                \n",
    "rp_ez.read_parameters(minimum_percent_per_drop_in = 0.5, \n",
    "                    t_len_u = 500, \n",
    "                    t_len_l=1, \n",
    "                    minimum_tracks_per_drop = 3,\n",
    "                    frame_step=500,\n",
    "                    frame_total=500)\n",
    "\n",
    "rp_ez.get_blob_parameters(threshold=1e-2,\n",
    "                        overlap=0,\n",
    "                        detection_name='bp',\n",
    "                        min_sigma=1/np.sqrt(2),\n",
    "                        max_sigma=6/np.sqrt(2),\n",
    "                        num_sigma=500,median = False)\n",
    "\n",
    "rp_ez.get_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                    \"plot_fit\":False,\n",
    "                                    \"fitting_image\":\"Original\",\n",
    "                                    \"radius_func\":identity,\n",
    "                                    \"residual_func\":residuals_gaus2d,\n",
    "                                    \"sigma_range\":2,\n",
    "                                    \"centroid_range\":0.5,\n",
    "                                    \"height_range\":2})\n",
    "tem = rp_ez.run_flow_sim('/Users/baljyot/Documents',\"test_seg\")\n",
    "track_dict = rp_ez._convert_to_track_dict_bulk()\n",
    "tracks = track_dict[\"ALL\"]\n",
    "a = msd_calc(tracks,h=None,tau_lim=None,tick_space=2,plot=True,msd_fit_lim=10)\n",
    "msds = np.array(list(a[\"tavg_t1_msd\"].values()))\n",
    "plt.hist(msds,bins=100)\n",
    "plt.show()\n",
    "#true xy\n",
    "true_xy = np.array([100,100])\n",
    "#collect all the xys \n",
    "xys = []\n",
    "for i,j in tracks.items():\n",
    "    track_xy = np.stack([j[:,0],j[:,1]],axis=1)\n",
    "    xys+=list(track_xy)\n",
    "diff_x = np.zeros(len(xys))\n",
    "diff_y = np.zeros(len(xys))\n",
    "diff_r = np.zeros(len(xys))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(xys)):\n",
    "    #find the difference in true xy and the xy\n",
    "    diff = true_xy - xys[i]\n",
    "    diff_x[i] = diff[0]\n",
    "    diff_y[i] = diff[1]\n",
    "    diff_r[i] = np.sqrt(diff_x[i]**2 + diff_y[i]**2)\n",
    "#plot the histogram of diff_x, diff_y\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,5))\n",
    "ax[0].hist(diff_x,bins=30,label=\"diff_x\")\n",
    "ax[1].hist(diff_y,bins=30,label=\"diff_y\")\n",
    "#plot in text the std of diff_x, diff_y named as localization error x,y\n",
    "ax[0].text(0.05,0.95,\"localization error x: \"+str(np.std(diff_x)*globals[\"olympus_pixel_size\"])[:5]+\" nm\",transform=ax[0].transAxes)\n",
    "ax[1].text(0.05,0.95,\"localization error y: \"+str(np.std(diff_y)*globals[\"olympus_pixel_size\"])[:5]+\" nm\",transform=ax[1].transAxes)\n",
    "#make the legend\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "#put the x and y labels\n",
    "ax[0].set_xlabel(\"diff_x (pixels)\")\n",
    "ax[1].set_xlabel(\"diff_y (pixels)\")\n",
    "ax[0].set_ylabel(\"count\")\n",
    "ax[1].set_ylabel(\"count\")\n",
    "#make a title\n",
    "ax[0].set_title(\"Histogram of diff_x\")\n",
    "ax[1].set_title(\"Histogram of diff_y\")\n",
    "plt.show()\n",
    "#plot the histogram of diff_r\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,5))\n",
    "ax.hist(diff_r,bins=30,label=\"diff_r\")\n",
    "#make the legend\n",
    "ax.legend()\n",
    "#put the x and y labels\n",
    "ax.set_xlabel(\"diff_r (pixels)\")\n",
    "ax.set_ylabel(\"count\")\n",
    "#make a title\n",
    "ax.set_title(\"Histogram of diff_r\")\n",
    "plt.show()\n",
    "\n",
    "#print the mean and std of diff_x, diff_y, diff_r after converting to nm using the olympus pixel size in global variables\n",
    "print(\"mean of diff_x: \",np.mean(diff_x)*globals[\"olympus_pixel_size\"],\"nm\")\n",
    "print(\"std of diff_x: \",np.std(diff_x)*globals[\"olympus_pixel_size\"],\"nm\")\n",
    "print(\"mean of diff_y: \",np.mean(diff_y)*globals[\"olympus_pixel_size\"],\"nm\")\n",
    "print(\"std of diff_y: \",np.std(diff_y)*globals[\"olympus_pixel_size\"],\"nm\")\n",
    "print(\"mean of diff_r: \",np.mean(diff_r)*globals[\"olympus_pixel_size\"],\"nm\")\n",
    "print(\"std of diff_r: \",np.std(diff_r)*globals[\"olympus_pixel_size\"],\"nm\")\n",
    "\n",
    "#define a dataframe to store the msds\n",
    "msds_df = pd.DataFrame(columns=[\"msds\",\"diffusion_coefficient\",\"track_length\",\"track_id\"])\n",
    "#loop over all the tracks\n",
    "for i,j in rp_ez.Movie.items():\n",
    "    for k,l in j.Cells.items():\n",
    "        for m,n in l.All_Tracjectories.items():\n",
    "            #if n.Classification == \"IN\":\n",
    "            msd_t = n.MSD_total_um\n",
    "            diff_t = msd_t\n",
    "            track_length = len(n.X)\n",
    "            track_ID = i+k+m\n",
    "            msds_df = msds_df.append({\"msds\":msd_t,\"diffusion_coefficient\":diff_t,\"track_length\":track_length,\"track_id\":track_ID},ignore_index=True)\n",
    "\n",
    "#plot the histogram of msds\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,5))\n",
    "ax.hist(msds_df[\"msds\"],bins=30,label=\"msds\")\n",
    "#make the legend\n",
    "ax.legend()\n",
    "#put the x and y labels\n",
    "ax.set_xlabel(\"msds (um^2)\")\n",
    "ax.set_ylabel(\"count\")\n",
    "#make a title\n",
    "ax.set_title(\"Histogram of msds\")\n",
    "plt.show()\n",
    "\n",
    "#plot the histogram of diffusion_coefficient\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,5))\n",
    "ax.hist(np.log10(msds_df[\"diffusion_coefficient\"]),bins=30,label=\"diffusion_coefficient\")\n",
    "#make the legend\n",
    "ax.legend()\n",
    "#put the x and y labels\n",
    "ax.set_xlabel(\"diffusion_coefficient (um^2/s)\")\n",
    "ax.set_ylabel(\"count\")\n",
    "#make a title\n",
    "ax.set_title(\"Histogram of diffusion_coefficient\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_ez= run_analysis(wd=sim_names[0],\n",
    "                t_string=\"test_seg\",sim=True)\n",
    "                \n",
    "rp_ez.read_parameters(minimum_percent_per_drop_in = 0.5, \n",
    "                    t_len_u = 500, \n",
    "                    t_len_l=1, \n",
    "                    minimum_tracks_per_drop = 3,\n",
    "                    frame_step=500,\n",
    "                    frame_total=500)\n",
    "\n",
    "rp_ez.get_blob_parameters(threshold=1e-2,\n",
    "                        overlap=0,\n",
    "                        detection_name='bp',\n",
    "                        min_sigma=1/np.sqrt(2),\n",
    "                        max_sigma=6/np.sqrt(2),\n",
    "                        num_sigma=500,median = False)\n",
    "\n",
    "rp_ez.get_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                    \"plot_fit\":False,\n",
    "                                    \"fitting_image\":\"Original\",\n",
    "                                    \"radius_func\":identity,\n",
    "                                    \"residual_func\":residuals_gaus2d,\n",
    "                                    \"sigma_range\":2,\n",
    "                                    \"centroid_range\":0.5,\n",
    "                                    \"height_range\":2})\n",
    "tem = rp_ez.run_flow_sim(sim_names[0],\"test_seg\")\n",
    "rp_ez._make_NOBIAS_files()\n",
    "rp_ez._make_SMAUG_files()\n",
    "track_dict = rp_ez._convert_to_track_dict_bulk()\n",
    "a = msd_calc(track_dict[\"ALL\"],h=None,tau_lim=None,tick_space=2,plot=True,msd_fit_lim=3)\n",
    "msds = np.array(list(a[\"track_diffusion\"].values()))\n",
    "loc_err = np.array(list(a[\"loc_err\"].values()))\n",
    "plt.hist(np.sqrt(loc_err/4.),bins=30)\n",
    "plt.show()\n",
    "\n",
    "# true_tracks = open_pickle_data(sim_names[0])\n",
    "# true_track_msd_calc = msd_calc(true_tracks,h=None,tau_lim=None,tick_space=2,plot=True,msd_fit_lim=3)\n",
    "# true_track_msds = np.array(list(true_track_msd_calc[\"track_diffusion\"].values()))\n",
    "\n",
    "\n",
    "#define a df to store the diff\n",
    "msds_df = pd.DataFrame(columns=[\"diffusion_coefficient\",\"track_length\",\"track_id\"])\n",
    "#loop over all the tracks\n",
    "for i,j in rp_ez.Movie.items():\n",
    "    for k,l in j.Cells.items():\n",
    "        for m,n in l.All_Tracjectories.items():\n",
    "            diff_t = n.MSD_total_um\n",
    "            track_length = len(n.X)\n",
    "            track_ID = i+k+m\n",
    "            msds_df = msds_df.append({\"diffusion_coefficient\":diff_t,\"track_length\":track_length,\"track_id\":track_ID},ignore_index=True)   \n",
    "\n",
    "\n",
    "#plot the histogram of msds\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,5))\n",
    "ax.hist(np.log10(msds_df[\"diffusion_coefficient\"]),bins=30,label=\"diffusion_coefficient\")\n",
    "#plot a vertical line at 0.023 um^2/s as mean of LL\n",
    "ax.axvline(0.023,color=\"red\",label=\"mean of LL\")\n",
    "#make the legend\n",
    "ax.legend()\n",
    "#put the x and y labels\n",
    "ax.set_xlabel(\"diffusion_coefficient (um^2/s)\")\n",
    "ax.set_ylabel(\"count\")\n",
    "#make a title\n",
    "ax.set_title(\"Histogram of diffusion_coefficient\")\n",
    "ax.set_xlim(-4,1)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#define the localization error in nm \n",
    "loc_err = [15]\n",
    "#convert the localization error to um\n",
    "loc_err = [i/1000. for i in loc_err]\n",
    "loc_diff = find_static_localization_error_MSD(loc_err,2)\n",
    "def convert_diff(n):\n",
    "    m =n*(1000./20.)*(130**2)*1e-6\n",
    "    return m\n",
    "#plot the log10 of the msds as a histogram\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.hist(np.log10(convert_diff(msds*4)),bins=50)\n",
    "\n",
    "#label the lines with the nm version of the localization error\n",
    "for i in range(len(loc_diff)):\n",
    "    ax.axvline(np.log10(loc_diff[i]),color=\"red\")\n",
    "    ax.text(np.log10(loc_diff[i])+0.1,10,\"{0}nm\".format(int(loc_err[i]*1000.)),rotation=0,color=\"red\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"log10(msd)\")\n",
    "ax.set_ylabel(\"count\")\n",
    "ax.set_title(\"log10(msd) histogram\")\n",
    "plt.show()\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist(np.log10((msds)*(1000./20.)*(130**2)*1e-6),bins=50)\n",
    "\n",
    "ax.set_xlabel(\"log10(diffusion coefficient)\")\n",
    "ax.set_ylabel(\"count\")\n",
    "ax.set_title(\"log10(diffusion coefficient) histogram\")\n",
    "ax.set_xlim(-4,1)\n",
    "plt.show()\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist(np.log10((true_track_msds)*(1000./20.)*(130**2)*1e-6),bins=50)\n",
    "for i in range(len(loc_diff)):\n",
    "    ax.axvline(np.log10(loc_diff[i]/4),color=\"red\")\n",
    "    ax.text(np.log10(loc_diff[i]/4)+0.1,10,\"{0}nm\".format(int(loc_err[i]*1000.)),rotation=0,color=\"red\")\n",
    "ax.set_xlabel(\"True log10(diffusion coefficient)\")\n",
    "ax.set_ylabel(\"count\")\n",
    "ax.set_title(\"True log10(diffusion coefficient) histogram\")\n",
    "ax.set_xlim(-4,1)\n",
    "plt.show()\n",
    "\n",
    "true_msd_cal = np.log10((true_track_msds) *(1000./20.)*(130**2)*1e-6)\n",
    "\n",
    "#find the KDE fo the true_msd_cal\n",
    "kde = stats.gaussian_kde(true_msd_cal)\n",
    "#find the x values for the KDE\n",
    "x = np.linspace(-4,1,1000)\n",
    "#find the y values for the KDE\n",
    "y = kde(x)\n",
    "\n",
    "#find the mean and std of the true_msd_cal and save them\n",
    "true_msd_mean = np.mean(true_msd_cal)\n",
    "true_msd_std = np.std(true_msd_cal)\n",
    "#along with the data plot a gaussian with the mean and std of the true_msd_cal\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist(true_msd_cal,bins=50,density=True,label = \"True\")\n",
    "ax.plot(x,y,label = \"KDE\")\n",
    "ax.plot(x,stats.norm.pdf(x,true_msd_mean,true_msd_std),label=\"Gaussian\")\n",
    "ax.set_xlabel(\"True log10(diffusion coefficient)\")\n",
    "ax.set_ylabel(\"count\")\n",
    "ax.set_title(\"True log10(diffusion coefficient) histogram\")\n",
    "ax.set_xlim(-4,1)\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for each sim_map open the true track data, and the extracted data\n",
    "threshold=0.25\n",
    "#make a figure \n",
    "plt.clf()\n",
    "#make the figure large enough to fit the legend outside the plot, make it 3D \n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "stored_data = np.zeros((len(sim_maps),3))\n",
    "\n",
    "point_pair_errors = np.zeros((len(sim_maps)))\n",
    "linking_errors = np.zeros((len(sim_maps)))\n",
    "\n",
    "fit_diff_error = np.zeros((len(sim_maps)))\n",
    "fit_alpha_error = np.zeros((len(sim_maps)))\n",
    "\n",
    "#store the point density and the convex hull density\n",
    "true_points_per_frame = []\n",
    "extracted_points_per_frame = []\n",
    "\n",
    "true_area_per_frame = []\n",
    "extracted_area_per_frame = []\n",
    "\n",
    "true_tavg_t1_msds = []\n",
    "extracted_tavg_t1_msds = []\n",
    "\n",
    "for i in range(len(sim_maps[:])):\n",
    "    print(\"sim_names: {0}\".format(sim_names[i]))\n",
    "    true_tracks = open_pickle_data(sim_maps[i])\n",
    "    extracted_tracks = read_data(sim_maps[i])[\"ALL\"]\n",
    "    #convert the true tracks and the extracted tracks to points per frame format\n",
    "    true_tracks_frame = points_per_frame_convert(true_tracks)\n",
    "    extracted_tracks_frame = points_per_frame_convert(extracted_tracks)\n",
    "    #perform the point_error calculations for the true and extracted tracks\n",
    "    percent_points_detected = point_error_detection(true_tracks_frame,extracted_tracks_frame,threshold=threshold)\n",
    "    #find the number of points per frame \n",
    "    true_points_per_frame.append({frames:len(points) for frames,points in true_tracks_frame.items()})\n",
    "    extracted_points_per_frame.append({frames:len(points) for frames,points in extracted_tracks_frame.items()})\n",
    "    #find the area of the points using radius of gyration\n",
    "    true_area_per_frame.append({frames: radius_of_gyration(points) if radius_of_gyration(points)!=None else 0 for frames,points in true_tracks_frame.items()})\n",
    "    extracted_area_per_frame.append({frames: radius_of_gyration(points) if radius_of_gyration(points)!=None else 0 for frames,points in extracted_tracks_frame.items()})\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    #find the point pairs for both \n",
    "    true_point_pairs = convert_point_pairs(true_tracks)\n",
    "    extracted_point_pairs = convert_point_pairs(extracted_tracks)\n",
    "    #perform the point_error calculations for the true and extracted tracks\n",
    "    percent_point_pairs_detected,linking_ratio = point_pair_error_detection(true_point_pairs,extracted_point_pairs,threshold=threshold)\n",
    "    #store the percent_point_pairs_detected\n",
    "    point_pair_errors[i] = percent_point_pairs_detected\n",
    "    linking_errors[i] = linking_ratio\n",
    "    #for the true and extracted tracks do the msd calculations (only run this for new data that is being analysed for the first time)\n",
    "    msd_true = msd_calc(true_tracks,h=None,tau_lim=40,tick_space=2,save=False,cd=sim_maps[i],data_type=\"True\",plot=False,msd_fit_lim=3)\n",
    "    msd_extracted = msd_calc(extracted_tracks,h=None,tau_lim=40,tick_space=2,save=False,cd=sim_maps[i],data_type=\"Extracted\",plot=False,msd_fit_lim=3)\n",
    "    #find the percent error between true and extracted alphas\n",
    "    if msd_true[\"fit_ens\"] != None:\n",
    "        fit_alpha_error[i] = percent_error(msd_true[\"fit_ens\"][0][0],msd_extracted[\"fit_ens\"][0][0],abs=True)\n",
    "        #find the percent error between true and extracted diffusion coefficients\n",
    "        fit_diff_error[i] = msd_extracted[\"fit_ens\"][0][1]/msd_true[\"fit_ens\"][0][1]#percent_error(np.exp(msd_true[\"fit_ens\"][0][1]),np.exp(msd_extracted[\"fit_ens\"][0][1]),abs=True)\n",
    "    #store the tavg_t1_msds\n",
    "    true_tavg_t1_msds.append(msd_true[\"track_diffusion\"])\n",
    "    extracted_tavg_t1_msds.append(msd_extracted[\"track_diffusion\"])\n",
    "\n",
    "    track_match_dict = identity_track_matrix(true_tracks,extracted_tracks,threshold=threshold)\n",
    "    mean_identiy = np.mean(track_match_dict[\"max_identity\"])\n",
    "    mean_abs_length_error = np.mean(np.abs(track_match_dict[\"length_error\"]))\n",
    "\n",
    "    #add the point (mean_identiy,mean_abs_length_error,percent_points_detected) to the plot with the label sim_names[i]\n",
    "    print(sim_names[i])\n",
    "    print(sim_markers[i])\n",
    "    print(sim_colors[i])\n",
    "    ax.scatter(mean_identiy,mean_abs_length_error,percent_points_detected,label=sim_names[i],marker=sim_markers[i],c=sim_colors[i])\n",
    "    #store the data in a numpy array\n",
    "    stored_data[i] = np.array([mean_identiy,mean_abs_length_error,percent_points_detected])\n",
    "    \n",
    "ax.set_xlabel(\"mean identity\")\n",
    "ax.set_ylabel(\"mean absolute length error\")\n",
    "ax.set_zlabel(\"percent of points detected\")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(true_area_per_frame)\n",
    "print(extracted_area_per_frame)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extracted_area_per_frame[0][3.0])\n",
    "print(true_area_per_frame[0][3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the density from the true_area_per_frame and the the true_points_per_frame\n",
    "#this is done for eahc sim_map\n",
    "true_density_per_frame = []\n",
    "for i in range(len(true_area_per_frame)):\n",
    "    map_density = []\n",
    "    for j in true_area_per_frame[i].keys():\n",
    "        if true_area_per_frame[i][j]!=0:\n",
    "            map_density.append(true_points_per_frame[i][j]/true_area_per_frame[i][j])\n",
    "    true_density_per_frame.append(np.median(map_density))\n",
    "\n",
    "#find the density from the extracted_area_per_frame and the the extracted_points_per_frame\n",
    "#this is done for eahc sim_map\n",
    "extracted_density_per_frame = []\n",
    "for i in range(len(extracted_area_per_frame)):\n",
    "    map_density = []\n",
    "    for j in extracted_area_per_frame[i].keys():\n",
    "        if extracted_area_per_frame[i][j]!=0:\n",
    "            map_density.append(extracted_points_per_frame[i][j]/extracted_area_per_frame[i][j])\n",
    "    extracted_density_per_frame.append(np.median(map_density))\n",
    "\n",
    "print(\"true_density_per_frame: {0}\".format(true_density_per_frame))\n",
    "print(\"extracted_density_per_frame: {0}\".format(extracted_density_per_frame))\n",
    "print(sim_names)\n",
    "\n",
    "#convert the true_density_per_frame and extracted_density_per_frame to numpy arrays and to nm^2 by dividing by 130^2\n",
    "true_density_per_frame = np.array(true_density_per_frame)/0.017\n",
    "extracted_density_per_frame = np.array(extracted_density_per_frame)/0.017\n",
    "#find the ratio of the true_density_per_frame and extracted_density_per_frame\n",
    "density_ratio = true_density_per_frame/extracted_density_per_frame\n",
    "#in density_ratio, replace all the nan values with 1\n",
    "density_ratio[np.isnan(density_ratio)] = -1\n",
    "print(density_ratio)\n",
    "##for each sim, plot the p val of the KS test for the tavg_t1_msds for true and extracted as a function of the density\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "p_val_ks = np.zeros(len(sim_maps))\n",
    "\n",
    "for i in range(len(sim_maps)):\n",
    "    p_val_ks[i] = stats.ks_2samp(list(true_tavg_t1_msds[i].values()),list(extracted_tavg_t1_msds[i].values()),alternative='two-sided')[1]\n",
    "    ax.scatter(true_density_per_frame[i],p_val_ks[i],label=sim_names[i],marker=sim_markers[i],c=sim_colors[i])\n",
    "\n",
    "#annotate the last point\n",
    "#ax.annotate(sim_names[-1],(true_density_per_frame[-1],stats.ks_2samp(list(true_tavg_t1_msds[-1].values()),list(extracted_tavg_t1_msds[-1].values()))[1]))\n",
    "ax.set_xlabel(\"true density (points per um^2))\")\n",
    "ax.set_ylabel(\"p value of KS test\")\n",
    "# ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "#plot a line at p=0.05\n",
    "ax.plot([np.min(np.array(true_density_per_frame)),np.max(np.array(true_density_per_frame))],[0.05,0.05],c=\"black\",linestyle=\"--\")\n",
    "#ax.legend()\n",
    "plt.show()\n",
    "print(\"true_density_per_frame: {0}\".format(np.array(true_density_per_frame)))\n",
    "\n",
    "for i in range(len(sim_maps)):\n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.hist(list(true_tavg_t1_msds[i].values()),bins=50,label=\"true\",alpha=0.5)\n",
    "    ax.hist(list(extracted_tavg_t1_msds[i].values()),bins=50,label=\"extracted\",alpha=0.5)\n",
    "    ax.set_xlabel(\"tavg_t1_msd\")\n",
    "    ax.set_ylabel(\"frequency\")\n",
    "    ax.set_title(sim_names[i])\n",
    "    #put a text box in the plot with the p value of the KS test\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    ax.text(0.05, 0.95, \"p val: {0}\".format(stats.ks_2samp(list(true_tavg_t1_msds[i].values()),list(extracted_tavg_t1_msds[i].values()),alternative='two-sided')[1]), transform=ax.transAxes, fontsize=14)\n",
    "    #make a test to show the linking_errors[i], and point_pair_errors[i]\n",
    "    ax.text(0.05, 0.85, \"linking error: {0}\".format(linking_errors[i]), transform=ax.transAxes, fontsize=14)\n",
    "    ax.text(0.05, 0.75, \"point pair detected: {0}\".format(point_pair_errors[i]), transform=ax.transAxes, fontsize=14)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#plot the ratio of the true density to the extracted density\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "for i in range(len(sim_maps)):\n",
    "    print(\"true_density_per_frame: {0}, density_ratio: {1}\".format(true_density_per_frame[i],density_ratio[i]))\n",
    "    ax.scatter(true_density_per_frame[i],density_ratio[i],label=sim_names[i],marker=sim_markers[i],c=sim_colors[i])\n",
    "#annotate the last point with the name of the sim and a custom text \n",
    "#ax.annotate(str(sim_names[-1]) + \"\\n Extracted density undefined \\n Only one point to use\",(true_density_per_frame[-1],density_ratio[-1]),xytext=(true_density_per_frame[-1]*0.1,density_ratio[-1]*1.5),arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "ax.set_xlabel(\"true density (points per um^2))\")\n",
    "ax.set_ylabel(\"ratio of true density to extracted density\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "#ax.legend()\n",
    "plt.show()\n",
    "print(\"true_density_per_frame: {0}\".format(np.array(true_density_per_frame)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a colormap from true_density_per_frame\n",
    "true_density_per_frame = np.array(true_density_per_frame)\n",
    "\n",
    "colormap = cm.jet\n",
    "normalize = mcolors.Normalize(vmin=np.min(np.log10(true_density_per_frame)), vmax=np.max(np.log10(true_density_per_frame)))\n",
    "s_map = cm.ScalarMappable(norm=normalize, cmap=colormap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the true_points_per_frame and the extracted_points_per_frame for a single sim_map defines as n\n",
    "n=4\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "#sort by the frame number before plotting\n",
    "true_points_per_frame[n] = {k: v for k, v in sorted(true_points_per_frame[n].items(), key=lambda item: item[0])}\n",
    "extracted_points_per_frame[n] = {k: v for k, v in sorted(extracted_points_per_frame[n].items(), key=lambda item: item[0])}\n",
    "#plot using the marker shape defined in sim_markers\n",
    "\n",
    "plt.plot(list(true_points_per_frame[n].keys()),list(true_points_per_frame[n].values()),label=\"True_{0}\".format(sim_names[n]),marker=sim_markers[n],color=sim_colors[n])\n",
    "plt.plot(list(extracted_points_per_frame[n].keys()),list(extracted_points_per_frame[n].values()),label=\"Extracted_{0}\".format(sim_names[n]),marker=sim_markers[n],color=sim_colors[n])\n",
    "\n",
    "plt.xlabel(\"Frame\")\n",
    "plt.ylabel(\"Number of points\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the point_pair_errors as a function of the stored_data[:,2]\n",
    "#label the points with the sim_names\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "for i in range(len(sim_maps)):\n",
    "    ax.scatter(stored_data[i,2],point_pair_errors[i],color=s_map.to_rgba(np.log10(true_density_per_frame)[i]))#,label=sim_names[i],marker=sim_markers[i])\n",
    "ax.set_xlabel(\"percent of points detected\")\n",
    "ax.set_ylabel(\"percent of point pairs detected\")\n",
    "#annotate the last point with the name of the sim and a custom text\n",
    "#plot a 1:1 line for reference \n",
    "# ax.plot([40,100],[40,100],color=\"black\", label = \"1:1 line\", linestyle = \"--\", linewidth = 1)\n",
    "#make the axis the same aspact ratio\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "#make the limits of the axis the same  \n",
    "ax.set_xlim(0,100)\n",
    "ax.set_ylim(0,100)\n",
    "\n",
    "#ax.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plot the point_pair_errors as a function of the linking errors\n",
    "#label the points with the sim_names\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "#make a colormap from true_density_per_frame\n",
    "true_density_per_frame = np.array(true_density_per_frame)\n",
    "\n",
    "colormap = cm.jet\n",
    "normalize = mcolors.Normalize(vmin=np.min(np.log10(true_density_per_frame)), vmax=np.max(np.log10(true_density_per_frame)))\n",
    "s_map = cm.ScalarMappable(norm=normalize, cmap=colormap)\n",
    "\n",
    "for i in range(len(sim_maps)):\n",
    "    print(\"linking_errors: {0}, point_pair_errors: {1}, true_density_per_frame: {2}, sim_names: {3}\".format(linking_errors[i],point_pair_errors[i],true_density_per_frame[i],sim_names[i]))\n",
    "    if p_val_ks[i] > 0.02:\n",
    "        ax.scatter(linking_errors[i],point_pair_errors[i],label=sim_names[i],color=s_map.to_rgba(np.log10(true_density_per_frame)[i]),marker ='o',alpha=1)#,marker=sim_markers[i])\n",
    "\n",
    "ax.set_xlabel(\"percent linking error\")\n",
    "ax.set_ylabel(\"percent of point pairs detected\")\n",
    "\n",
    "#plot two lines, one horizontal at 50% and one vertical at 50%\n",
    "ax.plot([0,100],[50,50],color=\"black\", linestyle = \"--\", linewidth = 1)\n",
    "ax.plot([50,50],[0,100],color=\"black\", linestyle = \"--\", linewidth = 1)\n",
    "\n",
    "#in the first quadrant annotate a text with (\"Good detection\",\"Good linking\")\n",
    "ax.text(30,60,\"Good detection\\nGood linking\",fontsize=12)\n",
    "#in the left bottom quadrant annotate a text with (\"Poor detection\",\"Good linking\")\n",
    "ax.text(30,40,\"Poor detection\\nGood linking\",fontsize=12)\n",
    "#in the right top quadrant annotate a text with (\"Good detection\",\"Poor linking\")\n",
    "ax.text(70,60,\"Good detection\\nPoor linking\",fontsize=12)\n",
    "#in the right bottom quadrant annotate a text with (\"Poor detection\",\"Poor linking\")\n",
    "ax.text(70,40,\"Poor detection\\nPoor linking\",fontsize=12)\n",
    "\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "#make the limits of the axis the same  \n",
    "ax.set_xlim(0,100)\n",
    "ax.set_ylim(0,100)\n",
    "\n",
    "#ax.legend()\n",
    "plt.show()\n",
    "\n",
    "color_bars = s_map.to_rgba(np.log10(true_density_per_frame))\n",
    "plt.figure(figsize=(9, 1.5))\n",
    "img = plt.imshow(color_bars, cmap=cm.jet)\n",
    "plt.gca().set_visible(False)\n",
    "cax = plt.axes([0.1, 0.2, 0.8, 0.6])\n",
    "plt.colorbar(orientation=\"horizontal\", cax=cax)\n",
    "\n",
    "#make a new list ordered by true_density_per_frame and sim_names\n",
    "#first sort the true_density_per_frame\n",
    "true_density_per_frame = np.array(true_density_per_frame)\n",
    "true_density_per_frame_sorted = np.sort(true_density_per_frame)\n",
    "#then sort the sim_names\n",
    "sim_names_sorted = [x for _,x in sorted(zip(true_density_per_frame,sim_names))]\n",
    "for i in range(len(sim_names_sorted)):\n",
    "    print(\"{0}: {1}\".format(sim_names_sorted[i],true_density_per_frame_sorted[i]))\n",
    "print(\"max: {0}\".format(np.max(true_density_per_frame)))\n",
    "print(\"min: {0}\".format(np.min(true_density_per_frame)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Density linking and identiy interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing for interpolation of x,y (linking_errors,point_pair_errors) values to z (true_density_per_frame)\n",
    "#we will use the depricated scipy.interpolate.interp2d\n",
    "from scipy import interpolate\n",
    "f = interpolate.interp2d(linking_errors, point_pair_errors, true_density_per_frame, kind='quintic')\n",
    "#test this out by plotting intropolated values on a grid of linking_errors and point_pair_errors from (0,50) in steps of 1\n",
    "#make a grid of linking_errors and point_pair_errors\n",
    "linking_errors_grid = np.arange(0,100,1)\n",
    "point_pair_errors_grid = np.arange(0,100,1)\n",
    "#evaluate the function on the grid\n",
    "true_density_per_frame_grid = f(linking_errors_grid,point_pair_errors_grid)\n",
    "\n",
    "#plot grid on a contour plot\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "a1 = ax.contourf(linking_errors_grid,point_pair_errors_grid,true_density_per_frame_grid)\n",
    "ax.set_xlabel(\"percent linking error\")\n",
    "ax.set_ylabel(\"percent of point pairs detected\")\n",
    "#plot two lines, one horizontal at 50% and one vertical at 50%\n",
    "ax.plot([0,100],[50,50],color=\"black\", linestyle = \"--\", linewidth = 1)\n",
    "ax.plot([50,50],[0,100],color=\"black\", linestyle = \"--\", linewidth = 1)\n",
    "plt.colorbar(a1)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#conclusion: this doesn't work well, the contour plot is not smooth and the values are not correct. We need more data points to make this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the fit_diff_error vs the fit_alpha_error for all sim_names\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "for i in range(len(sim_maps)):\n",
    "    ax.scatter(fit_diff_error[i],fit_alpha_error[i],label=sim_names[i],marker=sim_markers[i],c=sim_colors[i])\n",
    "ax.set_xlabel(\"Ratio of extracted diffusion coefficient over true\")\n",
    "ax.set_ylabel(\"Percent error in alpha\")\n",
    "ax.set_xlim(0,5)\n",
    "ax.set_ylim(0,100)\n",
    "#ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the hist of true_tavg_t1_msds and the hist of extracted_tavg_t1_msds for a single sim_map defines as n\n",
    "n=0\n",
    "print(sim_names)\n",
    "plt.clf()\n",
    "plt.hist(np.log10(list(true_tavg_t1_msds[n].values())),bins=15,label=\"True_{0}\".format(sim_names[n]),alpha=0.5)\n",
    "plt.hist(np.log10(list(extracted_tavg_t1_msds[n].values())),bins=15,label=\"Extracted_{0}\".format(sim_names[n]),alpha=0.5)\n",
    "plt.xlabel(\"MSD\")\n",
    "plt.ylabel(\"Number of points\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#perform a KS test on the two distributions\n",
    "print(stats.ks_2samp(list(true_tavg_t1_msds[n].values()),list(extracted_tavg_t1_msds[n].values())))\n",
    "#perform a mann whitney test on the two distributions\n",
    "print(stats.mannwhitneyu(list(true_tavg_t1_msds[n].values()),list(extracted_tavg_t1_msds[n].values())))\n",
    "#perform a t test on the two distributions\n",
    "print(stats.ttest_ind(list(true_tavg_t1_msds[n].values()),list(extracted_tavg_t1_msds[n].values())))\n",
    "\n",
    "#find the difference in the mean of the two distributions\n",
    "print(np.abs(np.mean(list(true_tavg_t1_msds[n].values()))-np.mean(list(extracted_tavg_t1_msds[n].values()))))\n",
    "#covert from pixels^2 to nm^2 assuming 79 nm pixels\n",
    "print(np.abs(np.mean(list(true_tavg_t1_msds[n].values()))-np.mean(list(extracted_tavg_t1_msds[n].values())))*79**2)\n",
    "#sqrt to get in nm\n",
    "print(np.sqrt(np.abs(np.mean(list(true_tavg_t1_msds[n].values()))-np.mean(list(extracted_tavg_t1_msds[n].values())))*79**2)/2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#plot the cdf of true_tavg_t1_msds and the cdf of extracted_tavg_t1_msds for a single sim_map defines as n\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(np.sort(list(true_tavg_t1_msds[n].values())),np.linspace(0,1,len(list(true_tavg_t1_msds[n].values()))),label=\"True_{0}\".format(sim_names[n]))\n",
    "plt.plot(np.sort(list(extracted_tavg_t1_msds[n].values())),np.linspace(0,1,len(list(extracted_tavg_t1_msds[n].values()))),label=\"Extracted_{0}\".format(sim_names[n]))\n",
    "plt.xlabel(\"MSD\")\n",
    "plt.ylabel(\"CDF\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#create a box plot of a single sim_map defined as n\n",
    "plt.clf()\n",
    "plt.boxplot([list(true_tavg_t1_msds[n].values()),list(extracted_tavg_t1_msds[n].values())],labels=[\"True_{0}\".format(sim_names[n]),\"Extracted_{0}\".format(sim_names[n])])\n",
    "plt.ylabel(\"MSD\")\n",
    "plt.show()\n",
    "\n",
    "#find the average distance between the cdf calculated above\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each sim, plot the p val of the KS test for the tavg_t1_msds for true and extracted\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "for i in range(len(sim_maps)):\n",
    "    ax.scatter(i,stats.ks_2samp(list(true_tavg_t1_msds[i].values()),list(extracted_tavg_t1_msds[i].values()))[1],label=sim_names[i],marker=sim_markers[i],c=sim_colors[i])\n",
    "ax.set_ylabel(\"p value to reject null hypothesis of same distribution \\n (lower than 0.05 means different distributions)\")\n",
    "ax.set_xticks(range(len(sim_names)))\n",
    "ax.set_xticklabels(range(len(sim_names)))\n",
    "ax.set_yscale(\"log\")\n",
    "#make a horizontal line at 0.05\n",
    "ax.plot([0,len(sim_names)],[0.05,0.05],color=\"black\", linestyle = \"--\", linewidth = 1)\n",
    "ax.set_title(\"KS test of true and extracted tavg_t1_msds\")\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we investigate the effects of different sample sizes in distributions when comparing KS p-vals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we investigate the applicability of the KS test for distribution comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#draw two samples from a normal distribution with mean N and standard deviation sigma, and samples of size K,L\n",
    "N = 100\n",
    "sigma = 10\n",
    "K = 100\n",
    "#let L range from 1 to 1000 in steps of 50\n",
    "L = np.arange(10,100,5)\n",
    "repeats = 20000\n",
    "#for each L, draw L samples for one distribution and K samples for the other distribution for each repeat\n",
    "sample_k = np.random.normal(N,sigma,K)\n",
    "\n",
    "p_val_below_05 = np.zeros(len(L))\n",
    "for i in range(len(L)):\n",
    "    for j in range(repeats):\n",
    "        sample_l = np.random.normal(N,sigma,L[i])\n",
    "        p_val_below_05[i] += stats.ks_2samp(sample_k,sample_l)[1]<0.05\n",
    "#divide by the number of repeats to get the probability\n",
    "\n",
    "\n",
    "p_val_below_05 = p_val_below_05/repeats\n",
    "plt.clf()\n",
    "plt.plot(L,p_val_below_05,label = \"Same distribution\")\n",
    "\n",
    "#repeat the above but with sample_k using N+10\n",
    "sample_k = np.random.normal(N+10,sigma,K)\n",
    "p_val_below_05 = np.zeros(len(L))\n",
    "for i in range(len(L)):\n",
    "    for j in range(repeats):\n",
    "        sample_l = np.random.normal(N,sigma,L[i])\n",
    "        p_val_below_05[i] += stats.ks_2samp(sample_k,sample_l)[1]<0.05\n",
    "#divide by the number of repeats to get the probability\n",
    "p_val_below_05 = p_val_below_05/repeats\n",
    "plt.plot(L,p_val_below_05,label = \"Different distribution\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Number of samples in second distribution\")\n",
    "plt.ylabel(\"Probability of p value below 0.05\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we test how many simulated track of varying sizes we need to accurately estimate the alpha/diffusion values when fitting the tavg msd with a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "h = 0.2\n",
    "diff = 0.01\n",
    "inital = np.array([100,100])\n",
    "mean_n = [4,5,6,7,8,9,10]#np.array(list(np.arange(4,20,4)))\n",
    "dims = (200,200)\n",
    "repeats = [200]#np.arange(50,250,50)\n",
    "\n",
    "#hacky \n",
    "def cell_params_util(track_num):\n",
    "    cell_parms = {\n",
    "        \"diffusion_coefficients\":diff*np.ones(track_num),\n",
    "        \"initials\":np.ones((track_num,2))*inital,#np.random.uniform(50,150,(200,2)),\n",
    "        \"num_tracks\":track_num,\n",
    "        \"track_type\":\"fbm\",\n",
    "        \"hursts\":h*np.ones(track_num),\n",
    "        \"dims\":dims,\n",
    "        \"movie_frames\":500\n",
    "    }\n",
    "    return cell_parms\n",
    "\n",
    "\n",
    "def sim_params_util(track_num):\n",
    "    sim_params = {\n",
    "        \"track_length_mean\":track_num,\n",
    "        \"track_distribution\":\"constant\",\n",
    "        \"exposure_time\":20,#ms same as frame_time\n",
    "        \"base_noise\":140,\n",
    "        \"point_intensity\":20,\n",
    "        \"psf_sigma\":0.82,\n",
    "        \"frame_time\":20,#ms\n",
    "        \"pixel_size\":globals[\"confocal_pixel_size\"],\n",
    "    }\n",
    "    return sim_params\n",
    "\n",
    "\n",
    "error_alpha_all = np.zeros((len(mean_n),len(repeats)))\n",
    "error_diff_all = np.zeros((len(mean_n),len(repeats)))\n",
    "\n",
    "mean_alpha_all = np.zeros(len(mean_n))\n",
    "mean_diff_all = np.zeros(len(mean_n))\n",
    "interval_alpha_all = np.zeros((len(mean_n)))\n",
    "interval_diff_all = np.zeros((len(mean_n)))\n",
    "\n",
    "for i in range(len(mean_n)):\n",
    "    error_alpha_per_mean_n = np.zeros(len(repeats))\n",
    "    error_diff_per_mean_n = np.zeros(len(repeats))\n",
    "    for j in range(len(repeats)):\n",
    "        #get the cell_params\n",
    "        cell_params = cell_params_util(repeats[j])\n",
    "        #get the sim_params\n",
    "        sim_params = sim_params_util(mean_n[i])\n",
    "        #run the sim\n",
    "        a = simulate_cells.Simulate_cells(cell_params,sim_params)\n",
    "        tracks = a.get_cell()[\"tracks\"]\n",
    "        true_tracks = a._convert_track_dict_msd(tracks)\n",
    "        msd_true = msd_calc(true_tracks,h=None,tau_lim=40,tick_space=2,save=False,cd=\"\",data_type=\"True\",plot=True,msd_fit_lim=3)\n",
    "        if msd_true[\"fit_ens\"] != None:\n",
    "            error_alpha = percent_error(h*2,msd_true[\"fit_ens\"][0][0],abs=True)\n",
    "            error_alpha_per_mean_n[j] = error_alpha\n",
    "            #find the percent error between true and extracted diffusion coefficients\n",
    "            error_diff = msd_true[\"fit_ens\"][0][1]/np.log10(diff)\n",
    "            error_diff_per_mean_n[j] = error_diff\n",
    "        #store the tavg_t1_msds\n",
    "        true_tavg_t1_msds = np.array(list(msd_true[\"tavg_t1_msd\"].values()))\n",
    "        track_alpha = np.array(list(msd_true[\"track_alpha\"].values()))\n",
    "        msd_interval_95 = st.t.interval(0.95, len(true_tavg_t1_msds)-1, loc=np.mean(true_tavg_t1_msds), scale=st.sem(true_tavg_t1_msds))\n",
    "        alpha_interval_95 = st.t.interval(0.95, len(track_alpha)-1, loc=np.mean(track_alpha), scale=st.sem(track_alpha))\n",
    "        print(\"msd_interval_95 = \"+str(msd_interval_95))\n",
    "        print(\"alpha_interval_95 = \"+str(alpha_interval_95))\n",
    "        mean_alpha_all[i] = np.mean(track_alpha)\n",
    "        print(\"len alpha = \"+str(len(track_alpha)))\n",
    "        print(\"mean alpha = \"+str(mean_alpha_all[i]))\n",
    "        mean_diff_all[i] = np.mean(true_tavg_t1_msds)\n",
    "        interval_alpha_all[i] = range_distance(alpha_interval_95[1],alpha_interval_95[0])\n",
    "        interval_diff_all[i] = range_distance(msd_interval_95[1],msd_interval_95[0])\n",
    "\n",
    "    error_alpha_all[i] = error_alpha_per_mean_n\n",
    "    error_diff_all[i] = error_diff_per_mean_n\n",
    "    print(\"Done with mean_n = \"+str(mean_n[i]))\n",
    "\n",
    "#plot N lines for each repeats. x axis is mean_n, y axis is error_alpha_all\n",
    "plt.clf()\n",
    "for i in range(len(repeats)):\n",
    "    plt.plot(mean_n,error_alpha_all[:,i],label = \"repeats = \"+str(repeats[i]))\n",
    "plt.xlabel(\"Mean track length\")\n",
    "plt.ylabel(\"Percent error in alpha\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#plot N lines for each repeats. x axis is mean_n, y axis is error_diff_all\n",
    "plt.clf()\n",
    "for i in range(len(repeats)):\n",
    "    plt.plot(mean_n,error_diff_all[:,i],label = \"repeats = \"+str(repeats[i]))\n",
    "plt.xlabel(\"Mean track length\")\n",
    "plt.ylabel(\"Percent error in diffusion coefficient\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each mean_n plot the mean error_alpha_all with confidence interval interval_alpha_all as a filled area\n",
    "plt.clf()\n",
    "plt.plot(mean_n,mean_alpha_all,label = \"Mean alpha\")\n",
    "plt.fill_between(mean_n, mean_alpha_all-interval_alpha_all, mean_alpha_all+interval_alpha_all, alpha=0.5, label = \"95% confidence interval\")\n",
    "plt.xlabel(\"track length\")\n",
    "plt.ylabel(\"alpha\")\n",
    "#plot a horizontal line at h*2\n",
    "plt.axhline(y=h*2, color='r', linestyle='--')\n",
    "plt.legend()\n",
    "#set ylim to 0,2\n",
    "plt.ylim(0,2)\n",
    "\n",
    "plt.show()\n",
    "print(mean_alpha_all)\n",
    "\n",
    "def convert_diff(n):\n",
    "    m = n*(79**2)/(1e6) *(1000./20.)\n",
    "    return m\n",
    "#repeat for mean_diff_all\n",
    "plt.clf()\n",
    "plt.plot(mean_n,convert_diff(mean_diff_all),label = \"Mean diffusion coefficient\")\n",
    "plt.fill_between(mean_n, convert_diff(mean_diff_all-interval_diff_all), convert_diff(mean_diff_all+interval_diff_all), alpha=0.5, label = \"95% confidence interval\")\n",
    "plt.xlabel(\"track length\")\n",
    "plt.ylabel(\"diffusion coefficient\")\n",
    "#plot a horizontal line at diff\n",
    "plt.axhline(y=0.01, color='r', linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#plot the interval_alpha_all and interval_diff_all as a function of mean_n\n",
    "plt.clf()\n",
    "plt.plot(mean_n,100-100*(mean_alpha_all-interval_alpha_all)/mean_alpha_all,label = \"Alpha\")\n",
    "plt.plot(mean_n,100-100*(mean_diff_all-interval_diff_all)/mean_diff_all,label = \"Diffusion coefficient\")\n",
    "plt.xlabel(\"track length\")\n",
    "plt.ylabel(\"Percent difference of 95% confidence interval and mean\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.arange(4,10,1)\n",
    "\n",
    "def interval_per_tau(n):\n",
    "    n_len = n\n",
    "    max_tau = n_len-1\n",
    "    taus = np.arange(1,max_tau,1)\n",
    "    intervals = n_len-taus\n",
    "    return intervals, taus\n",
    "\n",
    "#for each n, plot the interval_per_tau as a function of tau\n",
    "plt.clf()\n",
    "for i in range(len(n)):\n",
    "    intervals, taus = interval_per_tau(n[i])\n",
    "    plt.plot(taus,intervals,label = \"n = \"+str(n[i]))\n",
    "plt.xlabel(\"tau\")\n",
    "plt.ylabel(\"Samples per Tau\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we want to test the issue of the line fitting via log-log plot and exponential fit.\n",
    "To do this we define the mean and variance of the interval process as 0 and (s^2)*h^(2H)\n",
    "letting s=1 for convinience \n",
    "\n",
    "Now for intervals h on 1-n we sample the square of the interval process(MSD) from the distribution above and fit the curve as an exponential to find H or as a log-log line. To do this we need to define the variance of the (MSD) as the variance of the variance of the interval process. For simplicity we assume it to be linear in h^2H."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=1\n",
    "h_n=10\n",
    "h_0=1\n",
    "h = np.arange(h_0,h_n,1)\n",
    "H=0.2\n",
    "k=3\n",
    "repeats = 1\n",
    "\n",
    "interval_process = np.zeros(len(h))\n",
    "for i in range(len(h)):\n",
    "    #sample from a gaussian distribution with mean 0 and standard deviation h[i]^(2H)\n",
    "    interval_process[i] = np.random.normal(h[i]**(2*H),2,size=repeats).mean()\n",
    "\n",
    "#fit the first k points of the interval_process to a linear function after taking the log of both sides\n",
    "\n",
    "def linear_func(x,a,b):\n",
    "    return a*x+b\n",
    "\n",
    "popt, pcov = curve_fit(linear_func, np.log(h[:k]), np.log(interval_process[:k]))\n",
    "print(popt)\n",
    "print(pcov)\n",
    "\n",
    "def power_func(x,b,a):\n",
    "    return a*x**b\n",
    "\n",
    "popt_power, pcov_power = curve_fit(power_func, h[:k], interval_process[:k])\n",
    "print(popt)\n",
    "print(pcov)\n",
    "\n",
    "\n",
    "#plot the interval_process as a function of h first with a linear scale and then with a log-log scale\n",
    "plt.clf()\n",
    "plt.plot(h,interval_process)\n",
    "#plot the linear fit\n",
    "plt.plot(h, np.exp(linear_func(np.log(h),*popt)), 'r-', label='fit: a=%5.3f' % tuple(popt)[0])\n",
    "plt.legend()\n",
    "plt.xlabel(\"h\")\n",
    "plt.ylabel(\"Interval process\")\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(h,interval_process)\n",
    "#plot the power fit with the pcov error\n",
    "plt.plot(h, power_func(h,*popt_power), 'r-', label='fit: a=%5.3f' % tuple(popt_power)[0])\n",
    "plt.xlabel(\"log(h)\")\n",
    "plt.ylabel(\"log(Interval process)\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show() \n",
    "\n",
    "\n",
    "process_repeats = 200\n",
    "alpha_fits_linear = np.zeros(process_repeats)\n",
    "alpha_fit_errors_linear = np.zeros(process_repeats)\n",
    "alpha_fits_power = np.zeros(process_repeats)\n",
    "alpha_fit_errors_power = np.zeros(process_repeats)\n",
    "for j in range(process_repeats):\n",
    "    interval_process = np.zeros(len(h))\n",
    "    for i in range(len(h)):\n",
    "        #sample from a gaussian distribution with mean 0 and standard deviation h[i]^(2H)\n",
    "        interval_process[i] = np.random.normal(h[i]**(2*H),2,size=repeats).mean()\n",
    "    \n",
    "    #fit the first k points of the interval_process to a linear function after taking the log of both sides\n",
    "    log_h = np.log(h[:k])\n",
    "    log_interval_process = np.log(interval_process[:k])\n",
    "    #replace the nan values with 0\n",
    "    log_h[np.isnan(log_h)] = 0\n",
    "    log_interval_process[np.isnan(log_interval_process)] = 0\n",
    "    \n",
    "    popt_linear, pcov_linear = curve_fit(linear_func, log_h, log_interval_process,maxfev=10000)\n",
    "    #repeat for power fit\n",
    "    popt_power, pcov_power = curve_fit(power_func, h[:k], interval_process[:k],maxfev=10000,p0=(0.4,1))\n",
    "    #save the alpha fit and the error\n",
    "    alpha_fits_linear[j] = popt_linear[0]\n",
    "    alpha_fit_errors_linear[j] = np.sqrt(pcov_linear[0][0])\n",
    "    alpha_fits_power[j] = popt_power[0]\n",
    "    alpha_fit_errors_power[j] = np.sqrt(pcov_power[0][0])\n",
    "\n",
    "#plot the histogram of the alpha fits\n",
    "plt.clf()\n",
    "#remove the outliers that are below -5 and above 5\n",
    "#alpha_fits_linear = alpha_fits_linear[(alpha_fits_linear>-5) & (alpha_fits_linear<5)]\n",
    "alpha_fits_power = alpha_fits_power[(alpha_fits_power>-5) & (alpha_fits_power<5)]\n",
    "plt.hist(alpha_fits_power, bins=20, label = \"Power fit\")\n",
    "#plot a vertical line at the mean\n",
    "plt.axvline(x=alpha_fits_power.mean(), color='r', linestyle='--', label = \"Mean = \"+str(alpha_fits_power.mean()))\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#repeat for linear fit\n",
    "plt.clf()\n",
    "plt.hist(alpha_fits_linear, bins=20, label = \"Linear fit\")\n",
    "#plot a vertical line at the mean\n",
    "plt.axvline(x=alpha_fits_linear.mean(), color='r', linestyle='--', label = \"Mean = \"+str(alpha_fits_linear.mean()))\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extras\n",
    "#msd_calc(cell_sim._convert_track_dict_msd(loaded_dict[\"tracks\"]),h=None,tau_lim=40,tick_space=2)\n",
    "# true_tracks = cell_sim._convert_track_dict_msd(loaded_dict[\"tracks\"])\n",
    "\n",
    "# fitted_tracks = track_dict[\"ALL\"]\n",
    "# track_match_dict = identity_track_matrix(true_tracks,fitted_tracks,threshold=0.5)\n",
    "\n",
    "# true_length = track_match_dict[\"true_track_lengths\"]\n",
    "# estimated_length = track_match_dict[\"estimate_track_lengths\"]\n",
    "\n",
    "\n",
    "\n",
    "# #plot the max_identity and the length errors, color the points by the length of the true track\n",
    "# plt.figure()\n",
    "# plt.scatter(track_match_dict[\"length_error\"],track_match_dict[\"max_identity\"],c=true_length,cmap='viridis',s=20,alpha=0.4)\n",
    "# plt.ylabel(\"max identity (0,1)\")\n",
    "# plt.xlabel(\"length error (percent)\")\n",
    "# #plot a vertical line at 0\n",
    "# plt.axvline(x=0, color='k', linestyle='--')\n",
    "\n",
    "# #make a zoomed inset of the plot from x-50 to x+50 and y-0.2 to y+0.2\n",
    "# axins = zoomed_inset_axes(plt.gca(), 1, loc=6) # zoom-factor: 2.5, location: upper-left\n",
    "# axins.scatter(track_match_dict[\"length_error\"],track_match_dict[\"max_identity\"],c=estimated_length,cmap='viridis',s=20,alpha=0.4)\n",
    "# axins.set_xlim(-500, 100) # apply the x-limits\n",
    "# axins.set_ylim(0.2, 1.2) # apply the y-limits\n",
    "# #make a colorbar for the inset\n",
    "# cbar = plt.colorbar()\n",
    "# cbar.ax.set_ylabel(\"track length\")\n",
    "# plt.yticks(visible=False)\n",
    "# plt.xticks(visible=False)\n",
    "# mark_inset(plt.gca(), axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n",
    "\n",
    "# #make a colorbar with ticks at the length of the true track\n",
    "# plt.show()\n",
    "\n",
    "# #plot the max_identity and the length errors matrix as a heatmap\n",
    "# plt.figure()\n",
    "# plt.imshow(track_match_dict[\"identity_matrix\"],cmap=\"gray\")\n",
    "# plt.ylabel(\"true track\")\n",
    "# plt.xlabel(\"fitted track\")\n",
    "# plt.show()\n",
    "\n",
    "# #plot the max_identity and the length errors matrix as a heatmap\n",
    "# #clear the figure \n",
    "# plt.clf()\n",
    "# plt.figure()\n",
    "# plt.imshow(track_match_dict[\"length_error_matrix\"],cmap=\"gray\")\n",
    "# plt.ylabel(\"true track\")\n",
    "# plt.xlabel(\"fitted track\")\n",
    "# plt.show()\n",
    "\n",
    "# #for each pair in lenght_error_matrix and identity_matrix plot it alongside the length error and max identity\n",
    "# #clear the figure\n",
    "# # plt.clf()\n",
    "# # plt.figure()\n",
    "# # for i in range(len(track_match_dict[\"length_error_matrix\"])):\n",
    "# #     for j in range(len(track_match_dict[\"length_error_matrix\"][i])):\n",
    "# #         if track_match_dict[\"length_error_matrix\"][i][j] != 0:\n",
    "# #             plt.plot(track_match_dict[\"length_error_matrix\"][i][j],track_match_dict[\"identity_matrix\"][i][j],\"r.\",alpha=0.1)\n",
    "# # plt.plot(track_match_dict[\"length_error\"],track_match_dict[\"max_identity\"],\"b.\",alpha=0.1)\n",
    "# # plt.ylabel(\"max identity (0,1)\")\n",
    "# # plt.xlabel(\"length error (percent)\")\n",
    "# # plt.show()\n",
    "\n",
    "# #print the mean length error and the mean max identity\n",
    "# print(\"mean length error: \",np.mean(np.abs(track_match_dict[\"length_error\"])))\n",
    "# print(\"mean max identity: \",np.mean(track_match_dict[\"max_identity\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysis of some confocal images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find files\n",
    "def find_files(path, extension, keyword = None):\n",
    "    '''\n",
    "    Docstring for find_files\n",
    "    Finds files in a directory with a specific extension and keyword in the name\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    path : str\n",
    "        path to the directory where the files are located\n",
    "    extension : str\n",
    "        extension of the files to be found\n",
    "    keyword : str    \n",
    "        keyword to be searched in the file name\n",
    "    Returns:\n",
    "    --------\n",
    "    files : list\n",
    "        list of files that match the criteria\n",
    "    '''\n",
    "    #find all images in the directory using import functions\n",
    "    files = find_image(path=path,ends_with=extension,full_path=True)\n",
    "    #sort the files to get only ones conatining the word \"RFP\" for the flourescent protein\n",
    "    files = name_sorter(strings=files,keyword=keyword)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run detection on a set of files \n",
    "def utility_batch_blob_detection(files,detection_args,fitting_args,focal_plane=None,project=False):\n",
    "    '''\n",
    "    Docstring for utility_batch_blob_detection\n",
    "    Runs blob detection on a set of files and returns the blobs in a list\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    files : list\n",
    "        list of files to be analyzed\n",
    "    detection_args : dict\n",
    "        dictionary of arguments for the blob detection\n",
    "    fitting_args : dict \n",
    "        dictionary of arguments for the blob fitting\n",
    "    focal_plane : int\n",
    "        focal plane to be analyzed in each image (default is None if there is only one plane)\n",
    "    project : bool\n",
    "        whether or not to project the image using max (default is False), focal_plane must be None if project is True\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    blobs : list\n",
    "        list of blobs detected in each image aggregated in a list\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    This doesn't care if the detection is verbose, the list may be a list of lists or a list of dictionaries, see blob_detection.detection for more details\n",
    "    '''\n",
    "    blobs=[]\n",
    "    for i in files:\n",
    "        try:\n",
    "            if project:\n",
    "                img = np.max(io.imread(i),axis=0)\n",
    "            else:\n",
    "                img = io.imread(i)[focal_plane]\n",
    "        except:\n",
    "            pass\n",
    "        blob_detector = blob_detection(path=img,**detection_args)\n",
    "        blob_detector._update_fitting_parameters(kwargs=fitting_args)\n",
    "        #detect blobs\n",
    "        c = blob_detector.detection(type=\"bp\")\n",
    "        #if c's shape's first element is 0, then there are no blobs detected and we don't want to add it to the list\n",
    "        if blob_detector.verbose:\n",
    "            if c[\"Fitted\"].shape[0] != 0:\n",
    "                blobs.append(c[\"Fitted\"])\n",
    "        else:\n",
    "            if c.shape[0] != 0:\n",
    "                blobs.append(c)\n",
    "            \n",
    "    return blobs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test the blob detection on hupA images and generic RpoC images from the confocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of paths of hupa images\n",
    "hupa_dir_list = [\"/Volumes/Baljyot_HD/Confocal_Data/HupA_confocal/191113_TC_WLBS81(HupA)/t120\",\n",
    "                \"/Volumes/Baljyot_HD/Confocal_Data/HupA_confocal/191205_TC_WLBS81(HupA)/t120\",\n",
    "                \"/Volumes/Baljyot_HD/Confocal_Data/HupA_confocal/191223_TC_WLBS81(HupA)/t120\",\n",
    "                \"/Volumes/Baljyot_HD/Confocal_Data/HupA_confocal/191230_TC_WLBS81(HupA)/t120\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory of AML's rpoc images\n",
    "aml_rpoc_dir = \"/Volumes/Baljyot_HD/Confocal_Data/HupA_confocal/thirty-seven/120\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using find_files, find hupa files\n",
    "#using all the hupa directories find all the files with the keyword \"RFP\"\n",
    "files_hupa = []\n",
    "for i in hupa_dir_list:\n",
    "    files_hupa+=find_files(path=i,extension=\".tif\",keyword=\"RFP\")\n",
    "    \n",
    "#using find_files, find rpoc files\n",
    "files_rpoc = find_files(path=\"/Volumes/Baljyot_HD/Confocal_Data/HupA_confocal/191205_TC_WLBS100(rpoC)/t120\",extension=\".tif\",keyword=\"RFP\")\n",
    "#find aml rpoc files\n",
    "files_aml_rpoc = find_files(path=aml_rpoc_dir,extension=\".tif\",keyword=\"RFP\")\n",
    "print(\"hupa files: \",files_hupa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the parameters for blob detection\n",
    "detection_args = {\"median\":False,\n",
    "                \"threshold\":1e-1,\n",
    "                \"min_sigma\":1./np.sqrt(2),\n",
    "                \"max_sigma\":10./np.sqrt(2),\n",
    "                \"num_sigma\":40,\n",
    "                \"overlap\":0,\n",
    "                \"logscale\":False,\n",
    "                \"verbose\":True}\n",
    "fitting_args = {\"mask_size\":4,\n",
    "                \"plot_fit\":False,\n",
    "                \"fitting_image\":\"Original\",\n",
    "                \"radius_func\":None,\n",
    "                \"residual_func\":residuals_gaus2d,\n",
    "                \"sigma_range\":0.5,\n",
    "                \"centeroid_range\":0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run utility_batch_blob_detection on the hupa files\n",
    "blobs_hupa = utility_batch_blob_detection(files_hupa,detection_args=detection_args,fitting_args=fitting_args,focal_plane=3,project=False)\n",
    "#run utility_batch_blob_detection on the rpoc files\n",
    "blobs_rpoc = utility_batch_blob_detection(files_rpoc,detection_args=detection_args,fitting_args=fitting_args,focal_plane=3,project=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#redo the detection on the AML rpoC images with detection args with threshold changed \n",
    "detection_args_AML = detection_args.copy()\n",
    "detection_args_AML[\"threshold\"] = 1e-1\n",
    "detection_args_AML[\"overlap\"] = 0\n",
    "#run utility_batch_blob_detection on the aml rpoc files\n",
    "blobs_aml_rpoc = utility_batch_blob_detection(files_aml_rpoc,detection_args=detection_args_AML,fitting_args=fitting_args,focal_plane=3,project=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_blob_sizes(blobs,verbose=False):\n",
    "    '''Return a flattened list of blob sizes\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    blobs : list\n",
    "        list of blobs detected in each image aggregated in a list\n",
    "    verbose : bool\n",
    "        True whether the blobs are Fitted and have sigma_x, and sigma_y. Or Flase if isotropic gaussian is used/Scale space (default is False)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    sizes : list\n",
    "        list of blob sizes\n",
    "    '''\n",
    "    if verbose:\n",
    "        return [np.concatenate([i[:,2] for i in blobs]).ravel(),np.concatenate([i[:,3] for i in blobs]).ravel()]\n",
    "    else:\n",
    "        return np.concatenate([i[:,2] for i in blobs]).ravel()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sizes of the blobs if verbose is True, if false do not run this cell\n",
    "sizes_hupa = get_blob_sizes(blobs_hupa,verbose=True)\n",
    "sizes_rpoc = get_blob_sizes(blobs_rpoc,verbose=True)\n",
    "sizes_aml_rpoc = get_blob_sizes(blobs_aml_rpoc,verbose=True)\n",
    "\n",
    "#for each, find the ratio of the x and y sigma\n",
    "ratio_hupa = sizes_hupa[0]/sizes_hupa[1]\n",
    "ratio_rpoc = sizes_rpoc[0]/sizes_rpoc[1]\n",
    "ratio_aml_rpoc = sizes_aml_rpoc[0]/sizes_aml_rpoc[1]\n",
    "\n",
    "#plot the ratio of the x and y sigma as a histogram with the bins=20\n",
    "#Make sure the weights are normalized\n",
    "\n",
    "#make a figure with two subplots\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,5))\n",
    "#redo the first plot on the first axis\n",
    "ax[0].hist(ratio_hupa,bins=20,weights=np.ones_like(ratio_hupa)/len(ratio_hupa),label=\"HupA\",alpha=0.2)\n",
    "ax[0].hist(ratio_rpoc,bins=20,weights=np.ones_like(ratio_rpoc)/len(ratio_rpoc),label=\"rpoC\",alpha=0.2)\n",
    "ax[0].hist(ratio_aml_rpoc,bins=20,weights=np.ones_like(ratio_aml_rpoc)/len(ratio_aml_rpoc),label=\"AML rpoC\",alpha=0.2)\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(r\"$\\frac{\\sigma_x}{\\sigma_y}$\",fontsize=20)\n",
    "ax[0].set_ylabel(\"Probability\")\n",
    "ax[0].set_title(\"Ratio of x and y sigma for HupA and rpoC blobs\")\n",
    "#redo the second plot on the second axis\n",
    "#find the mean of each x,y pair\n",
    "#make sure to divide by sqrt(2) because of the blob_detection output scales the sigma by sqrt(2)\n",
    "\n",
    "mean_hupa = np.mean(sizes_hupa,axis=0)/np.sqrt(2)\n",
    "mean_rpoc = np.mean(sizes_rpoc,axis=0)/np.sqrt(2)\n",
    "mean_aml_rpoc = np.mean(sizes_aml_rpoc,axis=0)/np.sqrt(2)\n",
    "\n",
    "#plot the mean of the x and y sigma vs the ratios calculated above, make sure to multiply the mean by the confocal pixel size to get the correct units in nm\n",
    "ax[1].scatter(mean_hupa*globals[\"confocal_pixel_size\"],ratio_hupa,label=\"HupA\")\n",
    "ax[1].scatter(mean_rpoc*globals[\"confocal_pixel_size\"],ratio_rpoc,label=\"rpoC\")\n",
    "ax[1].scatter(mean_aml_rpoc*globals[\"confocal_pixel_size\"],ratio_aml_rpoc,label=\"AML rpoC\")\n",
    "ax[1].legend()\n",
    "#write the the sigma as greek letters and make the font size of the labels bigger\n",
    "\n",
    "ax[1].set_ylabel(r\"$\\frac{\\sigma_x}{\\sigma_y}$\",fontsize=20)\n",
    "ax[1].set_xlabel(r\"$\\frac{\\sigma_x + \\sigma_y}{2}$ nm\",fontsize=20)\n",
    "ax[1].set_title(\"Mean of x and y sigma vs \\n Ratio of x and y sigma for HupA and rpoC blobs\")\n",
    "#make the plot look nice\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#find the absolute differences in the x and y sigmas for each blob\n",
    "diff_hupa = np.abs(sizes_hupa[0]-sizes_hupa[1])/np.sqrt(2)\n",
    "diff_rpoc = np.abs(sizes_rpoc[0]-sizes_rpoc[1])/np.sqrt(2)\n",
    "diff_aml_rpoc = np.abs(sizes_aml_rpoc[0]-sizes_aml_rpoc[1])/np.sqrt(2)\n",
    "\n",
    "#plot this as a scatter plot vs the mean of the x and y sigmas\n",
    "#make a figure with one subplot\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
    "#plot the scatter plot\n",
    "ax.scatter(mean_hupa*globals[\"confocal_pixel_size\"],diff_hupa*globals[\"confocal_pixel_size\"],label=\"HupA\")\n",
    "ax.scatter(mean_rpoc*globals[\"confocal_pixel_size\"],diff_rpoc*globals[\"confocal_pixel_size\"],label=\"rpoC\")\n",
    "ax.scatter(mean_aml_rpoc*globals[\"confocal_pixel_size\"],diff_aml_rpoc*globals[\"confocal_pixel_size\"],label=\"AML rpoC\")\n",
    "ax.legend()\n",
    "#write the the sigma as greek letters and make the font size of the labels bigger\n",
    "#the y label is the absolute difference of the x and y sigma\n",
    "ax.set_ylabel(r\"$\\left|\\sigma_x - \\sigma_y\\right|$ nm\",fontsize=20)\n",
    "ax.set_xlabel(r\"$\\frac{\\sigma_x + \\sigma_y}{2}$ nm\",fontsize=20)\n",
    "ax.set_title(\"Mean of x and y sigma vs \\n Absolute difference of x and y sigma for HupA and rpoC blobs\")\n",
    "#make the plot look nice\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the detection parameter verbose to False to see the progress of the blob detection\n",
    "detection_args[\"verbose\"] = False\n",
    "\n",
    "#run utility_batch_blob_detection on the hupa files\n",
    "blobs_hupa = utility_batch_blob_detection(files_hupa,detection_args=detection_args,fitting_args=fitting_args,focal_plane=3,project=False)\n",
    "#run utility_batch_blob_detection on the rpoc files\n",
    "blobs_rpoc = utility_batch_blob_detection(files_rpoc,detection_args=detection_args,fitting_args=fitting_args,focal_plane=3,project=False)\n",
    "#run utility_batch_blob_detection on the aml rpoc files\n",
    "blobs_aml_rpoc = utility_batch_blob_detection(files_aml_rpoc,detection_args=detection_args,fitting_args=fitting_args,focal_plane=3,project=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plot the two sizes on the same histogram plot with a legend and alpha transparency of 0.2, this does not work if verbose is True, do not run this cell if verbose is True\n",
    "sizes_hupa = get_blob_sizes(blobs_hupa,verbose=False)\n",
    "sizes_rpoc = get_blob_sizes(blobs_rpoc,verbose=False)\n",
    "sizes_aml_rpoc = get_blob_sizes(blobs_aml_rpoc,verbose=False)\n",
    "\n",
    "#make a figure with one subplot\n",
    "plt.clf()\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
    "#plot the histograms with the weights normaized to 1\n",
    "ax.hist(sizes_hupa*globals[\"confocal_pixel_size\"],bins=10,weights=np.ones_like(sizes_hupa)/len(sizes_hupa),label=\"HupA\",alpha=0.2)\n",
    "ax.hist(sizes_rpoc*globals[\"confocal_pixel_size\"],bins=10,weights=np.ones_like(sizes_rpoc)/len(sizes_rpoc),label=\"rpoC\",alpha=0.2)\n",
    "ax.hist(sizes_aml_rpoc*globals[\"confocal_pixel_size\"],bins=10,weights=np.ones_like(sizes_aml_rpoc)/len(sizes_aml_rpoc),label=\"AML rpoC\",alpha=0.2)\n",
    "\n",
    "#set the x and y labels\n",
    "ax.set_xlabel(\"Size of blob (nm)\")\n",
    "ax.set_ylabel(\"Probaility\")\n",
    "#add a title\n",
    "ax.set_title(\"Size of HupA and rpoC Blobs from Confocal Images\")\n",
    "#add an annotation for the mean and standard deviation for each distribution \n",
    "ax.annotate(\"HupA: $\\mu$ = {:.2f} nm, $\\sigma$ = {:.2f} nm\".format(np.mean(sizes_hupa*globals[\"confocal_pixel_size\"]),np.std(sizes_hupa*globals[\"confocal_pixel_size\"])),xy=(0.05,0.9),xycoords=\"axes fraction\")\n",
    "ax.annotate(\"rpoC: $\\mu$ = {:.2f} nm, $\\sigma$ = {:.2f} nm\".format(np.mean(sizes_rpoc*globals[\"confocal_pixel_size\"]),np.std(sizes_rpoc*globals[\"confocal_pixel_size\"])),xy=(0.05,0.85),xycoords=\"axes fraction\")\n",
    "ax.annotate(\"AML's rpoC: $\\mu$ = {:.2f} nm, $\\sigma$ = {:.2f} nm\".format(np.mean(sizes_aml_rpoc*globals[\"confocal_pixel_size\"]),np.std(sizes_aml_rpoc*globals[\"confocal_pixel_size\"])),xy=(0.05,0.75),xycoords=\"axes fraction\")\n",
    "#annotate the result of a ks test both the p value and the statistic\n",
    "ax.annotate(\"K-S Test: p_val = {:.2e}, D = {:.2f}\".format(stats.ks_2samp(sizes_hupa*globals[\"confocal_pixel_size\"],sizes_rpoc*globals[\"confocal_pixel_size\"])[1],stats.ks_2samp(sizes_hupa*globals[\"confocal_pixel_size\"],sizes_rpoc*globals[\"confocal_pixel_size\"])[0]),xy=(0.05,0.8),xycoords=\"axes fraction\")\n",
    "#make the legend in the middle right of the plot\n",
    "ax.legend(loc=\"center left\")\n",
    "plt.show()\n",
    "print(np.max(sizes_hupa*globals[\"confocal_pixel_size\"]))\n",
    "print(np.min(sizes_hupa*globals[\"confocal_pixel_size\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the mean and standard deviation of the hupa and rpoc blobs\n",
    "print(\"HupA: mean = {:.2f} nm, std = {:.2f} nm\".format(np.mean(sizes_hupa*globals[\"confocal_pixel_size\"]),np.std(sizes_hupa*globals[\"confocal_pixel_size\"])))\n",
    "print(\"rpoC: mean = {:.2f} nm, std = {:.2f} nm\".format(np.mean(sizes_rpoc*globals[\"confocal_pixel_size\"]),np.std(sizes_rpoc*globals[\"confocal_pixel_size\"])))\n",
    "#do the same for the aml rpoc blobs\n",
    "print(\"AML's rpoC: mean = {:.2f} nm, std = {:.2f} nm\".format(np.mean(sizes_aml_rpoc*globals[\"confocal_pixel_size\"]),np.std(sizes_aml_rpoc*globals[\"confocal_pixel_size\"])))\n",
    "#find the p value and the statistic of the ks test\n",
    "print(\"K-S Test: p_val = {:.2e}, D = {:.2f}\".format(stats.ks_2samp(sizes_hupa*globals[\"confocal_pixel_size\"],sizes_rpoc*globals[\"confocal_pixel_size\"])[1],stats.ks_2samp(sizes_hupa*globals[\"confocal_pixel_size\"],sizes_rpoc*globals[\"confocal_pixel_size\"])[0]))\n",
    "\n",
    "#test if the two rpoc distributions are from the same distribution\n",
    "print(\"K-S Test for rpoC and AML's rpoC: p_val = {:.2e}, D = {:.2f}\".format(stats.ks_2samp(sizes_rpoc*globals[\"confocal_pixel_size\"],sizes_aml_rpoc*globals[\"confocal_pixel_size\"])[1],stats.ks_2samp(sizes_rpoc*globals[\"confocal_pixel_size\"],sizes_aml_rpoc*globals[\"confocal_pixel_size\"])[0]))\n",
    "\n",
    "#test if AML's rpoC and HupA are from the same distribution\n",
    "print(\"K-S Test for AML's rpoC and HupA: p_val = {:.2e}, D = {:.2f}\".format(stats.ks_2samp(sizes_hupa*globals[\"confocal_pixel_size\"],sizes_aml_rpoc*globals[\"confocal_pixel_size\"])[1],stats.ks_2samp(sizes_hupa*globals[\"confocal_pixel_size\"],sizes_aml_rpoc*globals[\"confocal_pixel_size\"])[0]))\n",
    "\n",
    "#Statistics \n",
    "#perform a wilcoxon rank sum test on the hupa and rpoc blobs\n",
    "print(\"Wilcoxon Rank Sum Test: p_val = {:.2e}, W = {:.2f}\".format(stats.ranksums(sizes_hupa*globals[\"confocal_pixel_size\"],sizes_rpoc*globals[\"confocal_pixel_size\"])[1],stats.ranksums(sizes_hupa*globals[\"confocal_pixel_size\"],sizes_rpoc*globals[\"confocal_pixel_size\"])[0]))\n",
    "#perform a mann whitney u test on the hupa and rpoc blobs\n",
    "print(\"Mann Whitney U Test: p_val = {:.2e}, U = {:.2f}\".format(stats.mannwhitneyu(sizes_hupa*globals[\"confocal_pixel_size\"],sizes_rpoc*globals[\"confocal_pixel_size\"])[1],stats.mannwhitneyu(sizes_hupa*globals[\"confocal_pixel_size\"],sizes_rpoc*globals[\"confocal_pixel_size\"])[0]))\n",
    "\n",
    "#check if the hupa and rpoc blobs are normally distributed\n",
    "print(\"HupA: normal = {}, skew = {:.2f}, kurtosis = {:.2f}\".format(stats.normaltest(sizes_hupa*globals[\"confocal_pixel_size\"])[1]>0.05,stats.skew(sizes_hupa*globals[\"confocal_pixel_size\"]),stats.kurtosis(sizes_hupa*globals[\"confocal_pixel_size\"])))\n",
    "print(\"rpoC: normal = {}, skew = {:.2f}, kurtosis = {:.2f}\".format(stats.normaltest(sizes_rpoc*globals[\"confocal_pixel_size\"])[1]>0.05,stats.skew(sizes_rpoc*globals[\"confocal_pixel_size\"]),stats.kurtosis(sizes_rpoc*globals[\"confocal_pixel_size\"])))\n",
    "#check if the aml rpoc blobs are normally distributed\n",
    "print(\"AML's rpoC: normal = {}, skew = {:.2f}, kurtosis = {:.2f}\".format(stats.normaltest(sizes_aml_rpoc*globals[\"confocal_pixel_size\"])[1]>0.05,stats.skew(sizes_aml_rpoc*globals[\"confocal_pixel_size\"]),stats.kurtosis(sizes_aml_rpoc*globals[\"confocal_pixel_size\"])))\n",
    "\n",
    "#find the number of blobs in the hupa and rpoc files\n",
    "print(\"HupA: number of blobs = {}\".format(len(sizes_hupa)))\n",
    "print(\"rpoC: number of blobs = {}\".format(len(sizes_rpoc)))\n",
    "\n",
    "#find the number of blobs in each file\n",
    "print(\"HupA: number of blobs per file = {}\".format([len(i) for i in blobs_hupa]))\n",
    "print(\"rpoC: number of blobs per file = {}\".format([len(i) for i in blobs_rpoc]))\n",
    "\n",
    "#create a QQ plot for both the hupa and rpoc blobs to check if they are normally distributed. Do this on the same plot\n",
    "plt.clf()\n",
    "stats.probplot(sizes_hupa*globals[\"confocal_pixel_size\"],dist=\"norm\",plot=plt)\n",
    "stats.probplot(sizes_rpoc*globals[\"confocal_pixel_size\"],dist=\"norm\",plot=plt)\n",
    "#do this for the aml rpoc blobs\n",
    "stats.probplot(sizes_aml_rpoc*globals[\"confocal_pixel_size\"],dist=\"norm\",plot=plt)\n",
    "#annotate the two datasets with rpoc and hupa labels, make the text bold and readable at a distance\n",
    "plt.annotate(\"HupA\",xy=(0.05,0.35),xycoords=\"axes fraction\",fontweight=\"bold\",fontsize=16)\n",
    "plt.annotate(\"rpoC\",xy=(0.05,0.18),xycoords=\"axes fraction\",fontweight=\"bold\",fontsize=16)\n",
    "plt.annotate(\"AML's rpoC\",xy=(0.05,0.01),xycoords=\"axes fraction\",fontweight=\"bold\",fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detection on specific image just to see what it looks like\n",
    "#image path\n",
    "path = \"/Volumes/Baljyot_HD/Sim_Data/sim_data_condensate_axial/h_0.2(100)0.35(100)_track_200_diff_0.1(100)0.01(100)_condensate_100-100_r_2.5/segmented/1_test_seg.tif\"\n",
    "#open this image using function from import_functions.py\n",
    "\n",
    "#make a maximum projection of the image\n",
    "max_proj = np.max(io.imread(path),axis=0)\n",
    "img = max_proj\n",
    "img = io.imread(path)\n",
    "#create a blob detector object\n",
    "detection_args = {\"median\":False,\n",
    "                \"threshold\":4e0,\n",
    "                \"min_sigma\":1.1/np.sqrt(2),\n",
    "                \"max_sigma\":5./np.sqrt(2),\n",
    "                \"num_sigma\":500,\n",
    "                \"overlap\":0,\n",
    "                \"logscale\":False,\n",
    "                \"verbose\":True}\n",
    "fitting_args = {\"mask_size\":5,\n",
    "                \"plot_fit\":False,\n",
    "                \"fitting_image\":\"Original\",\n",
    "                \"radius_func\":np.mean,\n",
    "                \"residual_func\":residuals_gaus2d,\n",
    "                \"sigma_range\":2,\n",
    "                \"centroid_range\":1}\n",
    "\n",
    "blob_detector = blob_detection(path=img,**detection_args)\n",
    "blob_detector._update_fitting_parameters(kwargs=fitting_args)\n",
    "#detect blobs\n",
    "c = blob_detector.detection(type=\"bp\")\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.imshow(img,cmap=\"gray\")\n",
    "print(c)\n",
    "for i in c[\"Scale\"]:\n",
    "    cir = plt.Circle((i[1],i[0]),i[2],fill = False)\n",
    "    ax.add_artist(cir)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a trajectory from FBM and then use the msd analysis to find the alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 1\n",
    "h = 0.5\n",
    "d = 2\n",
    "n = 100\n",
    "track_num = 100\n",
    "diff_coeff = 1\n",
    "#make track_num of tracks\n",
    "track_dic = {}\n",
    "for i in range(track_num):\n",
    "    track = fbm_utility.get_fbm_sample(l,h,d,n)\n",
    "    x,y = track[1][0]*np.sqrt(2*diff_coeff),track[1][1]*np.sqrt(2*diff_coeff)\n",
    "    track_dic[str(i)] = np.array([x,y]).T\n",
    "\n",
    "a = msd_calc(track_dic,h=h,tau_lim=None,tick_space=2)\n",
    "msds = a[\"track_diffusion\"]\n",
    "print(msds)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create simualtions of blobs of varying points and determine the shape in 3D\n",
    "How does the blob look like as you vary the number of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib widget\n",
    "def dim_dif_total(radi,points):\n",
    "    maps = {}\n",
    "    for i in points:\n",
    "        sim = simulate_foci.sim_foci(max_x = 50,\n",
    "                            min_x = 0,\n",
    "                            radius = radi,\n",
    "                            center = [25.,25.],\n",
    "                            total_points = int(i),\n",
    "                            density_dif = 100000000.0,\n",
    "                            pdf = simulate_foci.tophat_function_2d)\n",
    "        sim.uniform_uniform_blob = True\n",
    "        sim.psf_sigma = 0.82\n",
    "        sim.base_noise = 140\n",
    "        sim.point_intensity = 40\n",
    "        map,sim_xy = sim.simulate_point()\n",
    "        maps[i] = np.array(map)\n",
    "    return maps\n",
    "#radius to simulate\n",
    "radi = 1\n",
    "#number of points to simulate\n",
    "points = np.arange(1,1001,200)\n",
    "maps = dim_dif_total(radi,points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maps contains the Z values of the simulation space\n",
    "#find the X,Y values of the simulation space\n",
    "x = np.arange(0,50,1)\n",
    "y = np.arange(0,50,1)\n",
    "X,Y = np.meshgrid(x,y,indexing=\"ij\")\n",
    "#plot the maps in 3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "for i in maps:\n",
    "    #label each plot with the number of points simulated\n",
    "    ax.plot_surface(X,Y,maps[i],alpha=0.5,label=\"{}\".format(i))\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "plt.show()\n",
    "\n",
    "#project the maps onto the Z,Y plane at the center of the blob (X=25)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "for i in maps:\n",
    "    ax.plot(y,maps[i][25,:],label=\"{} Points\".format(i))\n",
    "ax.set_xlabel(\"Y\")\n",
    "ax.set_ylabel(\"Z\")\n",
    "#plot two vertical lines to show the size of the blob based on the radius\n",
    "ax.axvline(x=25-radi,color=\"black\",linestyle=\"--\",linewidth=1)\n",
    "ax.axvline(x=25+radi,color=\"black\",linestyle=\"--\",linewidth=1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#do the above but sum the Z values along the X axis\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "for i in maps:\n",
    "    ax.plot(y,np.mean(maps[i],axis=0),label=\"{} Points\".format(i))\n",
    "ax.set_xlabel(\"Y\")\n",
    "ax.set_ylabel(\"Z\")\n",
    "#plot two vertical lines to show the size of the blob based on the radius\n",
    "ax.axvline(x=25-radi,color=\"black\",linestyle=\"--\",linewidth=1)\n",
    "ax.axvline(x=25+radi,color=\"black\",linestyle=\"--\",linewidth=1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#calculate the laplacian of the maps and create the above plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "sigma = radi/np.sqrt(2)\n",
    "for i in maps:\n",
    "    #apply a gaussian filter to smooth the data\n",
    "    img = ndimage.gaussian_filter(maps[i],sigma=sigma)\n",
    "    lap = sigma**2*ndimage.laplace(img)\n",
    "    ax.plot(y,np.mean(lap,axis=0),label=\"{} Points\".format(i))\n",
    "ax.set_xlabel(\"Y\")\n",
    "ax.set_ylabel(\"Laplacian\")\n",
    "#plot two vertical lines to show the size of the blob based on the radius\n",
    "ax.axvline(x=25-radi,color=\"black\",linestyle=\"--\",linewidth=1)\n",
    "ax.axvline(x=25+radi,color=\"black\",linestyle=\"--\",linewidth=1)\n",
    "#make the legend smaller\n",
    "plt.legend(fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "#calculate the maximum of the laplacian of the maps \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "sigmas = np.linspace(0.5,5,50)\n",
    "for i in maps:\n",
    "    laps = []\n",
    "    for j in sigmas:\n",
    "        #apply a gaussian filter to smooth the data\n",
    "        img = ndimage.gaussian_filter(maps[i],sigma=j)\n",
    "        lap = np.max(np.abs(j**2*ndimage.laplace(img)))\n",
    "        laps.append(lap)\n",
    "    ax.plot(sigmas*np.sqrt(2),laps,label=\"{} Points\".format(i))\n",
    "\n",
    "ax.set_xlabel(\"Sigma*sqrt(2) Used\")\n",
    "ax.set_ylabel(\"Abs Max of the Normalized Laplacian\")\n",
    "#plot the radius of the blob as a vertical line\n",
    "#ax.axvline(x=radi,color=\"black\",linestyle=\"-\",linewidth=1)\n",
    "\n",
    "#make the legend smaller\n",
    "plt.legend(fontsize=8)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the localization error of single molecules detection by simulating multiple single points and running the blob detection on it. \n",
    "Then find the difference in the sigma of the detection and also the x,y position found vs simulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a dictionary of the parameters to be used in the blob simulation\n",
    "SMT_param = {\"max_x\":50,\n",
    "             \"min_x\":0,\n",
    "             \"radius\":2,\n",
    "             \"center\":[25.,25.],\n",
    "             \"total_points\":1,\n",
    "             \"density_dif\":20.0,\n",
    "             \"pdf\":simulate_foci.tophat_function_2d,\n",
    "             \"uniform\":True,\n",
    "             \"psf_sigma\":0.82,\n",
    "             \"base_noise\":140,\n",
    "             \"point_intensity\":20}\n",
    "#define the blob detection parameters\n",
    "blob_param = {\"min_sigma\":1,\n",
    "            \"max_sigma\":10,\n",
    "            \"num_sigma\":100,\n",
    "            \"threshold\":3e1,\n",
    "            \"overlap\":0,\n",
    "            \"exclude_border\":False,\n",
    "            \"median\":False,\n",
    "            \"verbose\":True}\n",
    "#define the fitting parameters\n",
    "fit_param = {\"mask_size\":5,\n",
    "            \"plot_fit\":False,\n",
    "            \"fitting_image\":\"Original\",\n",
    "            \"radius_func\":None,\n",
    "            \"sigma_range\":0.1,\n",
    "            \"centroid_range\":0.1}\n",
    "#create a blob simulation object\n",
    "sim = simulate_foci.sim_foci(**SMT_param)\n",
    "#number of maps to make\n",
    "num_maps = 100\n",
    "#make the maps and store them in a list\n",
    "maps = []\n",
    "points = []\n",
    "for i in range(num_maps):\n",
    "    map,point = sim.simulate_point(generator = None)\n",
    "    maps.append(map)\n",
    "    points.append(point)\n",
    "\n",
    "\n",
    "#create the blob detection object\n",
    "b = blob_detection(path=None,**blob_param)\n",
    "#update the fitting parameters\n",
    "b._update_fitting_parameters(kwargs=fit_param)\n",
    "#use Fitted or Scale for the blob detection\n",
    "\n",
    "#run the blob detection on each of the maps\n",
    "blobs = []\n",
    "for i in maps:\n",
    "    b.img = i\n",
    "    blobs.append(b.detection(type = \"bp\")[\"Fitted\"])\n",
    "\n",
    "#find the difference in x,y coordinates of the points simulated and blobs detected\n",
    "diffs = []\n",
    "diffs_x = []\n",
    "diffs_y = []\n",
    "sigma_x = []\n",
    "sigma_y = []\n",
    "for i in range(len(blobs)):\n",
    "    diff = np.sqrt((points[i][:,0]-blobs[i][:,1])**2+(points[i][:,1]-blobs[i][:,0])**2)\n",
    "    diffs_x.append(points[i][:,0]-blobs[i][:,1])\n",
    "    diffs_y.append(points[i][:,1]-blobs[i][:,0])\n",
    "    diffs.append(diff)\n",
    "    sigma_x.append(blobs[i][:,2])\n",
    "    sigma_y.append(blobs[i][:,3])\n",
    "\n",
    "#find the difference in the radius of the points simulated and blobs detected\n",
    "radii = []\n",
    "for i in range(len(blobs)):\n",
    "    radi = np.sqrt((0.82*np.sqrt(2)-blobs[i][:,2])**2)\n",
    "    radii.append(radi)\n",
    "\n",
    "#print the mean and standard deviation of the differences for both the x,y coordinates and the radius. \n",
    "# Convert all to nm using the olympus pixel size from globals\n",
    "print(\"Mean of the x,y coordinate differences: {} nm\".format(np.mean(flatten(diffs))*globals[\"olympus_pixel_size\"]))\n",
    "print(\"Standard Deviation of the x,y coordinate differences: {} nm\".format(np.std(flatten(diffs))*globals[\"olympus_pixel_size\"]))\n",
    "print(\"Mean of the radius differences: {} nm\".format(np.mean(flatten(radii))*globals[\"olympus_pixel_size\"]))\n",
    "print(\"Standard Deviation of the radius differences: {} nm\".format(np.std(flatten(radii))*globals[\"olympus_pixel_size\"]))\n",
    "\n",
    "#find the mean and std for the diff_x and diff_y\n",
    "mean_x = np.mean(flatten(diffs_x))\n",
    "mean_y = np.mean(flatten(diffs_y))\n",
    "std_x = np.std(flatten(diffs_x))\n",
    "std_y = np.std(flatten(diffs_y))\n",
    "#print them out\n",
    "print(\"Mean of the x coordinate differences: {} nm\".format(mean_x*globals[\"olympus_pixel_size\"]))\n",
    "print(\"Standard Deviation of the x coordinate differences: {} nm\".format(std_x*globals[\"olympus_pixel_size\"]))\n",
    "print(\"Mean of the y coordinate differences: {} nm\".format(mean_y*globals[\"olympus_pixel_size\"]))\n",
    "print(\"Standard Deviation of the y coordinate differences: {} nm\".format(std_y*globals[\"olympus_pixel_size\"]))\n",
    "\n",
    "\n",
    "#plot the histogram of the differences in x,y coordinates\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.hist(flatten(diffs_x),bins=10,label=\"X\")\n",
    "ax.hist(flatten(diffs_y),bins=10,label=\"Y\")\n",
    "ax.set_xlabel(\"Difference in X,Y Coordinates (Pixels)\")\n",
    "ax.set_ylabel(\"Number of Points\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#plot the histogram of the differences in total diffs\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.hist(flatten(diffs),bins=10)\n",
    "ax.set_xlabel(\"Difference in R Coordinates (Pixels)\")\n",
    "ax.set_ylabel(\"Number of Points\")\n",
    "plt.show()\n",
    "\n",
    "#plot the sigma_x and sigma_y\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.hist(flatten(sigma_x),bins=10,label=\"Sigma X\")\n",
    "ax.hist(flatten(sigma_y),bins=10,label=\"Sigma Y\")\n",
    "ax.set_xlabel(\"Sigma X,Y (Pixels)\")\n",
    "ax.set_ylabel(\"Number of Points\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(maps[0])\n",
    "#plot the points from maps[0]\n",
    "for i in blobs[0]:\n",
    "    #make an ellipse\n",
    "    print(i)\n",
    "    e = Ellipse((i[1],i[0]),i[2]*2,i[3]*2,0,fill=False)\n",
    "    #add the ellipse to the plot\n",
    "    ax.add_patch(e)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#plot the ratio of sigma_x and sigma_y\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.hist(np.array(flatten(sigma_x))/np.array(flatten(sigma_y)),bins=10)\n",
    "ax.set_xlabel(\"Sigma X/Y\")\n",
    "ax.set_ylabel(\"Number of Points\")\n",
    "plt.show()\n",
    "\n",
    "#plot the sigma_x and sigma_y as a line plot with sigma_x on the x axis and sigma_y on the y axis\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(flatten(sigma_x),flatten(sigma_y),\".\")\n",
    "ax.set_xlabel(\"Sigma X (Pixels)\")\n",
    "ax.set_ylabel(\"Sigma Y (Pixels)\")\n",
    "#plot a 1:1 line\n",
    "ax.plot([0,2],[0,2])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "a = simulate_foci.generate_sphere_points(200,[1,1,1],10)\n",
    "b = simulate_foci.generate_radial_points(200,[1,1],10)\n",
    "fig,ax = plt.subplots(2,2,sharex=True,sharey=True)\n",
    "#make the x and y axis for each subplot the same aspect ratio\n",
    "ax[0,0].set_aspect('equal')\n",
    "ax[1,0].set_aspect('equal')\n",
    "ax[0,1].set_aspect('equal')\n",
    "ax[1,1].set_aspect('equal')\n",
    "\n",
    "ax[0,0].plot(a[:,0],a[:,1],'o',markersize=1)\n",
    "ax[1,0].plot(b[:,0],b[:,1],'o',markersize=1)\n",
    "#on the top right plot, plot the density of the points over the whole figure\n",
    "#first, make a histogram of the points\n",
    "hist, xedges, yedges = np.histogram2d(a[:,0],a[:,1],bins=100)\n",
    "#make a meshgrid of the x and y edges\n",
    "x,y = np.meshgrid(xedges,yedges)\n",
    "#plot the histogram as a contour plot\n",
    "ax[0,1].contour(x[1:,1:],y[1:,1:],hist)\n",
    "#on the bottom right plot, plot the density of the points over the whole figure\n",
    "#first, make a histogram of the points\n",
    "hist, xedges, yedges = np.histogram2d(b[:,0],b[:,1],bins=100)\n",
    "#make a meshgrid of the x and y edges\n",
    "x,y = np.meshgrid(xedges,yedges)\n",
    "#plot the histogram as a contour plot\n",
    "ax[1,1].contour(x[1:,1:],y[1:,1:],hist)\n",
    "#add the circles to the top left plot and the bottom left plot\n",
    "cir = Circle((1,1),10,fill = False)\n",
    "ax[0,0].add_patch(cir)\n",
    "cir2 = Circle((1,1),10,fill = False)\n",
    "ax[1,0].add_patch(cir2)\n",
    "#add the circles to the top right plot and hte bottom right plot\n",
    "cir = Circle((1,1),10,fill = False)\n",
    "ax[0,1].add_patch(cir)\n",
    "cir2 = Circle((1,1),10,fill = False)\n",
    "ax[1,1].add_patch(cir2)\n",
    "#title the plots\n",
    "ax[0,0].set_title(\"Uniform points in a sphere (projection to 2D)\") \n",
    "ax[1,0].set_title(\"Uniform points in a circle\")\n",
    "ax[0,1].set_title(\"Density of points in a sphere (projection to 2D)\")   \n",
    "ax[1,1].set_title(\"Density of points in a circle\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sim = simulate_foci.sim_foci(max_x = 10,\n",
    "                            min_x = 0,\n",
    "                            radius = 2,\n",
    "                            center = [5.,5.],\n",
    "                            total_points = 500,\n",
    "                            density_dif = 5.0,\n",
    "                            pdf = simulate_foci.tophat_function_2d,\n",
    "                            point_intensity = 40,\n",
    "                            projection_frames = 1000)\n",
    "sim.uniform_blob = False\n",
    "sim.psf_sigma = 0.82\n",
    "sim.base_noise = 140\n",
    "map,sim_xy = sim.simulate_point(generator = None,movie=False)\n",
    "#plot the points in sim_xy\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(sim_xy[:,0],sim_xy[:,1],\".\")\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_title(\"Simulated Points Locations\")\n",
    "plt.show()\n",
    "\n",
    "map = np.array(map)\n",
    "b = blob_detection(path = map,\\\n",
    "                    median= False,\\\n",
    "                    threshold= 5e3, \\\n",
    "                    min_sigma= 1/np.sqrt(2), \\\n",
    "                    max_sigma = 10/np.sqrt(2), \\\n",
    "                    num_sigma= 1000, \\\n",
    "                    overlap = 0, \\\n",
    "                    logscale=False,\n",
    "                    verbose=True)\n",
    "b._update_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                            \"plot_fit\":False,\n",
    "                            \"fitting_image\":\"Original\",\n",
    "                            \"radius_func\":None,\n",
    "                            \"sigma_range\":2,\n",
    "                            \"centroid_range\":2})\n",
    "c = b.detection(type = \"bp\")\n",
    "print(c)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.imshow(map,cmap=\"gray\")\n",
    "for i in c[\"Scale\"]:\n",
    "    cir = plt.Circle((i[1],i[0]),i[2],fill = False, edgecolor = \"green\")\n",
    "    ax.add_artist(cir)\n",
    "    ax.plot(i[1],i[0],\"o\",color = \"green\",markersize = 2,label = \"Fitted Blob Center and Radius\")\n",
    "# for i in sim_xy:\n",
    "#     ax.plot(i[0],i[1],\"o\",color = \"red\",markersize = 2,label = \"Simulated Blob Center\")\n",
    "#add a legend for the circle\n",
    "#plot the x,y points\n",
    "ax.plot(sim_xy[:,0],sim_xy[:,1],\"r.\")\n",
    "\n",
    "cir = plt.Circle((sim.center[0],sim.center[1]),sim.radius,fill = False,edgecolor = \"Red\")\n",
    "ax.plot(sim.center[0],sim.center[1],\"o\",color = \"red\",markersize = 2,label = \"Simulated Blob Center and Radius\")\n",
    "plt.legend()\n",
    "\n",
    "ax.add_artist(cir)\n",
    "plt.show()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.imshow(bin_img(map,bin=2,operation='mean'),cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roc curve for condensate detection using DBSCAN and bp method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point change\n",
    "density_scan = np.linspace(1,100,10)\n",
    "point_scan = np.arange(30,1000,100)\n",
    "num_blobs = np.zeros(len(point_scan))\n",
    "num_clusters = np.zeros(len(point_scan))\n",
    "error_min_cluster = np.zeros(len(point_scan)) -1\n",
    "min_blob_error = np.zeros(len(point_scan)) -1\n",
    "min_samples = 30\n",
    "for i in point_scan:\n",
    "    sim = simulate_foci.sim_foci(max_x = 15,\n",
    "                            min_x = 0,\n",
    "                            radius = 0.65,\n",
    "                            center = [7.,7.],\n",
    "                            total_points = i,\n",
    "                            density_dif = 5.0,\n",
    "                            pdf = simulate_foci.tophat_function_2d,\n",
    "                            point_intensity = 40,\n",
    "                            projection_frames = 1000)\n",
    "    sim.uniform_blob = False\n",
    "    sim.psf_sigma = 0.82\n",
    "    sim.base_noise = 140\n",
    "    map,sim_xy = sim.simulate_point(generator = None,movie=False)\n",
    "\n",
    "    clustering = OPTICS(min_samples=min_samples).fit(sim_xy)\n",
    "    fig,ax = plt.subplots()\n",
    "    a = ax.scatter(sim_xy[:,0],sim_xy[:,1],s= 1,c = clustering.labels_)\n",
    "    #make a circle for the simulated blob\n",
    "    cir = plt.Circle((sim.center[0],sim.center[1]),sim.radius,fill = False,edgecolor = \"Red\")\n",
    "    ax.add_artist(cir)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_title(\"OPTICS Clustering with {} minsamples\".format(min_samples))\n",
    "    #add a colorbar\n",
    "    fig.colorbar(a)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    num_cluster = len(clustering.cluster_hierarchy_) -1\n",
    "    num_clusters[np.where(point_scan == i)] = num_cluster\n",
    "\n",
    "    min_error = 10\n",
    "    #for each cluster use the smallest enclosing circle to find the radius of the cluster\n",
    "    for kk in np.unique(clustering.labels_):\n",
    "        #find the points in the cluster\n",
    "        cluster_points = sim_xy[np.where(clustering.labels_ == kk)]\n",
    "        #find the smallest enclosing circle\n",
    "        center_x,center_y,radius = smallestenclosingcircle.make_circle(cluster_points)\n",
    "        #find the error\n",
    "        if np.abs(radius - sim.radius)/sim.radius < min_error:\n",
    "            min_error = np.abs(radius - sim.radius)/sim.radius\n",
    "    if min_error == 10:\n",
    "        min_error = -1\n",
    "    error_min_cluster[np.where(point_scan == i)] = min_error\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    b = blob_detection(path = map,\\\n",
    "                    median= False,\\\n",
    "                    threshold= 6e2, \\\n",
    "                    min_sigma= 1.4/np.sqrt(2), \\\n",
    "                    max_sigma = 10/np.sqrt(2), \\\n",
    "                    num_sigma= 1000, \\\n",
    "                    overlap = 0, \\\n",
    "                    logscale=False,\n",
    "                    verbose=True)\n",
    "    b._update_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                \"plot_fit\":False,\n",
    "                                \"fitting_image\":\"Original\",\n",
    "                                \"radius_func\":None,\n",
    "                                \"sigma_range\":2,\n",
    "                                \"centroid_range\":2})\n",
    "    c = b.detection(type = \"bp\")\n",
    "    #find the number of blobs identified\n",
    "    num_blob = len(c[\"Scale\"])\n",
    "    num_blobs[np.where(point_scan == i)] = num_blob\n",
    "    #find the size of the blob identified\n",
    "    blob_size = c[\"Scale\"][:,2]\n",
    "    #comapre the blob size to the simulated one using a square error\n",
    "    try:\n",
    "        error_blob_size = np.min(np.abs(blob_size - sim.radius))/sim.radius\n",
    "    except:\n",
    "        error_blob_size = -1\n",
    "    min_blob_error[np.where(point_scan == i)] = error_blob_size\n",
    "\n",
    "\n",
    "    #plot the map with the identified blobs\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    ax.imshow(map,cmap=\"gray\")\n",
    "    for j in c[\"Scale\"]:\n",
    "        cir = plt.Circle((j[1],j[0]),j[2],fill = False, edgecolor = \"green\")\n",
    "        ax.add_artist(cir)\n",
    "        ax.plot(j[1],j[0],\"o\",color = \"green\",markersize = 2,label = \"Fitted Blob Center and Radius\")\n",
    "        cir2 = plt.Circle((sim.center[0],sim.center[1]),sim.radius,fill = False,edgecolor = \"Red\")\n",
    "        ax.add_artist(cir2)\n",
    "        ax.plot(sim.center[0],sim.center[1],\"o\",color = \"red\",markersize = 2,label = \"Simulated Blob Center and Radius\")\n",
    "    #add a legend outside the plot\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.title(\"Blob Detection with {} points\".format(i))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#create a plot with two y axis ( one for num_blobs and one for num_clusters)\n",
    "fig,ax1 = plt.subplots()\n",
    "ax1.plot(point_scan,num_blobs,\"o\",label = \"Number of Blobs\")\n",
    "ax1.set_xlabel(\"Number of Points\")\n",
    "ax1.set_ylabel(\"Number of Blobs\")\n",
    "ax1.set_title(\"Number of Blobs vs Number of Points\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(point_scan,num_clusters,\"o\",color = \"red\",label = \"Number of Clusters\")\n",
    "ax2.set_ylabel(\"Number of Clusters\")\n",
    "ax2.legend()\n",
    "plt.show()\n",
    "\n",
    "#make a new plot with both plots on the same axis\n",
    "fig,ax1 = plt.subplots()\n",
    "ax1.plot(point_scan,num_blobs,\"o\",label = \"Number of Blobs\")\n",
    "ax1.set_xlabel(\"Number of Points\")\n",
    "ax1.set_ylabel(\"Number of Blobs\")\n",
    "ax1.set_title(\"Number of Blobs vs Number of Points\")\n",
    "ax1.plot(point_scan,num_clusters,\"o\",color = \"red\",label = \"Number of Clusters\")\n",
    "ax1.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#plot the error in the blob size\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(point_scan,min_blob_error,\"o\")\n",
    "ax.set_xlabel(\"Number of Points\")\n",
    "ax.set_ylabel(\"Error in Blob Size\")\n",
    "ax.set_title(\"Error in Blob Size vs Number of Points\")\n",
    "plt.show()\n",
    "\n",
    "#plot the error in the cluster size\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(point_scan,error_min_cluster,\"o\")\n",
    "ax.set_xlabel(\"Number of Points\")\n",
    "ax.set_ylabel(\"Error in Cluster Size\")\n",
    "ax.set_title(\"Error in Cluster Size vs Number of Points\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#density change\n",
    "density_scan = np.linspace(1,100,10)\n",
    "point_scan = np.arange(10,1000,100)\n",
    "num_blobs = np.zeros(len(point_scan))\n",
    "num_clusters = np.zeros(len(point_scan))\n",
    "for i in density_scan:\n",
    "    sim = simulate_foci.sim_foci(max_x = 50,\n",
    "                            min_x = 0,\n",
    "                            radius = 2,\n",
    "                            center = [20.,25.],\n",
    "                            total_points = 400,\n",
    "                            density_dif = i,\n",
    "                            pdf = simulate_foci.tophat_function_2d,\n",
    "                            point_intensity = 40,\n",
    "                            projection_frames = 1000)\n",
    "    sim.uniform_blob = False\n",
    "    sim.psf_sigma = 0.82\n",
    "    sim.base_noise = 140\n",
    "    map,sim_xy = sim.simulate_point(generator = None,movie=False)\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.plot(sim_xy[:,0],sim_xy[:,1],\".\")\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_title(\"Simulated Points Locations {} density\".format(i))\n",
    "    plt.show()\n",
    "\n",
    "    clustering = OPTICS(min_samples=5).fit(sim_xy)\n",
    "    a = plt.scatter(sim_xy[:,0],sim_xy[:,1],s= 1,c = clustering.labels_)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.title(\"OPTICS Clustering with {} minsamples\".format(5))\n",
    "    #add a colorbar\n",
    "    plt.colorbar(a)\n",
    "    plt.show()\n",
    "\n",
    "    num_cluster = len(clustering.cluster_hierarchy_)\n",
    "    num_clusters[np.where(density_scan == i)] = num_cluster\n",
    "    b = blob_detection(path = map,\\\n",
    "                    median= False,\\\n",
    "                    threshold= 6e2, \\\n",
    "                    min_sigma= 1.4/np.sqrt(2), \\\n",
    "                    max_sigma = 10/np.sqrt(2), \\\n",
    "                    num_sigma= 1000, \\\n",
    "                    overlap = 0, \\\n",
    "                    logscale=False,\n",
    "                    verbose=True)\n",
    "    b._update_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                \"plot_fit\":False,\n",
    "                                \"fitting_image\":\"Original\",\n",
    "                                \"radius_func\":None,\n",
    "                                \"sigma_range\":2,\n",
    "                                \"centroid_range\":2})\n",
    "    c = b.detection(type = \"bp\")\n",
    "    #find the number of blobs identified\n",
    "    num_blob = len(c[\"Scale\"])\n",
    "    num_blobs[np.where(density_scan == i)] = num_blob\n",
    "    #find the size of the blob identified\n",
    "    blob_size = np.mean(c[\"Scale\"][:,2])\n",
    "    #comapre the blob size to the simulated one\n",
    "    error_blob_size = np.abs(blob_size - sim.radius)/sim.radius\n",
    "    #plot the map with the identified blobs\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    ax.imshow(map,cmap=\"gray\")\n",
    "    for j in c[\"Scale\"]:\n",
    "        cir = plt.Circle((j[1],j[0]),j[2],fill = False, edgecolor = \"green\")\n",
    "        ax.add_artist(cir)\n",
    "        ax.plot(j[1],j[0],\"o\",color = \"green\",markersize = 2,label = \"Fitted Blob Center and Radius\")\n",
    "    cir2 = plt.Circle((sim.center[0],sim.center[1]),sim.radius,fill = False,edgecolor = \"Red\")\n",
    "    ax.add_artist(cir2)\n",
    "    ax.plot(sim.center[0],sim.center[1],\"o\",color = \"red\",markersize = 2,label = \"Simulated Blob Center and Radius\")\n",
    "    #add a legend outside the plot\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.title(\"Blob Detection with {} density\".format(i))\n",
    "    plt.show()\n",
    "\n",
    "#create a plot with two y axis ( one for num_blobs and one for num_clusters)\n",
    "fig,ax1 = plt.subplots()\n",
    "ax1.plot(density_scan,num_blobs,\"o\",label = \"Number of Blobs\")\n",
    "ax1.set_xlabel(\"Number of Points\")\n",
    "ax1.set_ylabel(\"Number of Blobs\")\n",
    "ax1.set_title(\"Number of Blobs vs Number of Points\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(density_scan,num_clusters,\"o\",color = \"red\",label = \"Number of Clusters\")\n",
    "ax2.set_ylabel(\"Number of Clusters\")\n",
    "ax2.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#psf change \n",
    "density_scan = np.linspace(1,100,10)\n",
    "point_scan = np.arange(10,1000,100)\n",
    "psf_scan = np.linspace(0.1,5,20)\n",
    "num_blobs = np.zeros(len(psf_scan))\n",
    "num_clusters = np.zeros(len(psf_scan))\n",
    "for i in psf_scan:\n",
    "    sim = simulate_foci.sim_foci(max_x = 50,\n",
    "                            min_x = 0,\n",
    "                            radius = 2,\n",
    "                            center = [20.,25.],\n",
    "                            total_points = 1000,\n",
    "                            density_dif = 20.0,\n",
    "                            pdf = simulate_foci.tophat_function_2d,\n",
    "                            point_intensity = 40,\n",
    "                            projection_frames = 1000)\n",
    "    sim.uniform_blob = False\n",
    "    sim.psf_sigma = i\n",
    "    sim.base_noise = 140\n",
    "    map,sim_xy = sim.simulate_point(generator = None,movie=False)\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.plot(sim_xy[:,0],sim_xy[:,1],\".\")\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_title(\"Simulated Points Locations {} psf_sigma\".format(i*130))\n",
    "    plt.show()\n",
    "\n",
    "    clustering = OPTICS(min_samples=30).fit(sim_xy)\n",
    "    a = plt.scatter(sim_xy[:,0],sim_xy[:,1],s= 1,c = clustering.labels_)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.title(\"OPTICS Clustering with {} minsamples\".format(30))\n",
    "    #add a colorbar\n",
    "    plt.colorbar(a)\n",
    "    plt.show()\n",
    "\n",
    "    num_cluster = len(clustering.cluster_hierarchy_)\n",
    "    num_clusters[np.where(psf_scan == i)] = num_cluster\n",
    "    b = blob_detection(path = map,\\\n",
    "                    median= False,\\\n",
    "                    threshold= 1e3, \\\n",
    "                    min_sigma= 1.4/np.sqrt(2), \\\n",
    "                    max_sigma = 10/np.sqrt(2), \\\n",
    "                    num_sigma= 1000, \\\n",
    "                    overlap = 0, \\\n",
    "                    logscale=False,\n",
    "                    verbose=True)\n",
    "    b._update_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                \"plot_fit\":False,\n",
    "                                \"fitting_image\":\"Original\",\n",
    "                                \"radius_func\":None,\n",
    "                                \"sigma_range\":2,\n",
    "                                \"centroid_range\":2})\n",
    "    c = b.detection(type = \"bp\")\n",
    "    #find the number of blobs identified\n",
    "    num_blob = len(c[\"Scale\"])\n",
    "    num_blobs[np.where(psf_scan == i)] = num_blob\n",
    "    #find the size of the blob identified\n",
    "    blob_size = np.mean(c[\"Scale\"][:,2])\n",
    "    #comapre the blob size to the simulated one\n",
    "    error_blob_size = np.abs(blob_size - sim.radius)/sim.radius\n",
    "    #plot the map with the identified blobs\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    ax.imshow(map,cmap=\"gray\")\n",
    "    for j in c[\"Scale\"]:\n",
    "        cir = plt.Circle((j[1],j[0]),j[2],fill = False, edgecolor = \"green\")\n",
    "        ax.add_artist(cir)\n",
    "        ax.plot(j[1],j[0],\"o\",color = \"green\",markersize = 2,label = \"Fitted Blob Center and Radius\")\n",
    "        \n",
    "    cir2 = plt.Circle((sim.center[0],sim.center[1]),sim.radius,fill = False,edgecolor = \"Red\")\n",
    "    ax.add_artist(cir2)\n",
    "    ax.plot(sim.center[0],sim.center[1],\"o\",color = \"red\",markersize = 2,label = \"Simulated Blob Center and Radius\")\n",
    "    #add a legend outside the plot\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.title(\"Blob Detection with {} psf_sigma\".format(i*130))\n",
    "    plt.show()\n",
    "\n",
    "#create a plot with two y axis ( one for num_blobs and one for num_clusters)\n",
    "fig,ax1 = plt.subplots()\n",
    "ax1.plot(psf_scan*130,num_blobs,\"o\",label = \"Number of Blobs\")\n",
    "ax1.set_xlabel(\"psf sigma (nm)\")\n",
    "ax1.set_ylabel(\"Number of Blobs\")\n",
    "ax1.set_title(\"Number of Blobs vs Number of Points\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(psf_scan*130,num_clusters,\"o\",color = \"red\",label = \"Number of Clusters\")\n",
    "ax2.set_ylabel(\"Number of Clusters\")\n",
    "#plot a line at y = 1 to show the number of clusters\n",
    "ax1.axhline(y=1, color='g', linestyle='--')\n",
    "fig.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#radius change \n",
    "density_scan = np.linspace(1,100,10)\n",
    "point_scan = np.arange(10,1000,100)\n",
    "psf_scan = np.linspace(0.1,5,20)\n",
    "radius_scan = np.linspace(0.5,10,10)\n",
    "num_blobs = np.zeros(len(radius_scan))\n",
    "num_clusters = np.zeros(len(radius_scan))\n",
    "min_samples = 50\n",
    "\n",
    "\n",
    "for i in radius_scan:\n",
    "    sim = simulate_foci.sim_foci(max_x = 50,\n",
    "                            min_x = 0,\n",
    "                            radius = i,\n",
    "                            center = [20.,25.],\n",
    "                            total_points = 1000,\n",
    "                            density_dif = 20.0,\n",
    "                            pdf = simulate_foci.tophat_function_2d,\n",
    "                            point_intensity = 40,\n",
    "                            projection_frames = 1000)\n",
    "    sim.uniform_blob = False\n",
    "    sim.psf_sigma = 0.82\n",
    "    sim.base_noise = 140\n",
    "    map,sim_xy = sim.simulate_point(generator = None,movie=False)\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.plot(sim_xy[:,0],sim_xy[:,1],\".\")\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_title(\"Simulated Points Locations {} radius\".format(i))\n",
    "    plt.show()\n",
    "\n",
    "    clustering = OPTICS(min_samples=min_samples).fit(sim_xy)\n",
    "    a = plt.scatter(sim_xy[:,0],sim_xy[:,1],s= 1,c = clustering.labels_)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.title(\"OPTICS Clustering with {} minsamples\".format(min_samples))\n",
    "    #add a colorbar\n",
    "    plt.colorbar(a)\n",
    "    plt.show()\n",
    "\n",
    "    num_cluster = len(clustering.cluster_hierarchy_)\n",
    "    num_clusters[np.where(radius_scan == i)] = num_cluster\n",
    "    b = blob_detection(path = map,\\\n",
    "                    median= False,\\\n",
    "                    threshold= 1e3, \\\n",
    "                    min_sigma= 1.1/np.sqrt(2), \\\n",
    "                    max_sigma = 10/np.sqrt(2), \\\n",
    "                    num_sigma= 1000, \\\n",
    "                    overlap = 0, \\\n",
    "                    logscale=False,\n",
    "                    verbose=True)\n",
    "    b._update_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                \"plot_fit\":False,\n",
    "                                \"fitting_image\":\"Original\",\n",
    "                                \"radius_func\":None,\n",
    "                                \"sigma_range\":2,\n",
    "                                \"centroid_range\":2})\n",
    "    c = b.detection(type = \"bp\")\n",
    "    #find the number of blobs identified\n",
    "    num_blob = len(c[\"Scale\"])\n",
    "    num_blobs[np.where(radius_scan == i)] = num_blob\n",
    "    #find the size of the blob identified\n",
    "    blob_size = np.mean(c[\"Scale\"][:,2])\n",
    "    #comapre the blob size to the simulated one\n",
    "    error_blob_size = np.abs(blob_size - sim.radius)/sim.radius\n",
    "    #plot the map with the identified blobs\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    ax.imshow(map,cmap=\"gray\")\n",
    "    for j in c[\"Scale\"]:\n",
    "        cir = plt.Circle((j[1],j[0]),j[2],fill = False, edgecolor = \"green\")\n",
    "        ax.add_artist(cir)\n",
    "        ax.plot(j[1],j[0],\"o\",color = \"green\",markersize = 2,label = \"Fitted Blob Center and Radius\")\n",
    "        \n",
    "    cir2 = plt.Circle((sim.center[0],sim.center[1]),sim.radius,fill = False,edgecolor = \"Red\")\n",
    "    ax.add_artist(cir2)\n",
    "    ax.plot(sim.center[0],sim.center[1],\"o\",color = \"red\",markersize = 2,label = \"Simulated Blob Center and Radius\")\n",
    "    #add a legend outside the plot\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.title(\"Blob Detection with {} radius\".format(i))\n",
    "    plt.show()\n",
    "\n",
    "#create a plot with two y axis ( one for num_blobs and one for num_clusters)\n",
    "fig,ax1 = plt.subplots()\n",
    "ax1.plot(radius_scan,num_blobs,\"o\",label = \"Number of Blobs\")\n",
    "ax1.set_xlabel(\"radius\")\n",
    "ax1.set_ylabel(\"Number of Blobs\")\n",
    "ax1.set_title(\"Number of Blobs vs Size of Condensate\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(radius_scan,num_clusters,\"o\",color = \"red\",label = \"Number of Clusters\")\n",
    "ax2.set_ylabel(\"Number of Clusters\")\n",
    "#plot a line at y = 1 to show the number of clusters\n",
    "ax1.axhline(y=1, color='g', linestyle='--')\n",
    "fig.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point and density change\n",
    "density_scan = np.linspace(1,100,10)\n",
    "point_scan = np.arange(10,1000,100)\n",
    "psf_scan = np.linspace(0.1,5,20)\n",
    "num_blobs = np.zeros((len(density_scan),len(point_scan)))\n",
    "num_clusters = np.zeros((len(density_scan),len(point_scan)))\n",
    "min_blob_error = np.zeros((len(density_scan),len(point_scan)))\n",
    "\n",
    "repeats = 10\n",
    "\n",
    "\n",
    "for i in range(len(density_scan)):\n",
    "    for k in range(len(point_scan)):\n",
    "        mean_num_blob = 0\n",
    "        mean_num_cluster = 0\n",
    "        mean_min_blob_error = 0\n",
    "\n",
    "        for l in range(repeats):\n",
    "            sim = simulate_foci.sim_foci(max_x = 50,\n",
    "                                    min_x = 0,\n",
    "                                    radius = 2,\n",
    "                                    center = [20.,25.],\n",
    "                                    total_points = point_scan[k],\n",
    "                                    density_dif = density_scan[i],\n",
    "                                    pdf = simulate_foci.tophat_function_2d,\n",
    "                                    point_intensity = 40,\n",
    "                                    projection_frames = 1000)\n",
    "            sim.uniform_blob = False\n",
    "            sim.psf_sigma = 0.82\n",
    "            sim.base_noise = 140\n",
    "            map,sim_xy = sim.simulate_point(generator = None,movie=False)\n",
    "            # fig,ax = plt.subplots()\n",
    "            # ax.plot(sim_xy[:,0],sim_xy[:,1],\".\")\n",
    "            # ax.set_aspect(\"equal\")\n",
    "            # ax.set_xlabel(\"X\")\n",
    "            # ax.set_ylabel(\"Y\")\n",
    "            # ax.set_title(\"Simulated Points Locations {} psf_sigma\".format(i*130))\n",
    "            # plt.show()\n",
    "\n",
    "            clustering = OPTICS(min_samples=5).fit(sim_xy)\n",
    "            # a = plt.scatter(sim_xy[:,0],sim_xy[:,1],s= 1,c = clustering.labels_)\n",
    "            # plt.xlabel(\"X\")\n",
    "            # plt.ylabel(\"Y\")\n",
    "            # plt.title(\"OPTICS Clustering with {} minsamples\".format(5))\n",
    "            # #add a colorbar\n",
    "            # plt.colorbar(a)\n",
    "            # plt.show()\n",
    "\n",
    "            num_cluster = len(clustering.cluster_hierarchy_)\n",
    "            mean_num_cluster += num_cluster\n",
    "            b = blob_detection(path = map,\\\n",
    "                            median= False,\\\n",
    "                            threshold= 6e2, \\\n",
    "                            min_sigma= 1.4/np.sqrt(2), \\\n",
    "                            max_sigma = 10/np.sqrt(2), \\\n",
    "                            num_sigma= 1000, \\\n",
    "                            overlap = 0, \\\n",
    "                            logscale=False,\n",
    "                            verbose=True)\n",
    "            b._update_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                        \"plot_fit\":False,\n",
    "                                        \"fitting_image\":\"Original\",\n",
    "                                        \"radius_func\":None,\n",
    "                                        \"sigma_range\":2,\n",
    "                                        \"centroid_range\":2})\n",
    "            c = b.detection(type = \"bp\")\n",
    "            #find the number of blobs identified\n",
    "            num_blob = len(c[\"Scale\"])\n",
    "            mean_num_blob += num_blob\n",
    "            #find the size of the blob identified\n",
    "            blob_size = c[\"Scale\"][:,2]\n",
    "            #comapre the blob size to the simulated one using a square error\n",
    "            try:\n",
    "                error_blob_size = np.min(np.abs(blob_size - sim.radius))/sim.radius\n",
    "            except:\n",
    "                error_blob_size = -1\n",
    "            mean_min_blob_error += error_blob_size\n",
    "            #plot the map with the identified blobs\n",
    "            # fig = plt.figure()\n",
    "            # ax = fig.add_subplot()\n",
    "            # ax.imshow(map,cmap=\"gray\")\n",
    "            # for j in c[\"Scale\"]:\n",
    "            #     cir = plt.Circle((j[1],j[0]),j[2],fill = False, edgecolor = \"green\")\n",
    "            #     ax.add_artist(cir)\n",
    "            #     ax.plot(j[1],j[0],\"o\",color = \"green\",markersize = 2,label = \"Fitted Blob Center and Radius\")\n",
    "                \n",
    "            # cir2 = plt.Circle((sim.center[0],sim.center[1]),sim.radius,fill = False,edgecolor = \"Red\")\n",
    "            # ax.add_artist(cir2)\n",
    "            # ax.plot(sim.center[0],sim.center[1],\"o\",color = \"red\",markersize = 2,label = \"Simulated Blob Center and Radius\")\n",
    "            # #add a legend outside the plot\n",
    "            # ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "            # plt.title(\"Blob Detection with {} psf_sigma\".format(i*130))\n",
    "            # plt.show()\n",
    "        num_blobs[i,k] = mean_num_blob/repeats\n",
    "        num_clusters[i,k] = mean_num_cluster/repeats\n",
    "        min_blob_error[i,k] = mean_min_blob_error/repeats\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the number of blobs and clusters as a function of density and number of points in a 3d plot\n",
    "xv, yv = np.meshgrid(density_scan, point_scan)\n",
    "from matplotlib import cm\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "a1 = ax.plot_surface(xv.T,yv.T,num_blobs,cmap=cm.coolwarm)\n",
    "ax.set_xlabel(\"Density\")\n",
    "ax.set_ylabel(\"Number of Points\")\n",
    "ax.set_zlabel(\"Number of Blobs\")\n",
    "plt.title(\"Number of Blobs vs Density and Number of Points\")\n",
    "fig.colorbar(a1,shrink=0.5, aspect=5)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "a2 = ax.plot_surface(xv.T,yv.T,num_clusters,cmap=cm.coolwarm)\n",
    "ax.set_xlabel(\"Density\")\n",
    "ax.set_ylabel(\"Number of Points\")\n",
    "ax.set_zlabel(\"Number of Clusters\")\n",
    "plt.title(\"Number of Clusters vs Density and Number of Points\")\n",
    "fig.colorbar(a2,shrink=0.5, aspect=5)\n",
    "plt.show()\n",
    "\n",
    "#plot the error in blob size as a function of density and number of points in a 3d plot\n",
    "#relace all np.nan with -1\n",
    "min_blob_error[np.isnan(min_blob_error)] = -1\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "a3 = ax.plot_surface(xv.T,yv.T,min_blob_error,cmap=cm.coolwarm)\n",
    "ax.set_xlabel(\"Density\")\n",
    "ax.set_ylabel(\"Number of Points\")\n",
    "ax.set_zlabel(\"Error in Blob Size\")\n",
    "plt.title(\"Error in Blob Size vs Density and Number of Points\")\n",
    "fig.colorbar(a3,shrink=0.5, aspect=5)\n",
    "plt.show()\n",
    "\n",
    "dict = {\"doc\": \"There is 2 independent parameters 1) point_scan, 2) density_scan \\n and 3 dependent parameters 1) num_blobs, 2) num_clusters, 3) min_blob_error.\",\n",
    "        \"point_scan\":point_scan,\n",
    "        \"density_scan\":density_scan,\n",
    "        \"num_blobs\":num_blobs,\n",
    "        \"num_clusters\":num_clusters,\n",
    "        \"min_blob_error\":min_blob_error}\n",
    "\n",
    "#save the dictionary as a pickle file\n",
    "with open(\"/Users/baljyot/Documents/2022-2023/PhD_Thesis/pickled_data/num_blobs_clusters.pkl\",\"wb\") as f:\n",
    "        pickle.dump(dict,f)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the # blobs, as a function of psf/d ratio ( d = average distance betweeen points)\n",
    "#do this by fixing the density and varying the number of points\n",
    "from scipy.spatial import distance\n",
    "density_scan = 20\n",
    "point_scan = np.arange(20,1000,50)\n",
    "psf_scan = np.linspace(0.1,2.5,20)\n",
    "num_blobs = np.zeros((len(psf_scan),len(point_scan)))\n",
    "num_clusters = np.zeros((len(psf_scan),len(point_scan)))\n",
    "min_blob_error = np.zeros((len(psf_scan),len(point_scan)))\n",
    "min_cluster_error = np.zeros((len(psf_scan),len(point_scan)))\n",
    "mean_point_distances = np.zeros((len(psf_scan),len(point_scan)))\n",
    "psf_point_distance_holder = np.zeros((len(psf_scan),len(point_scan)))\n",
    "\n",
    "\n",
    "repeats = 10\n",
    "\n",
    "for i in range(len(psf_scan)):\n",
    "    for k in range(len(point_scan)):\n",
    "        mean_num_blob = 0\n",
    "        mean_num_cluster = 0\n",
    "        mean_min_blob_error = 0\n",
    "        error_min_cluster = 0\n",
    "        mean_point_distance = 0\n",
    "        for l in range(repeats):\n",
    "            sim = simulate_foci.sim_foci(max_x = 50,\n",
    "                                    min_x = 0,\n",
    "                                    radius = 2,\n",
    "                                    center = [20.,25.],\n",
    "                                    total_points = point_scan[k],\n",
    "                                    density_dif = density_scan,\n",
    "                                    pdf = simulate_foci.tophat_function_2d,\n",
    "                                    point_intensity = 40,\n",
    "                                    projection_frames = 1000)\n",
    "            sim.uniform_blob = False\n",
    "            sim.psf_sigma = psf_scan[i]\n",
    "            sim.base_noise = 140\n",
    "            map,sim_xy = sim.simulate_point(generator = None,movie=False)\n",
    "            # fig,ax = plt.subplots()\n",
    "            # ax.plot(sim_xy[:,0],sim_xy[:,1],\".\")\n",
    "            # ax.set_aspect(\"equal\")\n",
    "            # ax.set_xlabel(\"X\")\n",
    "            # ax.set_ylabel(\"Y\")\n",
    "            # ax.set_title(\"Simulated Points Locations {} psf_sigma\".format(i*130))\n",
    "            # plt.show()\n",
    "\n",
    "            #find the mean distance between points\n",
    "            #only use the points in the circle of radius 2 at the center\n",
    "            sim_xy_cluster = sim_xy[np.where(np.sqrt((sim_xy[:,0] - 20)**2 + (sim_xy[:,1] - 25)**2) <= 2)]\n",
    "            mean_point_distance += np.mean(distance.pdist(sim_xy_cluster))\n",
    "            clustering = OPTICS(min_samples=20).fit(sim_xy)\n",
    "            # a = plt.scatter(sim_xy[:,0],sim_xy[:,1],s= 1,c = clustering.labels_)\n",
    "            # plt.xlabel(\"X\")\n",
    "            # plt.ylabel(\"Y\")\n",
    "            # plt.title(\"OPTICS Clustering with {} minsamples\".format(5))\n",
    "            # #add a colorbar\n",
    "            # plt.colorbar(a)\n",
    "            # plt.show()\n",
    "\n",
    "            num_cluster = len(clustering.cluster_hierarchy_) -1\n",
    "            mean_num_cluster += num_cluster\n",
    "            min_error = 10\n",
    "            #for each cluster use the smallest enclosing circle to find the radius of the cluster\n",
    "            for kk in np.unique(clustering.labels_):\n",
    "                #find the points in the cluster\n",
    "                cluster_points = sim_xy[np.where(clustering.labels_ == kk)]\n",
    "                #find the smallest enclosing circle\n",
    "                center_x,center_y,radius = smallestenclosingcircle.make_circle(cluster_points)\n",
    "                #find the error\n",
    "                if np.abs(radius - sim.radius)/sim.radius < min_error:\n",
    "                    min_error = np.abs(radius - sim.radius)/sim.radius\n",
    "            if min_error == 10:\n",
    "                min_error = -1\n",
    "            error_min_cluster += min_error\n",
    "\n",
    "            b = blob_detection(path = map,\\\n",
    "                            median= False,\\\n",
    "                            threshold= 6e2, \\\n",
    "                            min_sigma= 1.4/np.sqrt(2), \\\n",
    "                            max_sigma = 10/np.sqrt(2), \\\n",
    "                            num_sigma= 1000, \\\n",
    "                            overlap = 0, \\\n",
    "                            logscale=False,\n",
    "                            verbose=True)\n",
    "            b._update_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                        \"plot_fit\":False,\n",
    "                                        \"fitting_image\":\"Original\",\n",
    "                                        \"radius_func\":None,\n",
    "                                        \"sigma_range\":2,\n",
    "                                        \"centroid_range\":2})\n",
    "            c = b.detection(type = \"bp\")\n",
    "            #find the number of blobs identified\n",
    "            num_blob = len(c[\"Scale\"])\n",
    "            mean_num_blob += num_blob\n",
    "            #find the size of the blob identified\n",
    "            blob_size = c[\"Scale\"][:,2]\n",
    "            #comapre the blob size to the simulated one using a square error\n",
    "            try:\n",
    "                error_blob_size = np.min(np.abs(blob_size - sim.radius))/sim.radius\n",
    "            except:\n",
    "                error_blob_size = -1\n",
    "            mean_min_blob_error += error_blob_size\n",
    "            #plot the map with the identified blobs\n",
    "            # fig = plt.figure()\n",
    "            # ax = fig.add_subplot()\n",
    "            # ax.imshow(map,cmap=\"gray\")\n",
    "            # for j in c[\"Scale\"]:\n",
    "            #     cir = plt.Circle((j[1],j[0]),j[2],fill = False, edgecolor = \"green\")\n",
    "            #     ax.add_artist(cir)\n",
    "            #     ax.plot(j[1],j[0],\"o\",color = \"green\",markersize = 2,label = \"Fitted Blob Center and Radius\")\n",
    "                \n",
    "            # cir2 = plt.Circle((sim.center[0],sim.center[1]),sim.radius,fill = False,edgecolor = \"Red\")\n",
    "            # ax.add_artist(cir2)\n",
    "            # ax.plot(sim.center[0],sim.center[1],\"o\",color = \"red\",markersize = 2,label = \"Simulated Blob Center and Radius\")\n",
    "            # #add a legend outside the plot\n",
    "            # ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "            # plt.title(\"Blob Detection with {} psf_sigma\".format(i*130))\n",
    "            # plt.show()\n",
    "            print(\"Done with {} psf_sigma and {} points, repeat {}\".format(psf_scan[i]*130,point_scan[k],l))\n",
    "        \n",
    "        num_blobs[i,k] = mean_num_blob/repeats\n",
    "        num_clusters[i,k] = mean_num_cluster/repeats\n",
    "        min_blob_error[i,k] = mean_min_blob_error/repeats\n",
    "        min_cluster_error[i,k] = error_min_cluster/repeats\n",
    "        mean_point_distances[i,k] = mean_point_distance/repeats\n",
    "        psf_point_distance_holder[i,k] = psf_scan[i]\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"/Users/baljyot/Documents/2022-2023/PhD_Thesis/pickled_data/num_blobs_psf_dense/num_blobs_clusters_psf.pkl\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "point_scan = np.arange(20,1000,50)\n",
    "psf_scan = np.linspace(0.1,2.5,20)\n",
    "\n",
    "mean_points = data[\"mean_point_distances\"]\n",
    "point_point_distance_holder = np.zeros((len(psf_scan),len(point_scan)))\n",
    "psf_holder = np.zeros((len(psf_scan),len(point_scan)))\n",
    "for i in range(len(psf_scan)):\n",
    "    for k in range(len(point_scan)):\n",
    "        point_point_distance_holder[i,k] = point_scan[k]\n",
    "        psf_holder[i,k] = psf_scan[i]\n",
    "\n",
    "plt.plot(point_point_distance_holder.flatten(),mean_points.flatten(),\"o\")\n",
    "plt.ylabel(\"Mean Point Distance\")\n",
    "plt.xlabel(\"#points\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(psf_holder.flatten(),mean_points.flatten(),\"o\")\n",
    "plt.ylabel(\"Mean Point Distance\")\n",
    "plt.xlabel(\"psf_sigma\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(psf_holder.flatten()*2.355/mean_points.flatten(),bins = 20)\n",
    "plt.xlabel(\"psf_sigma/mean_point_distance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plot the number of blobs and clusters as a function of density and number of points in a 3d plot\n",
    "xv, yv = np.meshgrid(psf_scan, point_scan)\n",
    "from matplotlib import cm\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "a1 = ax.plot_surface(xv.T*130,yv.T,num_blobs,cmap=cm.coolwarm)\n",
    "ax.set_xlabel(\"psf_sigma (nm)\")\n",
    "ax.set_ylabel(\"Number of Points\")\n",
    "ax.set_zlabel(\"Number of Blobs\")\n",
    "plt.title(\"Number of Blobs vs psf_sigma and Number of Points\")\n",
    "fig.colorbar(a1,shrink=0.5, aspect=5)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "a2 = ax.plot_surface(xv.T*130,yv.T,num_clusters,cmap=cm.coolwarm)\n",
    "ax.set_xlabel(\"psf_sigma (nm)\")\n",
    "ax.set_ylabel(\"Number of Points\")\n",
    "ax.set_zlabel(\"Number of Clusters\")\n",
    "plt.title(\"Number of Clusters vs psf_sigma and Number of Points\")\n",
    "fig.colorbar(a2,shrink=0.5, aspect=5)\n",
    "plt.show()\n",
    "\n",
    "#plot the error in blob size as a function of density and number of points in a 3d plot\n",
    "#relace all np.nan with -1\n",
    "min_blob_error[np.isnan(min_blob_error)] = -1\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "a3 = ax.plot_surface(xv.T*130,yv.T,min_blob_error,cmap=cm.coolwarm)\n",
    "ax.set_xlabel(\"psf_sigma (nm)\")\n",
    "ax.set_ylabel(\"Number of Points\")\n",
    "ax.set_zlabel(\"Error in Blob Size\")\n",
    "plt.title(\"Error in Blob Size vs psf_sigma and Number of Points\")\n",
    "fig.colorbar(a3,shrink=0.5, aspect=5)\n",
    "plt.show()\n",
    "\n",
    "#plot the error in cluster size as a function of psf_scan and number of points in a 3d plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "a4 = ax.plot_surface(xv.T*130,yv.T,min_cluster_error,cmap=cm.coolwarm)\n",
    "ax.set_xlabel(\"psf_sigma (nm)\")\n",
    "ax.set_ylabel(\"Number of Points\")\n",
    "ax.set_zlabel(\"Error in Cluster Size\")\n",
    "plt.title(\"Error in Cluster Size vs psf_sigma and Number of Points\")\n",
    "fig.colorbar(a4,shrink=0.5, aspect=5)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter3D(mean_point_distances,psf_point_distance_holder*130,min_blob_error,marker = \".\")\n",
    "ax.set_xlabel(\"Mean Distance between Points\")\n",
    "ax.set_ylabel(\"psf_sigma (nm)\")\n",
    "ax.set_zlabel(\"Error in Blob Size\")\n",
    "plt.title(\"Number of Blobs vs Mean Distance between Points\")\n",
    "plt.show()\n",
    "\n",
    "#do the same for num_blobs \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter3D(mean_point_distances,psf_point_distance_holder*130,num_blobs,marker = \".\")\n",
    "ax.set_xlabel(\"Mean Distance between Points\")\n",
    "ax.set_ylabel(\"psf_sigma (nm)\")\n",
    "ax.set_zlabel(\"Number of Blobs\")\n",
    "plt.title(\"Number of Blobs vs Mean Distance between Points\")\n",
    "plt.show()\n",
    "\n",
    "psf_mean_distance_ratio = psf_point_distance_holder/mean_point_distances\n",
    "point_point_distance_holder = np.zeros((len(psf_scan),len(point_scan)))\n",
    "\n",
    "for i in range(len(psf_scan)):\n",
    "    for k in range(len(point_scan)):\n",
    "        point_point_distance_holder[i,k] = point_scan[k]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "a1 = ax.scatter(psf_mean_distance_ratio.flatten()*2.355,num_blobs.flatten(),marker = \".\",c=point_point_distance_holder.flatten())\n",
    "ax.set_xlabel(\"Ratio of psf_sigma*FWHM to Mean Distance between Points\")\n",
    "ax.set_ylabel(\"Number of Blobs\")\n",
    "#plot a verticle line at 1\n",
    "ax.axvline(x=1, color='k', linestyle='--',label = \"Ratio = 1\")\n",
    "#plot a horizontal line at 1\n",
    "ax.axhline(y=1, color='g', linestyle='--',label = \"Number of Blobs = 1\")\n",
    "#colorbar\n",
    "plt.colorbar(a1)\n",
    "plt.legend()\n",
    "plt.title(\"Number of Blobs vs Ratio of psf_sigma*FWHM \\n to Mean Distance between Points\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "d1_blob_error = min_blob_error.flatten()\n",
    "d1_psf_dist_ratio_fwhm = psf_mean_distance_ratio.flatten()*2.355\n",
    "\n",
    "#only keep values above 0\n",
    "index_nan_error = np.isnan(d1_blob_error)\n",
    "index_nan_ratio = np.isnan(d1_psf_dist_ratio_fwhm)\n",
    "#only index ones where both are true\n",
    "index_nan = ~index_nan_error & ~index_nan_ratio\n",
    "\n",
    "d1_blob_error = d1_blob_error[index_nan]\n",
    "d1_psf_dist_ratio_fwhm = d1_psf_dist_ratio_fwhm[index_nan]\n",
    "point_pair_distances = point_point_distance_holder.flatten()[index_nan]\n",
    "\n",
    "\n",
    "index_error = d1_blob_error > 0\n",
    "d1_blob_error = d1_blob_error[index_error]\n",
    "d1_psf_dist_ratio_fwhm = d1_psf_dist_ratio_fwhm[index_error]\n",
    "point_pair_distances = point_pair_distances[index_error]\n",
    "\n",
    "#fit these values to a function of the form of a LINEX function\n",
    "def func(x,a,b,c,d,k):\n",
    "        return a*np.exp(-b*x)+c*x+d\n",
    "\n",
    "popt, pcov = curve_fit(func,d1_psf_dist_ratio_fwhm,d1_blob_error)\n",
    "\n",
    "#plot the fit\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "a1 = ax.scatter(d1_psf_dist_ratio_fwhm,d1_blob_error,marker = \".\",label = \"Data\",c=point_pair_distances)\n",
    "ax.plot(d1_psf_dist_ratio_fwhm,func(d1_psf_dist_ratio_fwhm,*popt),label = \"Fit\")\n",
    "ax.set_xlabel(\"Ratio of psf_sigma*FWHM to Mean Distance between Points\")\n",
    "ax.set_ylabel(\"Error in Blob Size\")\n",
    "plt.colorbar(a1)\n",
    "plt.legend()\n",
    "plt.title(\"Error in Blob Size vs Ratio of psf_sigma*FWHM \\n to Mean Distance between Points\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "a1 = ax.scatter(psf_mean_distance_ratio.flatten()*2.355,min_blob_error.flatten(),marker = \".\",c=point_point_distance_holder.flatten())\n",
    "ax.set_xlabel(\"Ratio of psf_sigma*FWHM to Mean Distance between Points\")\n",
    "ax.set_ylabel(\"Error in Blob Size\")\n",
    "#plot a verticle line at 1\n",
    "ax.axvline(x=1, color='k', linestyle='--',label = \"Ratio = 1\")\n",
    "#plot a horizontal line at 0\n",
    "ax.axhline(y=0, color='g', linestyle='--',label = \"Error in Blob Size = 0\")\n",
    "plt.legend()\n",
    "plt.colorbar(a1)\n",
    "plt.title(\"Error in Blob Size vs Ratio of psf_sigma*FWHM \\n to Mean Distance between Points\")\n",
    "plt.show()\n",
    "\n",
    "#do the same for num_clusters\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "a1 = ax.scatter(psf_mean_distance_ratio.flatten()*2.355,num_clusters.flatten(),marker = \".\",c=point_point_distance_holder.flatten())\n",
    "ax.set_xlabel(\"Ratio of psf_sigma*FWHM to Mean Distance between Points\")\n",
    "ax.set_ylabel(\"Number of Clusters\")\n",
    "#plot a verticle line at 1\n",
    "ax.axvline(x=1, color='k', linestyle='--',label = \"Ratio = 1\")\n",
    "#plot a horizontal line at 1\n",
    "ax.axhline(y=1, color='g', linestyle='--',label = \"Number of Clusters = 1\")\n",
    "plt.legend()\n",
    "plt.colorbar(a1)\n",
    "plt.title(\"Number of Clusters vs Ratio of psf_sigma*FWHM \\n to Mean Distance between Points\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "a1 = ax.scatter(psf_mean_distance_ratio.flatten()*2.355,min_cluster_error.flatten(),marker = \".\",c=point_point_distance_holder.flatten())\n",
    "ax.set_xlabel(\"Ratio of psf_sigma*FWHM to Mean Distance between Points\")\n",
    "ax.set_ylabel(\"Error in Cluster Size\")          \n",
    "#plot a verticle line at 1\n",
    "ax.axvline(x=1, color='k', linestyle='--',label = \"Ratio = 1\")\n",
    "#plot a horizontal line at 0\n",
    "ax.axhline(y=0, color='g', linestyle='--',label = \"Error in Cluster Size = 0\")\n",
    "plt.legend()\n",
    "plt.colorbar(a1)\n",
    "plt.title(\"Error in Cluster Size vs Ratio of psf_sigma*FWHM \\n to Mean Distance between Points\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dict = {\"doc\": \"There is 2 independent parameters 1) point_scan, 2) psf_scan \\n and 5 dependent parameters 1) num_blobs, 2) num_clusters, 3) min_blob_error, 4) min_cluster_error, 5) mean_point_distances, 6) psf_point_distance_holder\",\n",
    "        \"point_scan\":point_scan,\n",
    "        \"density_scan\":density_scan,\n",
    "        \"num_blobs\":num_blobs,\n",
    "        \"num_clusters\":num_clusters,\n",
    "        \"min_blob_error\":min_blob_error,\n",
    "        \"min_cluster_error\":min_cluster_error,\n",
    "        \"mean_point_distances\":mean_point_distances,\n",
    "        \"psf_point_distance_holder\":psf_point_distance_holder}\n",
    "\n",
    "#save the dictionary as a pickle file\n",
    "with open(\"/Users/baljyot/Documents/2022-2023/PhD_Thesis/pickled_data/num_blobs_psf_dense/num_blobs_clusters_psf.pkl\",\"wb\") as f:\n",
    "        pickle.dump(dict,f)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_scan = np.linspace(30,1000,10)\n",
    "dprime_scan = np.linspace(1,10,10)\n",
    "from scipy.spatial import distance\n",
    "num_clusters = np.zeros((len(point_scan),len(dprime_scan)))\n",
    "min_cluster_error = np.zeros((len(point_scan),len(dprime_scan)))\n",
    "mean_point_distances = np.zeros((len(point_scan),len(dprime_scan)))\n",
    "point_scan_holder = np.zeros((len(point_scan),len(dprime_scan)))\n",
    "dprime_scan_holder = np.zeros((len(point_scan),len(dprime_scan)))\n",
    "\n",
    "for i in range(len(point_scan)):\n",
    "    for j in range(len(dprime_scan)):\n",
    "        point_scan_holder[i,j] = point_scan[i]\n",
    "        dprime_scan_holder[i,j] = dprime_scan[j]\n",
    "\n",
    "\n",
    "repeat = 3\n",
    "\n",
    "for i in range(len(point_scan)):\n",
    "    for j in range(len(dprime_scan)):\n",
    "        mean_num_cluster = 0\n",
    "        error_min_cluster = 0\n",
    "        mean_point_distance = 0\n",
    "        for tt in range(repeat):\n",
    "            sim = simulate_foci.sim_foci(max_x = 50,\n",
    "                        min_x = 0,\n",
    "                        radius = 2,\n",
    "                        center = [20.,25.],\n",
    "                        total_points = int(point_scan[i]),\n",
    "                        density_dif = 20,\n",
    "                        pdf = simulate_foci.tophat_function_2d,\n",
    "                        point_intensity = 40,\n",
    "                        projection_frames = 1000)\n",
    "            sim.uniform_blob = False\n",
    "            sim.psf_sigma = 0.82\n",
    "            sim.base_noise = 140\n",
    "            map,sim_xy = sim.simulate_point(generator = None,movie=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #find the mean distance between points\n",
    "            #only use the points in the circle of radius 2 at the center\n",
    "            sim_xy_cluster = sim_xy[np.where(np.sqrt((sim_xy[:,0] - 20)**2 + (sim_xy[:,1] - 25)**2) <= 2)]\n",
    "            mean_point_distance += np.mean(distance.pdist(sim_xy_cluster))\n",
    "            clustering = DBSCAN(dprime_scan[j],min_samples=20).fit(sim_xy)\n",
    "            mean_num_cluster = len(np.unique(clustering.labels_))\n",
    "            #find the error in the cluster size\n",
    "            min_error = 10\n",
    "            #for each cluster use the smallest enclosing circle to find the radius of the cluster\n",
    "            for kk in np.unique(clustering.labels_):\n",
    "                #find the points in the cluster\n",
    "                cluster_points = sim_xy[np.where(clustering.labels_ == kk)]\n",
    "                #find the smallest enclosing circle\n",
    "                center_x,center_y,radius = smallestenclosingcircle.make_circle(cluster_points)\n",
    "                #find the error\n",
    "                if np.abs(radius - sim.radius)/sim.radius < min_error:\n",
    "                    min_error = np.abs(radius - sim.radius)/sim.radius\n",
    "            if min_error == 10:\n",
    "                min_error = -1\n",
    "            error_min_cluster += min_error\n",
    "            \n",
    "            print(\"done #points {0}, dprime {1}, repeat {2}\".format(point_scan[i],dprime_scan[j],tt))\n",
    "        num_clusters[i,j] = mean_num_cluster/repeat\n",
    "        min_cluster_error[i,j] = error_min_cluster/repeat\n",
    "        mean_point_distances[i,j] = mean_point_distance/repeat\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {\"doc\": \"There is 2 independent parameters 1) point_scan, 2) dprime_scan \\n and 3 dependent parameters 1) num_clusters, 2) min_cluster_error, 3) mean_point_distances\",\n",
    "        \"point_scan\":point_scan,\n",
    "        \"dprime_scan\":dprime_scan,\n",
    "        \"num_clusters\":num_clusters,\n",
    "        \"min_cluster_error\":min_cluster_error,\n",
    "        \"mean_point_distances\":mean_point_distances,\n",
    "        \"dp_scan_holder\":dprime_scan_holder,\n",
    "        \"point_scan_holder\":point_scan_holder}\n",
    "\n",
    "#save the dictionary as a pickle file\n",
    "#make the directory if it does not exist\n",
    "if not os.path.exists(\"/Users/baljyot/Documents/2022-2023/PhD_Thesis/pickled_data/num_clusters_DBSCAN\"):\n",
    "    os.makedirs(\"/Users/baljyot/Documents/2022-2023/PhD_Thesis/pickled_data/num_clusters_DBSCAN\")\n",
    "with open(\"/Users/baljyot/Documents/2022-2023/PhD_Thesis/pickled_data/num_clusters_DBSCAN/num_clusters_error.pkl\",\"wb\") as f:\n",
    "        pickle.dump(dict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the num of clusters as a function of dprime and point_scan in a 3d plot\n",
    "xv, yv = np.meshgrid(dprime_scan, point_scan)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(xv.T,yv.T,num_clusters)\n",
    "ax.set_xlabel(\"point_scan\")\n",
    "ax.set_ylabel(\"dprime_scan\")\n",
    "ax.set_zlabel(\"num_clusters\")\n",
    "plt.savefig(\"/Users/baljyot/Documents/2022-2023/PhD_Thesis/pickled_data/num_clusters_DBSCAN/num_clusters.png\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "#plot the min_cluster_error as a function of dprime and point_scan in a 3d plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(xv.T,yv.T,min_cluster_error)\n",
    "ax.set_xlabel(\"point_scan\")\n",
    "ax.set_ylabel(\"dprime_scan\")\n",
    "ax.set_zlabel(\"min_cluster_error\")\n",
    "plt.savefig(\"/Users/baljyot/Documents/2022-2023/PhD_Thesis/pickled_data/num_clusters_DBSCAN/min_cluster_error.png\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "#ratio of dp_scan/mean_point_distances\n",
    "ratio = (dprime_scan_holder/mean_point_distances).flatten() + 0.2\n",
    "flat_error = min_cluster_error.flatten()\n",
    "flat_num_clusters = num_clusters.flatten()\n",
    "flat_point_scan = point_scan_holder.flatten()\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "a1 = ax.scatter(ratio,flat_error,c = flat_point_scan)\n",
    "ax.set_xlabel(\"D_scan/optimal_D\")\n",
    "ax.set_ylabel(\"min_cluster_error\")\n",
    "#plot a horizontal line at 0 with label \"Line below no clusters itentified\"\n",
    "ax.axhline(y=0, color='r', linestyle='--',label = \"Line below no clusters itentified\")\n",
    "#legend\n",
    "ax.legend()\n",
    "#colorbar\n",
    "cbar = plt.colorbar(a1)\n",
    "#set ylim to 0\n",
    " \n",
    "cbar.set_label(\"points\")\n",
    "plt.savefig(\"/Users/baljyot/Documents/2022-2023/PhD_Thesis/pickled_data/num_clusters_DBSCAN/min_cluster_error_vs_ratio.png\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "#define a metric that is the error from optimal dprime/mean_point_distances (1)\n",
    "optimal_ratio = 1\n",
    "ratio_error = abs(ratio - optimal_ratio)/optimal_ratio\n",
    "\n",
    "#plot the min_cluster_error as a function of the ratio_error\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "a1 = ax.scatter(ratio_error,flat_error,c = flat_point_scan)\n",
    "ax.set_xlabel(\"difference from optimal D\")\n",
    "ax.set_ylabel(\"min_cluster_error\")\n",
    "#plot a horizontal line at 0 with label \"Line below no clusters itentified\"\n",
    "ax.axhline(y=0, color='r', linestyle='--',label = \"Line below no clusters itentified\")\n",
    "#legend\n",
    "ax.legend()\n",
    "#colorbar\n",
    "cbar = plt.colorbar(a1)\n",
    "cbar.set_label(\"points\")\n",
    "plt.savefig(\"/Users/baljyot/Documents/2022-2023/PhD_Thesis/pickled_data/num_clusters_DBSCAN/min_cluster_error_vs_ratio_error.png\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "a1 = ax.scatter(ratio,flat_num_clusters,c = flat_point_scan)\n",
    "ax.set_xlabel(\"dprime_scan/mean_point_distances\")\n",
    "ax.set_ylabel(\"num_clusters\")\n",
    "#colorbar\n",
    "cbar = plt.colorbar(a1)   \n",
    "cbar.set_label(\"points\")\n",
    "plt.savefig(\"/Users/baljyot/Documents/2022-2023/PhD_Thesis/pickled_data/num_clusters_DBSCAN/num_clusters_vs_ratio.png\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.SMT_Analysis_BP.helpers.condensate_movement as cm\n",
    "\n",
    "temper = cm.Condensate(inital_position=np.array([0,0]),\\\n",
    "                       initial_time=0,\\\n",
    "                        diffusion_coefficient=0.001,\\\n",
    "                        hurst_exponent=0.2,\\\n",
    "                        units_time='ms',\\\n",
    "                        initial_scale=2)\n",
    "temper(2000,'ms')\n",
    "fig,ax = plt.subplots()\n",
    "temper.plot_condensate(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isotropic residual gaussain\n",
    "def iso_gaus(p,x,y,z):\n",
    "    p[\"sigma_x\"] = p[\"sigma_y\"]\n",
    "    return residuals_gaus2d(p,x,y,z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pionts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radii = np.linspace(0.1,7,50)\n",
    "sim_args = {\"max_x\":50,\n",
    "            \"min_x\":0,\n",
    "            \"center\":[25,25],\n",
    "            \"total_points\":300,\n",
    "            \"density_dif\":1e7,\n",
    "            \"pdf\":simulate_foci.tophat_function_2d,\n",
    "            \"uniform\":True,\n",
    "            \"psf_sigma\":0.82}\n",
    "detection_args = {\"median\":False,\n",
    "                \"threshold\":1e-2,\n",
    "                \"min_sigma\":1./np.sqrt(2),\n",
    "                \"max_sigma\":10./np.sqrt(2),\n",
    "                \"num_sigma\":200,\n",
    "                \"overlap\":0,\n",
    "                \"logscale\":False,\n",
    "                \"verbose\":True}\n",
    "fitting_args = {\"mask_size\":5,\n",
    "                \"plot_fit\":False,\n",
    "                \"fitting_image\":\"Original\",\n",
    "                \"radius_func\":np.mean,\n",
    "                \"residual_func\":residuals_gaus2d,\n",
    "                \"sigma_range\":2}\n",
    "track_args = {\"track_length_mean\":10,\n",
    "            \"track_type\":\"fbm\",\n",
    "            \"track_hurst\":0.2,\n",
    "            \"track_distribution\":\"uniform\",\n",
    "            \"diffusion_coefficient\":1e-2}\n",
    "a = simulate_foci.sim_focii(radii=radii,repeats=3,detection_kwargs=detection_args,sim_kwargs=sim_args,fitting_parm=fitting_args,track_parm=track_args)\n",
    "a.use_points = True\n",
    "point_density = 10\n",
    "c = a.radius_analysis()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(radii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_rp_rad = 206.2 #nm\n",
    "max_rp_rad = 375.0 #nm\n",
    "\n",
    "min_ll_rad = 160.7 #nm\n",
    "max_ll_rad = 252.0 #nm\n",
    "\n",
    "min_hupa_rad = 240 #nm\n",
    "max_hupa_rad = 720 #nm\n",
    "\n",
    "\n",
    "#find the upper and lower confidence intervals for each fit_mean and sig_mean\n",
    "#use z= 1.96 for 95% confidence interval\n",
    "#use n=5 for 5 repeats\n",
    "#use the fit_std and sig_std for standard deviation\n",
    "upper_ci_fit = np.asarray(c[\"fit_mean\"]) + 1.96*np.asarray(c[\"fit_stds\"])/np.sqrt(5)\n",
    "lower_ci_fit = np.asarray(c[\"fit_mean\"]) - 1.96*np.asarray(c[\"fit_stds\"])/np.sqrt(5)\n",
    "\n",
    "upper_ci_sig = np.asarray(c[\"sig_mean\"]) + 1.96*np.asarray(c[\"sig_std\"])/np.sqrt(5)\n",
    "lower_ci_sig = np.asarray(c[\"sig_mean\"]) - 1.96*np.asarray(c[\"sig_std\"])/np.sqrt(5)\n",
    "\n",
    "#using the upper and lower confidence intervals, create a fill_between plot while plotting the fit_mean and sig_mean\n",
    "plt.clf()\n",
    "#make a figure with 1 subplot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "#redo the below with ax. rather than plt. \n",
    "\n",
    "ax.fill_between(np.asarray(radii)*globals[\"olympus_pixel_size\"],upper_ci_fit*globals[\"olympus_pixel_size\"],lower_ci_fit*globals[\"olympus_pixel_size\"],alpha=0.5)\n",
    "ax.plot(np.asarray(radii)*globals[\"olympus_pixel_size\"],np.asarray(c[\"fit_mean\"])*globals[\"olympus_pixel_size\"],label = \"Gaussian Fit\")\n",
    "ax.fill_between(np.asarray(radii)*globals[\"olympus_pixel_size\"],upper_ci_sig*globals[\"olympus_pixel_size\"],lower_ci_sig*globals[\"olympus_pixel_size\"],alpha=0.5)\n",
    "ax.plot(np.asarray(radii)*globals[\"olympus_pixel_size\"],np.asarray(c[\"sig_mean\"])*globals[\"olympus_pixel_size\"],label = \"Scale-space Fit\")\n",
    "\n",
    "#plot a straight line with slope 1\n",
    "ax.plot(np.asarray(radii)*globals[\"olympus_pixel_size\"],np.asarray(radii)*globals[\"olympus_pixel_size\"])\n",
    "#plot a horizontal dashed line at the minumim blob size of psf_sigma*sqrt(2)*globals[\"olympus_pixel_size\"]\n",
    "ax.axhline(y=sim_args[\"psf_sigma\"]*np.sqrt(2)*globals[\"olympus_pixel_size\"],color=\"black\",linestyle=\"--\",label=\"Minimum Blob Size\")\n",
    "\n",
    "\n",
    "min_rad = np.min(np.asarray(radii)*globals[\"olympus_pixel_size\"])\n",
    "max_rad = np.max(np.asarray(radii)*globals[\"olympus_pixel_size\"])\n",
    "\n",
    "ax.set_ylabel(\"Found Radius (nm)\")\n",
    "ax.set_xlabel(\"Simulated Radius (nm)\")\n",
    "#set the xlim and ylim to 0 \n",
    "ax.set_xlim(0,max_hupa_rad)\n",
    "ax.set_ylim(0)\n",
    "\n",
    "#make a rectangle with the hupa radii\n",
    "hupa = mpatches.Rectangle((min_hupa_rad, 0), max_hupa_rad-min_hupa_rad, 20,linewidth=1,fill=True,color=\"red\",alpha=0.2,label=\"hupA Experimental Radii\")\n",
    "ax.add_patch(hupa)\n",
    "#make a rectangle with the ll radii\n",
    "ll = mpatches.Rectangle((min_ll_rad, 41), max_ll_rad-min_ll_rad, 20,linewidth=1,fill=True,color=\"green\",alpha=0.2,label=\"ll Experimental Radii\")\n",
    "ax.add_patch(ll)\n",
    "#make a rectangle with the rp radii\n",
    "rp = mpatches.Rectangle((min_rp_rad, 21), max_rp_rad-min_rp_rad, 20,linewidth=1,fill=True,color=\"blue\",alpha=0.2,label=\"rp Experimental Radii\")\n",
    "ax.add_patch(rp)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(np.asarray(radii)*globals[\"olympus_pixel_size\"],100*np.abs(np.asarray(c[\"fit_mean\"])-np.asarray(radii))/np.asarray(radii),label = \"Gaussian Fit\")\n",
    "ax.plot(np.asarray(radii)*globals[\"olympus_pixel_size\"],100*np.abs(np.asarray(c[\"sig_mean\"])-np.asarray(radii))/np.asarray(radii),label = \"Space-Space\")\n",
    "#plt.plot(np.linspace(0,10,100),np.linspace(0,10,100),'r-')\n",
    "#print(radius/np.sqrt(2),found_radius,std_radius)\n",
    "# plt.ylim((0,4))\n",
    "# plt.xlim((0,4))\n",
    "plt.ylabel(\"Percent Error\")\n",
    "plt.xlabel(\"Simulated Radius (nm)\")\n",
    "\n",
    "#make a rectangle with the hupa radii\n",
    "hupa = mpatches.Rectangle((min_hupa_rad, 30), max_hupa_rad-min_hupa_rad, 10,linewidth=1,fill=True,color=\"red\",alpha=0.2,label=\"hupA Experimental Radii\")\n",
    "ax.add_patch(hupa)\n",
    "#make a rectangle with the ll radii\n",
    "ll = mpatches.Rectangle((min_ll_rad, 52), max_ll_rad-min_ll_rad, 10,linewidth=1,fill=True,color=\"green\",alpha=0.2,label=\"ll Experimental Radii\")\n",
    "ax.add_patch(ll)\n",
    "#make a rectangle with the rp radii\n",
    "rp = mpatches.Rectangle((min_rp_rad, 41), max_rp_rad-min_rp_rad, 10,linewidth=1,fill=True,color=\"blue\",alpha=0.2,label=\"rp Experimental Radii\")\n",
    "ax.add_patch(rp)\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#redo this plot and make it look nice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.errorbar((np.asarray(radii))*globals[\"olympus_pixel_size\"],(np.asarray(c[\"fit_mean\"])-np.asarray(radii))*globals[\"olympus_pixel_size\"],yerr=globals[\"olympus_pixel_size\"]*(np.asarray(c[\"fit_stds\"])),label = \"Gaussian Fit\")\n",
    "plt.errorbar((np.asarray(radii))*globals[\"olympus_pixel_size\"],(np.asarray(c[\"sig_mean\"])-np.asarray(radii))*globals[\"olympus_pixel_size\"],yerr=globals[\"olympus_pixel_size\"]*(np.asarray(c[\"sig_std\"])),label = \"Scale-Space\")\n",
    "#plt.plot(np.linspace(0,10,100),np.linspace(0,10,100),'r-')\n",
    "#print(radius/np.sqrt(2),found_radius,globals[pixel_size]*std_radius)\n",
    "# plt.ylim((0,4))\n",
    "# plt.xlim((0,4))\n",
    "plt.legend()\n",
    "#plt.yscale(\"log\")\n",
    "plt.ylabel(\"Error from True (nm)\")\n",
    "plt.xlabel(\"Simulated Radius (nm)\")\n",
    "plt.axvspan(min_rp_rad, max_rp_rad, alpha=0.2, color='red')\n",
    "plt.axvspan(min_ll_rad, max_ll_rad, alpha=0.2, color='green')\n",
    "#do the same for hupa\n",
    "plt.axvspan(min_hupa_rad, max_hupa_rad, alpha=0.2, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## changing alpha values for tracks used, constant total_points\n",
    "radii = np.linspace(0.5,4.5,20)\n",
    "sim_args = {\"max_x\":50,\n",
    "            \"min_x\":0,\n",
    "            \"center\":[25,25],\n",
    "            \"total_points\":500,\n",
    "            \"density_dif\":1e7,\n",
    "            \"pdf\":simulate_foci.tophat_function_2d,\n",
    "            \"uniform\":True,\n",
    "            \"psf_sigma\":0.82}\n",
    "detection_args = {\"median\":False,\n",
    "                \"threshold\":1e-2,\n",
    "                \"min_sigma\":1./np.sqrt(2),\n",
    "                \"max_sigma\":10./np.sqrt(2),\n",
    "                \"num_sigma\":200,\n",
    "                \"overlap\":0,\n",
    "                \"logscale\":False,\n",
    "                \"verbose\":True}\n",
    "fitting_args = {\"mask_size\":5,\n",
    "                \"plot_fit\":False,\n",
    "                \"fitting_image\":\"Original\",\n",
    "                \"radius_func\":np.mean,\n",
    "                \"residual_func\":residuals_gaus2d,\n",
    "                \"sigma_range\":2}\n",
    "track_args = {\"track_length_mean\":10,\n",
    "            \"track_type\":\"fbm\",\n",
    "            \"track_hurst\":0.35,\n",
    "            \"track_distribution\":\"uniform\",\n",
    "            \"diffusion_coefficient\":1e-2}\n",
    "\n",
    "#hurst values from 0.15 to 0.5\n",
    "hursts = np.linspace(0.15,0.5,10)\n",
    "data_store = {}\n",
    "\n",
    "for i in hursts:\n",
    "    track_args[\"track_hurst\"] = i\n",
    "    a = simulate_foci.sim_focii(radii=radii,repeats=3,detection_kwargs=detection_args,sim_kwargs=sim_args,fitting_parm=fitting_args,track_parm=track_args)\n",
    "    a.use_points=False\n",
    "    data_store[i] = a.total_points_radius_analysis(total_points=250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the results of the hurst analysis, the hurst value vs percent error for each radii\n",
    "plt.clf()\n",
    "#make a figure with one subplot\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,5))\n",
    "for i,c in data_store.items():\n",
    "    #make the label with the hurst to the closest decimal\n",
    "    ax.errorbar(np.asarray(radii)*globals[\"olympus_pixel_size\"],100*np.abs(np.asarray(c[\"sig_mean\"])-np.asarray(radii))/np.asarray(radii),yerr=100*np.asarray(c[\"sig_std\"])/np.asarray(radii),label = \"Hurst = \"+str(np.around(i,3)))\n",
    "ax.axvspan(min_rp_rad, max_rp_rad, alpha=0.2, color='red',label = \"rp_ez_spread\")\n",
    "ax.axvspan(min_ll_rad, max_ll_rad, alpha=0.2, color='green',label = \"ll_ez_spread\")\n",
    "#do the same for hupa\n",
    "ax.axvspan(min_hupa_rad, max_hupa_rad, alpha=0.2, color='blue',label = \"hupa_spread\")\n",
    "ax.set_ylabel(\"Percent Error\")\n",
    "ax.set_xlabel(\"Simulated Radius (nm)\")\n",
    "ax.set_title(\"Percent Error vs Simulated Radius for Different Hurst Values \\n Using 200 Total Points per Blob\")\n",
    "ax.legend()\n",
    "#make the plot look nice\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "radius change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## changing points simulated for each radius using tracks instead of random points\n",
    "radii = np.linspace(0.1,4.5,5)\n",
    "sim_args = {\"max_x\":50,\n",
    "            \"min_x\":0,\n",
    "            \"center\":[25,25],\n",
    "            \"total_points\":500,\n",
    "            \"density_dif\":1e7,\n",
    "            \"pdf\":simulate_foci.tophat_function_2d,\n",
    "            \"uniform\":True,\n",
    "            \"psf_sigma\":0.82}\n",
    "detection_args = {\"median\":False,\n",
    "                \"threshold\":2e1,\n",
    "                \"min_sigma\":1./np.sqrt(2),\n",
    "                \"max_sigma\":10./np.sqrt(2),\n",
    "                \"num_sigma\":200,\n",
    "                \"overlap\":0,\n",
    "                \"logscale\":False,\n",
    "                \"verbose\":True}\n",
    "fitting_args = {\"mask_size\":5,\n",
    "                \"plot_fit\":False,\n",
    "                \"fitting_image\":\"Original\",\n",
    "                \"radius_func\":np.mean,\n",
    "                \"residual_func\":residuals_gaus2d,\n",
    "                \"sigma_range\":2}\n",
    "track_args = {\"track_length_mean\":10,\n",
    "            \"track_type\":\"fbm\",\n",
    "            \"track_hurst\":0.35,\n",
    "            \"track_distribution\":\"uniform\",\n",
    "            \"diffusion_coefficient\":1e-2}\n",
    "a = simulate_foci.sim_focii(radii=radii,repeats=1,detection_kwargs=detection_args,sim_kwargs=sim_args,fitting_parm=fitting_args,track_parm=track_args)\n",
    "a.use_points=True\n",
    "total_points = np.arange(0,500,100) + 1\n",
    "data_store = {}\n",
    "for i in total_points:\n",
    "    data_store[i] = a.total_points_radius_analysis(total_points=i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "for i,c in data_store.items():\n",
    "    #plt.clf()\n",
    "    #plt.errorbar(np.asarray(radii)*globals[pixel_size],100*np.abs(np.asarray(c[\"fit_mean\"])-np.asarray(radii))/np.asarray(radii),yerr=100*np.asarray(c[\"fit_stds\"])/np.asarray(radii),label = \"Gaussian Fit Points:{0}\".format(i))\n",
    "    plt.errorbar(np.asarray(radii)*globals[\"olympus_pixel_size\"],100*np.abs(np.asarray(c[\"sig_mean\"])-np.asarray(radii))/np.asarray(radii),yerr=100*np.asarray(c[\"sig_std\"])/np.asarray(radii),label = \"Space-Space Points:{0}\".format(i))\n",
    "    #plt.plot(np.linspace(0,10,100),np.linspace(0,10,100),'r-')\n",
    "    #print(radius/np.sqrt(2),found_radius,std_radius)\n",
    "    # plt.ylim((0,4))\n",
    "    # plt.xlim((0,4))\n",
    "plt.ylabel(\"Percent Error\")\n",
    "plt.xlabel(\"Simulated Radius (nm)\")\n",
    "#plt.axvspan(min_rp_rad, max_rp_rad, alpha=0.2, color='red',label = \"rp_ez_spread\")\n",
    "#plt.axvspan(min_ll_rad, max_ll_rad, alpha=0.2, color='green',label = \"ll_ez_spread\")\n",
    "#do the same for the hupa \n",
    "\n",
    "plt.ylim((-5,150))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RPOC_HEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_hex5= run_analysis(wd=\"/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/15/rp_ez_hex5_2\",\n",
    "                    t_string=\"rp_ez_hex5\")\n",
    "                    \n",
    "rp_hex5.read_parameters(minimum_percent_per_drop_in = .5, \n",
    "                    t_len_u = 25, \n",
    "                    t_len_l=4, \n",
    "                    minimum_tracks_per_drop = 3,lower_bp=0.5)\n",
    "\n",
    "rp_hex5.get_blob_parameters(threshold=8e-4,\n",
    "                        overlap=0,\n",
    "                        detection_name='bp',\n",
    "                        min_sigma=1/np.sqrt(2),\n",
    "                        max_sigma=3/np.sqrt(2),\n",
    "                        num_sigma=100,\n",
    "                        median = False)\n",
    "\n",
    "rp_hex5.get_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                    \"plot_fit\":False,\n",
    "                                    \"fitting_image\":\"Original\",\n",
    "                                    \"radius_func\":np.mean,#identity,\n",
    "                                    \"residual_func\":residuals_gaus2d,\n",
    "                                    \"sigma_range\":0.2,\n",
    "                                    \"centroid_range\":0.2,\n",
    "                                    \"height_range\":2})\n",
    "rp_hex5.type_of_blob = \"TRACKMATE\"\n",
    "rp_hex5.run_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_hex5._make_SMAUG_files()\n",
    "rp_hex5._make_NOBIAS_files()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RPOC_M9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_m9= run_analysis(wd=\"/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190524/rpoc_m9\",\n",
    "                    t_string=\"rpoc_M9\")\n",
    "                    \n",
    "rp_m9.read_parameters(minimum_percent_per_drop_in = .5, \n",
    "                    t_len_u = 25, \n",
    "                    t_len_l=4, \n",
    "                    minimum_tracks_per_drop = 3,lower_bp=0.5)\n",
    "\n",
    "rp_m9.get_blob_parameters(threshold=8e-4,\n",
    "                        overlap=0,\n",
    "                        detection_name='bp',\n",
    "                        min_sigma=1/np.sqrt(2),\n",
    "                        max_sigma=3/np.sqrt(2),\n",
    "                        num_sigma=100,\n",
    "                        median = False)\n",
    "\n",
    "rp_m9.get_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                    \"plot_fit\":False,\n",
    "                                    \"fitting_image\":\"Original\",\n",
    "                                    \"radius_func\":np.mean,#identity,\n",
    "                                    \"residual_func\":residuals_gaus2d,\n",
    "                                    \"sigma_range\":0.2,\n",
    "                                    \"centroid_range\":0.2,\n",
    "                                    \"height_range\":2})\n",
    "rp_m9.type_of_blob = \"TRACKMATE\"\n",
    "rp_m9.run_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_m9._make_SMAUG_files()\n",
    "rp_m9._make_NOBIAS_files()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rpoc_ez_ceph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_ez_ceph= run_analysis(wd=\"/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/chephalexin/20190731/rpoc_ez_50_ceph\",\n",
    "                    t_string=\"rpoc_ez_50ceph\")\n",
    "                    \n",
    "rp_ez_ceph.read_parameters(minimum_percent_per_drop_in = .5, \n",
    "                    t_len_u = 25, \n",
    "                    t_len_l=10, \n",
    "                    minimum_tracks_per_drop = 3,lower_bp=0.5)\n",
    "\n",
    "rp_ez_ceph.get_blob_parameters(threshold=8e-4,\n",
    "                        overlap=0,\n",
    "                        detection_name='bp',\n",
    "                        min_sigma=1/np.sqrt(2),\n",
    "                        max_sigma=3/np.sqrt(2),\n",
    "                        num_sigma=100,\n",
    "                        median = False)\n",
    "\n",
    "rp_ez_ceph.get_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                    \"plot_fit\":False,\n",
    "                                    \"fitting_image\":\"Original\",\n",
    "                                    \"radius_func\":np.mean,#identity,\n",
    "                                    \"residual_func\":residuals_gaus2d,\n",
    "                                    \"sigma_range\":0.2,\n",
    "                                    \"centroid_range\":0.2,\n",
    "                                    \"height_range\":2})\n",
    "rp_ez_ceph.type_of_blob = \"TRACKMATE\"\n",
    "rp_ez_ceph.run_flow()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rpoc_ez_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rp_ez_2= run_analysis(wd=\"/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/Other_RPOC\",\n",
    "                    t_string=\"rpoc\")\n",
    "                    \n",
    "rp_ez_2.read_parameters(minimum_percent_per_drop_in = .5, \n",
    "                    t_len_u = 100, \n",
    "                    t_len_l=10, \n",
    "                    minimum_tracks_per_drop = 3,lower_bp=0.5)\n",
    "\n",
    "rp_ez_2.get_blob_parameters(threshold=8e-4,\n",
    "                        overlap=0,\n",
    "                        detection_name='bp',\n",
    "                        min_sigma=1/np.sqrt(2),\n",
    "                        max_sigma=3/np.sqrt(2),\n",
    "                        num_sigma=100,\n",
    "                        median = False)\n",
    "\n",
    "rp_ez_2.get_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                    \"plot_fit\":False,\n",
    "                                    \"fitting_image\":\"Original\",\n",
    "                                    \"radius_func\":np.mean,#identity,\n",
    "                                    \"residual_func\":residuals_gaus2d,\n",
    "                                    \"sigma_range\":0.2,\n",
    "                                    \"centroid_range\":0.2,\n",
    "                                    \"height_range\":2})\n",
    "rp_ez_2.type_of_blob = \"TRACKMATE\"\n",
    "rp_ez_2.run_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_ez_3= run_analysis(wd=\"/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/RPOC_new\",\n",
    "                    t_string=\"RPOC\")\n",
    "                    \n",
    "rp_ez_3.read_parameters(minimum_percent_per_drop_in = .5, \n",
    "                    t_len_u = 100, \n",
    "                    t_len_l=10, \n",
    "                    minimum_tracks_per_drop = 3,lower_bp=0.5)\n",
    "\n",
    "rp_ez_3.get_blob_parameters(threshold=8e-4,\n",
    "                        overlap=0,\n",
    "                        detection_name='bp',\n",
    "                        min_sigma=1/np.sqrt(2),\n",
    "                        max_sigma=3/np.sqrt(2),\n",
    "                        num_sigma=100,\n",
    "                        median = False)\n",
    "\n",
    "rp_ez_3.get_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                    \"plot_fit\":False,\n",
    "                                    \"fitting_image\":\"Original\",\n",
    "                                    \"radius_func\":np.mean,#identity,\n",
    "                                    \"residual_func\":residuals_gaus2d,\n",
    "                                    \"sigma_range\":0.2,\n",
    "                                    \"centroid_range\":0.2,\n",
    "                                    \"height_range\":2})\n",
    "rp_ez_3.type_of_blob = \"TRACKMATE\"\n",
    "rp_ez_3.run_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_ez_4= run_analysis(wd=\"/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/20190620\",\n",
    "                    t_string=\"rpoc_ez\")\n",
    "                    \n",
    "rp_ez_4.read_parameters(minimum_percent_per_drop_in = .5, \n",
    "                    t_len_u = 100, \n",
    "                    t_len_l=10, \n",
    "                    minimum_tracks_per_drop = 3,lower_bp=0.5)\n",
    "\n",
    "rp_ez_4.get_blob_parameters(threshold=8e-4,\n",
    "                        overlap=0,\n",
    "                        detection_name='bp',\n",
    "                        min_sigma=1/np.sqrt(2),\n",
    "                        max_sigma=3/np.sqrt(2),\n",
    "                        num_sigma=100,\n",
    "                        median = False)\n",
    "\n",
    "rp_ez_4.get_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                    \"plot_fit\":False,\n",
    "                                    \"fitting_image\":\"Original\",\n",
    "                                    \"radius_func\":np.mean,#identity,\n",
    "                                    \"residual_func\":residuals_gaus2d,\n",
    "                                    \"sigma_range\":0.2,\n",
    "                                    \"centroid_range\":0.2,\n",
    "                                    \"height_range\":2})\n",
    "rp_ez_4.type_of_blob = \"TRACKMATE\"\n",
    "rp_ez_4.run_flow()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RPOC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_ez= run_analysis(wd=\"/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez\",\n",
    "                    t_string=\"rpoc_ez\")\n",
    "                    \n",
    "rp_ez.read_parameters(minimum_percent_per_drop_in = .5, \n",
    "                    t_len_u = 100, \n",
    "                    t_len_l=10, \n",
    "                    minimum_tracks_per_drop = 3,lower_bp=0.5)\n",
    "\n",
    "rp_ez.get_blob_parameters(threshold=8e-4,\n",
    "                        overlap=0,\n",
    "                        detection_name='bp',\n",
    "                        min_sigma=1/np.sqrt(2),\n",
    "                        max_sigma=3/np.sqrt(2),\n",
    "                        num_sigma=100,\n",
    "                        median = False)\n",
    "\n",
    "rp_ez.get_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                    \"plot_fit\":False,\n",
    "                                    \"fitting_image\":\"Original\",\n",
    "                                    \"radius_func\":np.mean,#identity,\n",
    "                                    \"residual_func\":residuals_gaus2d,\n",
    "                                    \"sigma_range\":0.2,\n",
    "                                    \"centroid_range\":0.2,\n",
    "                                    \"height_range\":2})\n",
    "rp_ez.type_of_blob = \"TRACKMATE\"\n",
    "#rp_ez.a_file_style = \"new\"\n",
    "rp_ez.run_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/Analysis_new/rpoc_ez_1_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/Analysis_new/rpoc_ez_2_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/Analysis_new/rpoc_ez_3_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/Analysis_new/rpoc_ez_4_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/Analysis_new/rpoc_ez_5_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/Analysis_new/rpoc_ez_6_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/Analysis_new/rpoc_ez_7_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/Analysis_new/rpoc_ez_8_seg.tif_spots.csv']\n",
      "['/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/1_rpoc_ez_1_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/2_rpoc_ez_1_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/3_rpoc_ez_1_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/4_rpoc_ez_1_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/5_rpoc_ez_1_seg.tif']\n",
      "['/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/1_rpoc_ez_1_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/2_rpoc_ez_1_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/3_rpoc_ez_1_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/4_rpoc_ez_1_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/5_rpoc_ez_1_seg.tif_spots.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/Scripts/src/SMT_Analysis_BP/databases/trajectory_analysis_script.py:472: UserWarning: loadtxt: input contained no data: \"/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/2_rpoc_ez_1_seg.tif_spots.csv\"\n",
      "  points = np.loadtxt(\"{0}\".format(i),delimiter=\",\",usecols=use_cols,skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/1_rpoc_ez_2_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/2_rpoc_ez_2_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/3_rpoc_ez_2_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/4_rpoc_ez_2_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/5_rpoc_ez_2_seg.tif']\n",
      "['/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/1_rpoc_ez_2_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/2_rpoc_ez_2_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/3_rpoc_ez_2_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/4_rpoc_ez_2_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/5_rpoc_ez_2_seg.tif_spots.csv']\n",
      "['/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/1_rpoc_ez_3_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/2_rpoc_ez_3_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/3_rpoc_ez_3_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/4_rpoc_ez_3_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/5_rpoc_ez_3_seg.tif']\n",
      "['/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/1_rpoc_ez_3_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/2_rpoc_ez_3_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/3_rpoc_ez_3_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/4_rpoc_ez_3_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/5_rpoc_ez_3_seg.tif_spots.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/Scripts/src/SMT_Analysis_BP/databases/trajectory_analysis_script.py:472: UserWarning: loadtxt: input contained no data: \"/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/5_rpoc_ez_3_seg.tif_spots.csv\"\n",
      "  points = np.loadtxt(\"{0}\".format(i),delimiter=\",\",usecols=use_cols,skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Drop data is not in the correct format. Please check the drop data file.\n",
      "Warning: Drop data is not in the correct format. Please check the drop data file.\n",
      "['/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/1_rpoc_ez_4_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/2_rpoc_ez_4_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/3_rpoc_ez_4_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/4_rpoc_ez_4_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/5_rpoc_ez_4_seg.tif']\n",
      "['/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/1_rpoc_ez_4_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/2_rpoc_ez_4_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/3_rpoc_ez_4_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/4_rpoc_ez_4_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/5_rpoc_ez_4_seg.tif_spots.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/Scripts/src/SMT_Analysis_BP/databases/trajectory_analysis_script.py:472: UserWarning: loadtxt: input contained no data: \"/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/3_rpoc_ez_4_seg.tif_spots.csv\"\n",
      "  points = np.loadtxt(\"{0}\".format(i),delimiter=\",\",usecols=use_cols,skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/1_rpoc_ez_5_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/2_rpoc_ez_5_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/3_rpoc_ez_5_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/4_rpoc_ez_5_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/5_rpoc_ez_5_seg.tif']\n",
      "['/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/1_rpoc_ez_5_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/2_rpoc_ez_5_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/3_rpoc_ez_5_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/4_rpoc_ez_5_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/5_rpoc_ez_5_seg.tif_spots.csv']\n",
      "['/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/1_rpoc_ez_6_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/2_rpoc_ez_6_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/3_rpoc_ez_6_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/4_rpoc_ez_6_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/5_rpoc_ez_6_seg.tif']\n",
      "['/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/1_rpoc_ez_6_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/2_rpoc_ez_6_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/3_rpoc_ez_6_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/4_rpoc_ez_6_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/5_rpoc_ez_6_seg.tif_spots.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/Scripts/src/SMT_Analysis_BP/databases/trajectory_analysis_script.py:472: UserWarning: loadtxt: input contained no data: \"/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/3_rpoc_ez_6_seg.tif_spots.csv\"\n",
      "  points = np.loadtxt(\"{0}\".format(i),delimiter=\",\",usecols=use_cols,skiprows=skiprows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Drop data is not in the correct format. Please check the drop data file.\n",
      "['/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/1_rpoc_ez_7_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/2_rpoc_ez_7_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/3_rpoc_ez_7_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/4_rpoc_ez_7_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/5_rpoc_ez_7_seg.tif']\n",
      "['/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/1_rpoc_ez_7_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/2_rpoc_ez_7_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/3_rpoc_ez_7_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/4_rpoc_ez_7_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/5_rpoc_ez_7_seg.tif_spots.csv']\n",
      "Warning: Drop data is not in the correct format. Please check the drop data file.\n",
      "Warning: Drop data is not in the correct format. Please check the drop data file.\n",
      "['/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/1_rpoc_ez_8_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/2_rpoc_ez_8_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/3_rpoc_ez_8_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/4_rpoc_ez_8_seg.tif', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/5_rpoc_ez_8_seg.tif']\n",
      "['/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/1_rpoc_ez_8_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/2_rpoc_ez_8_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/3_rpoc_ez_8_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/4_rpoc_ez_8_seg.tif_spots.csv', '/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez/segmented_scale_space_plus/Analysis_DBSCAN/5_rpoc_ez_8_seg.tif_spots.csv']\n"
     ]
    }
   ],
   "source": [
    "rp_ez_scale= run_analysis(wd=\"/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/rpoc_ez\",\n",
    "                    t_string=\"rpoc_ez\")\n",
    "                    \n",
    "rp_ez_scale.read_parameters(minimum_percent_per_drop_in = .5, \n",
    "                    t_len_u = 25, \n",
    "                    t_len_l=5, \n",
    "                    minimum_tracks_per_drop = 3)\n",
    "\n",
    "rp_ez_scale.get_blob_parameters(threshold=1e-4,\n",
    "                        overlap=0,\n",
    "                        detection_name='bp',\n",
    "                        min_sigma=1/np.sqrt(2),\n",
    "                        max_sigma=3/np.sqrt(2),\n",
    "                        num_sigma=100,\n",
    "                        median = False)\n",
    "\n",
    "rp_ez_scale.get_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                    \"plot_fit\":False,\n",
    "                                    \"fitting_image\":\"Original\",\n",
    "                                    \"radius_func\":np.mean,#identity,\n",
    "                                    \"residual_func\":residuals_gaus2d,\n",
    "                                    \"sigma_range\":0.2,\n",
    "                                    \"centroid_range\":0.2,\n",
    "                                    \"height_range\":2})\n",
    "rp_ez_scale.type_of_blob = \"DBSCAN\"#\"SCALE_SPACE_PLUS\"\n",
    "rp_ez_scale.a_file_style = \"new\"\n",
    "rp_ez_scale.run_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_durations = rp_ez_scale.track_durations\n",
    "print(temp_durations)\n",
    "#lets plot the histogram for the IN track durations\n",
    "plt.clf()\n",
    "plt.hist(temp_durations[\"IO\"],bins=20,density=True,label=\"IO\",alpha=0.5)\n",
    "plt.hist(temp_durations[\"IN\"],bins=20,density=True,label=\"IN\",alpha=0.5)\n",
    "plt.hist(temp_durations[\"OUT\"],bins=20,density=True,label=\"OO\",alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Track Duration (frames)\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Histogram of Track Durations for IN Tracks\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "print(len(temp_durations[\"IO\"]),len(temp_durations[\"IN\"]),len(temp_durations[\"OUT\"]))\n",
    "\n",
    "all_track_lengths = temp_durations[\"IN\"]\n",
    "all_track_diff = []\n",
    "for i in rp_ez_scale.track_collection[\"IN\"]:\n",
    "    all_track_diff.append(i.MSD_total_um)\n",
    "\n",
    "plt.scatter(all_track_lengths,all_track_diff)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_ez_scale._make_SMAUG_files(dir_name = \"SMAUG_new\")\n",
    "rp_ez_scale._make_NOBIAS_files(dir_name = \"NOBIAS_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load a .mat file \n",
    "from scipy.io import loadmat\n",
    "mat_test = loadmat('/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/ll_ez/Analysis/laco_laci_ezMATLAB_dat/ALL/LL_ez_ALL.mat')\n",
    "print(mat_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NusA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ez_scale= run_analysis(wd=\"/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/NUSA\",\n",
    "                    t_string=\"NUSA\")\n",
    "                    \n",
    "n_ez_scale.read_parameters(minimum_percent_per_drop_in = .5, \n",
    "                    t_len_u = 25, \n",
    "                    t_len_l=7, \n",
    "                    minimum_tracks_per_drop = 3)\n",
    "\n",
    "n_ez_scale.get_blob_parameters(threshold=1e-4,\n",
    "                        overlap=0,\n",
    "                        detection_name='bp',\n",
    "                        min_sigma=1/np.sqrt(2),\n",
    "                        max_sigma=3/np.sqrt(2),\n",
    "                        num_sigma=100,\n",
    "                        median = False)\n",
    "\n",
    "n_ez_scale.get_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                    \"plot_fit\":False,\n",
    "                                    \"fitting_image\":\"Original\",\n",
    "                                    \"radius_func\":np.mean,#identity,\n",
    "                                    \"residual_func\":residuals_gaus2d,\n",
    "                                    \"sigma_range\":0.2,\n",
    "                                    \"centroid_range\":0.2,\n",
    "                                    \"height_range\":2})\n",
    "n_ez_scale.type_of_blob = \"TRACKMATE\"\n",
    "n_ez_scale.run_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ez_hex5_scale= run_analysis(wd=\"/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/15/nusa_ez_hex5_3\",\n",
    "                    t_string=\"nusa_ez_hex5\")\n",
    "                    \n",
    "n_ez_hex5_scale.read_parameters(minimum_percent_per_drop_in = .5, \n",
    "                    t_len_u = 25, \n",
    "                    t_len_l=7, \n",
    "                    minimum_tracks_per_drop = 3)\n",
    "\n",
    "n_ez_hex5_scale.get_blob_parameters(threshold=1e-4,\n",
    "                        overlap=0,\n",
    "                        detection_name='bp',\n",
    "                        min_sigma=1/np.sqrt(2),\n",
    "                        max_sigma=3/np.sqrt(2),\n",
    "                        num_sigma=100,\n",
    "                        median = False)\n",
    "\n",
    "n_ez_hex5_scale.get_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                    \"plot_fit\":False,\n",
    "                                    \"fitting_image\":\"Original\",\n",
    "                                    \"radius_func\":np.mean,#identity,\n",
    "                                    \"residual_func\":residuals_gaus2d,\n",
    "                                    \"sigma_range\":0.2,\n",
    "                                    \"centroid_range\":0.2,\n",
    "                                    \"height_range\":2})\n",
    "n_ez_hex5_scale.type_of_blob = \"TRACKMATE\"\n",
    "n_ez_hex5_scale.run_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_m9= run_analysis(wd=\"/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/20191216/nusa_m9\",\n",
    "                    t_string=\"nusa_m9\")\n",
    "                    \n",
    "n_m9.read_parameters(minimum_percent_per_drop_in = .5, \n",
    "                    t_len_u = 25, \n",
    "                    t_len_l=7, \n",
    "                    minimum_tracks_per_drop = 3)\n",
    "\n",
    "n_m9.get_blob_parameters(threshold=1e-4,\n",
    "                        overlap=0,\n",
    "                        detection_name='bp',\n",
    "                        min_sigma=1/np.sqrt(2),\n",
    "                        max_sigma=3/np.sqrt(2),\n",
    "                        num_sigma=100,\n",
    "                        median = False)\n",
    "\n",
    "n_m9.get_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                    \"plot_fit\":False,\n",
    "                                    \"fitting_image\":\"Original\",\n",
    "                                    \"radius_func\":np.mean,#identity,\n",
    "                                    \"residual_func\":residuals_gaus2d,\n",
    "                                    \"sigma_range\":0.2,\n",
    "                                    \"centroid_range\":0.2,\n",
    "                                    \"height_range\":2})\n",
    "n_m9.type_of_blob = \"TRACKMATE\"\n",
    "n_m9.run_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_data = rp_ez_scale\n",
    "from sklearn.mixture import GaussianMixture\n",
    "# #make a df for the msds\n",
    "msd_df_rp_ez = pd.DataFrame(columns=[\"MSD\"])\n",
    "msd_IO_df_rp_ez = pd.DataFrame(columns=[\"MSD\"])\n",
    "msd_IN_df_rp_ez = pd.DataFrame(columns=[\"MSD\"])\n",
    "msd_OUT_df_rp_ez = pd.DataFrame(columns=[\"MSD\"])\n",
    "msd_alls = []\n",
    "for l,k in which_data.Movie.items():\n",
    "    for m,n in k.Cells.items():\n",
    "        for i,j in which_data.Movie[l].Cells[m].All_Tracjectories.items():\n",
    "            msd_alls.append(j.MSD_total_um)\n",
    "            if j.Classification == \"IN\":\n",
    "            #if (j.Classification == \"OUT\") | (j.Classification == \"IN\") | (j.Classification == \"IO\") | (j.Classification == None):\n",
    "                #make a unique identifier for the dictionary consiting on l,m,i\n",
    "                identifier = str(l)+str(m)+str(i)\n",
    "                msd_df_rp_ez.loc[identifier] = j.MSD_total_um\n",
    "                #add a new column for the drop identifier which will be a string.\n",
    "                msd_df_rp_ez.loc[identifier,\"Drop\"] = j.Drop_Identifier\n",
    "            if j.Classification == \"IO\":\n",
    "                #make a unique identifier for the dictionary consiting on l,m,i\n",
    "                identifier = str(l)+str(m)+str(i)\n",
    "                msd_IO_df_rp_ez.loc[identifier] = j.MSD_total_um\n",
    "                #add a new column for the drop identifier which will be a string.\n",
    "                msd_IO_df_rp_ez.loc[identifier,\"Drop\"] = j.Drop_Identifier\n",
    "            if j.Classification == \"IN\":\n",
    "                #make a unique identifier for the dictionary consiting on l,m,i\n",
    "                identifier = str(l)+str(m)+str(i)\n",
    "                msd_IN_df_rp_ez.loc[identifier] = j.MSD_total_um\n",
    "                #add a new column for the drop identifier which will be a string.\n",
    "                msd_IN_df_rp_ez.loc[identifier,\"Drop\"] = j.Drop_Identifier\n",
    "            if j.Classification == \"OUT\":\n",
    "                #make a unique identifier for the dictionary consiting on l,m,i\n",
    "                identifier = str(l)+str(m)+str(i)\n",
    "                msd_OUT_df_rp_ez.loc[identifier] = j.MSD_total_um\n",
    "                #add a new column for the drop identifier which will be a string.\n",
    "                msd_OUT_df_rp_ez.loc[identifier,\"Drop\"] = j.Drop_Identifier\n",
    "            if j.Classification == None:\n",
    "                #make a unique identifier for the dictionary consiting on l,m,i\n",
    "                identifier = str(l)+str(m)+str(i)\n",
    "                msd_OUT_df_rp_ez.loc[identifier] = j.MSD_total_um\n",
    "                #add a new column for the drop identifier which will be a string.\n",
    "                msd_OUT_df_rp_ez.loc[identifier,\"Drop\"] = j.Drop_Identifier\n",
    "\n",
    "#find the combination of msd_IO_df_rp_ez, msd_IN_df_rp_ez, msd_OUT_df_rp_ez\n",
    "msd_df_rp_ez = pd.concat([msd_IO_df_rp_ez,msd_IN_df_rp_ez,msd_OUT_df_rp_ez])\n",
    "#plot the histogram of the msd after logging it\n",
    "#fit a 2 gaussian mixture model to the data\n",
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=2)\n",
    "gmm.fit(np.log10(np.array(msd_df_rp_ez[\"MSD\"].values)).reshape(-1,1))\n",
    "#gmm.fit(msd_df_rp_ez[\"MSD\"].values.reshape(-1,1))\n",
    "plt.clf()\n",
    "plt.hist(np.log10(msd_df_rp_ez[\"MSD\"]),bins=10,density=True,alpha=0.5)\n",
    "\n",
    "#print the means of the two gaussians\n",
    "print(gmm.means_)\n",
    "#print the probabilities of the two gaussians\n",
    "print(gmm.weights_)\n",
    "plt.xlabel(\"Log10(MSD)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of MSD for rpoc_ez\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(len(msd_df_rp_ez))\n",
    "#plot the histogram of the msd after logging it\n",
    "#fit a 2 gaussian mixture model to the data\n",
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=2)\n",
    "gmm.fit(np.log10(msd_df_rp_ez[\"MSD\"]).values.reshape(-1,1))\n",
    "plt.clf()\n",
    "plt.hist(np.log10(msd_IN_df_rp_ez[\"MSD\"]),bins=12,density=True,alpha=0.5)\n",
    "plt.hist(np.log10(msd_IO_df_rp_ez[\"MSD\"]),bins=12,density=True,alpha=0.5)\n",
    "plt.hist(np.log10(msd_OUT_df_rp_ez[\"MSD\"]),bins=12,density=True,alpha=0.5)\n",
    "#print the means of the two gaussians\n",
    "print(gmm.means_)\n",
    "plt.xlabel(\"Log10(MSD)\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"MSD of RPoc EZ\")\n",
    "plt.show()\n",
    "print(msd_df_rp_ez[\"Drop\"])\n",
    "#find the number of i that are similar in both\n",
    "\n",
    "# #print the mean of the msd\n",
    "# # print(np.mean(msd_df_rp_ez[\"MSD\"]))\n",
    "track_dict = which_data._convert_to_track_dict_bulk()\n",
    "# msds_calc_in = msd_calc(track_dict[\"IN\"],h=None,tau_lim=2,tick_space=10,msd_fit_lim=[0,5],plot=True)\n",
    "# msdss_in = con_pix_si(np.array(list(msds_calc_in[\"track_diffusion\"].values())),which = \"msd\")\n",
    "# msds_calc_io = msd_calc(track_dict[\"IO\"],h=None,tau_lim=2,tick_space=10,msd_fit_lim=[0,5],plot=True)\n",
    "# msdss_io = con_pix_si(np.array(list(msds_calc_io[\"track_diffusion\"].values())),which = \"msd\")\n",
    "# msds_calc_ot = msd_calc(track_dict[\"OUT\"],h=None,tau_lim=2,tick_space=10,msd_fit_lim=[0,5],plot=True)\n",
    "# msdss_ot = con_pix_si(np.array(list(msds_calc_ot[\"track_diffusion\"].values())),which = \"msd\")\n",
    "msds_calc_ALL = msd_calc(track_dict[\"IN\"],h=None,tau_lim=2,tick_space=10,msd_fit_lim=[0,5],plot=True,convert=0.13)\n",
    "plt.hist(con_pix_si(np.array(list(msds_calc_ALL[\"track_diffusion\"].values())),which='msd'))\n",
    "plt.show()\n",
    "# plt.hist(np.log10(msdss_in),bins=20,alpha=0.5)\n",
    "# plt.hist(np.log10(msdss_io),bins=20,alpha=0.5)\n",
    "# plt.hist(np.log10(msdss_ot),bins=20,alpha=0.5)\n",
    "# plt.xlabel(\"Log10(MSD)\")\n",
    "# plt.ylabel(\"Counts\")\n",
    "# plt.title(\"MSD of RPoc EZ\")\n",
    "# plt.show()\n",
    "# loc_err = np.sqrt(np.array(list(msds_calc_ALL[\"loc_err\"].values()))/4.)\n",
    "# plt.hist(loc_err,bins=20,alpha=0.5)\n",
    "# plt.show()\n",
    "# #remove the nan values from loc_err\n",
    "# loc_err = loc_err[~np.isnan(loc_err)]\n",
    "# #fit an exponential decay to the data\n",
    "# from scipy.optimize import curve_fit\n",
    "# def exp_decay(x,a,b):\n",
    "#     return a*np.exp(-b*x)\n",
    "# popt,pcov = curve_fit(exp_decay,np.arange(0,len(loc_err)),loc_err)\n",
    "# print(popt)\n",
    "\n",
    "# #out_all = msd_calc(track_dict[\"IN\"],h=None,tau_lim=None,tick_space=10,msd_fit_lim=[0,4],plot=False)\n",
    "# #fit a 3 gaussian mixture model to the data\n",
    "# from sklearn.mixture import GaussianMixture\n",
    "# gmm = GaussianMixture(n_components=3)\n",
    "# gmm.fit((np.array(list(out_all[\"track_diffusion\"].values()))).reshape(-1,1))\n",
    "# #print the means and the weights of the gaussians\n",
    "# print(gmm.means_)\n",
    "# print(gmm.weights_)\n",
    "# #plot the histogram of the msd\n",
    "# plt.clf()\n",
    "# plt.hist(np.array(list(out_all[\"track_diffusion\"].values())),bins=25,density=True,alpha=0.5)\n",
    "# plt.xlabel(\"MSD\")\n",
    "# plt.ylabel(\"Counts\")\n",
    "# plt.title(\"MSD of RPoc EZ\")\n",
    "# plt.show()\n",
    "# track_dict_rp_ez = rp_ez_scale._convert_to_track_dict_bulk()\n",
    "# out_all_ez = msd_calc(track_dict_rp_ez[\"IN\"],h=None,tau_lim=None,tick_space=10,msd_fit_lim=[0,4],plot=False)\n",
    "# from sklearn.mixture import GaussianMixture\n",
    "# gmm = GaussianMixture(n_components=3)\n",
    "# gmm.fit((np.array(list(out_all_ez[\"track_diffusion\"].values()))).reshape(-1,1))\n",
    "# #print the means and the weights of the gaussians\n",
    "# print(gmm.means_)\n",
    "# print(gmm.weights_)\n",
    "# #plot the histogram of the msd\n",
    "# plt.clf()\n",
    "# plt.hist(np.array(list(out_all_ez[\"track_diffusion\"].values())),bins=25,density=True,alpha=0.5)\n",
    "# plt.hist(np.array(list(out_all[\"track_diffusion\"].values())),bins=25,density=True,alpha=0.5)\n",
    "# plt.xlabel(\"MSD\")\n",
    "# plt.ylabel(\"Counts\")\n",
    "# plt.title(\"MSD of RPoc EZ\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.clf()\n",
    "# plt.hist(np.array(list(out_all_ez[\"track_diffusion\"].values())),bins=50,density=True,alpha=0.5)\n",
    "# plt.hist(np.array(list(out_all[\"track_diffusion\"].values())),bins=50,density=True,alpha=0.5)\n",
    "# #fit both to a 3 gaussian mixture model\n",
    "# gmm = GaussianMixture(n_components=2)\n",
    "# gmm.fit((np.array(list(out_all_ez[\"track_diffusion\"].values()))).reshape(-1,1))\n",
    "# #plot the fit along with the histogram\n",
    "# x = np.linspace(0,4,1000)\n",
    "# plt.plot(x,np.exp(gmm.score_samples(x.reshape(-1,1))))\n",
    "# #print the means and the weights of the gaussians\n",
    "# print(gmm.means_)\n",
    "# print(gmm.weights_)\n",
    "# #print the goodness of fit\n",
    "# print(gmm.score(np.array(list(out_all_ez[\"track_diffusion\"].values())).reshape(-1,1)))\n",
    "\n",
    "# #do the same for the other\n",
    "# gmm = GaussianMixture(n_components=2)\n",
    "# gmm.fit((np.array(list(out_all[\"track_diffusion\"].values()))).reshape(-1,1))\n",
    "# x = np.linspace(0,4,1000)\n",
    "# plt.plot(x,np.exp(gmm.score_samples(x.reshape(-1,1))))\n",
    "# #print the means and the weights of the gaussians\n",
    "# print(gmm.means_)\n",
    "# print(gmm.weights_)\n",
    "# #print the goodness of fit\n",
    "# print(gmm.score(np.array(list(out_all[\"track_diffusion\"].values())).reshape(-1,1)))\n",
    "# plt.xlabel(\"MSD\")\n",
    "# plt.ylabel(\"Counts\")\n",
    "# plt.title(\"MSD of RPoc EZ\")\n",
    "# #set x lim from 0 to 3\n",
    "# plt.xlim(0,3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_data_list = {rp_ez_scale:{\"IN\":0,\"IO\":0,\"OUT\":0,\"ALL\":0}}#,\n",
    "                   #rp_m9:{\"IN\":0,\"IO\":0,\"OUT\":0,\"ALL\":0},\n",
    "                   #ll_ez:{\"IN\":0,\"IO\":0,\"OUT\":0,\"ALL\":0},\n",
    "                   #rp_hex5:{\"IN\":0,\"IO\":0,\"OUT\":0,\"ALL\":0}}\n",
    "                   \n",
    "\n",
    "for i,j in which_data_list.items():\n",
    "    track_dict = i._convert_to_track_dict_bulk()\n",
    "    j[\"IO\"] = msd_calc(track_dict[\"IO\"],h=None,tau_lim=1,tick_space=10,msd_fit_lim=[0,5],plot=False)\n",
    "    j[\"IN\"] = msd_calc(track_dict[\"IN\"],h=None,tau_lim=1,tick_space=10,msd_fit_lim=[0,5],plot=False)\n",
    "    j[\"OUT\"] = msd_calc(track_dict[\"OUT\"],h=None,tau_lim=1,tick_space=10,msd_fit_lim=[0,5],plot=False)\n",
    "    j[\"ALL\"] = msd_calc(track_dict[\"ALL\"],h=None,tau_lim=1,tick_space=10,msd_fit_lim=[0,5],plot=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "for i,k in which_data_list.items():\n",
    "    j=k[\"IN\"]\n",
    "    #(list(j[\"msd_curve_ens\"].keys())[:10],list(j[\"msd_curve_ens\"].values())[:10],label=i)\n",
    "    ax.errorbar(np.array(list(j[\"msd_curve_ens\"].keys())[:10])*0.02,list(j[\"msd_curve_ens\"].values())[:10],yerr=list(j[\"msd_curve_ens_err\"].values())[:10],fmt='o',capsize=5,label=i.wd)\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.set_ylabel(\"MSD \")\n",
    "#place the legend outside the plot\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i,k in which_data_list.items():\n",
    "    fig,ax = plt.subplots(figsize=(10,10))\n",
    "    for k,j in k.items():\n",
    "        #(list(j[\"msd_curve_ens\"].keys())[:10],list(j[\"msd_curve_ens\"].values())[:10],label=i)\n",
    "        ax.errorbar(np.array(list(j[\"msd_curve_ens\"].keys())[:20])*0.02,list(j[\"msd_curve_ens\"].values())[:20],yerr=list(j[\"msd_curve_ens_err\"].values())[:20],fmt='o',capsize=5,label=str(i.wd)+\"_\"+str(k))\n",
    "        plt.legend()\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_ez_scale_dif = which_data_list[rp_ez_scale][\"IN\"][\"track_diffusion\"]\n",
    "plt.hist(np.array(list(rp_ez_scale_dif.values())),bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log10(np.array(list(msds_calc_in[\"tavg_t1_msd\"].values()))/0.02))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_dict = rp_ez._convert_to_track_dict_bulk()[\"ALL\"]\n",
    "tau = 1\n",
    "msd_dict_rp_ez_scale,ens_displacements_rp_ez_scale = MSD_Tracks(track_dict,permutation=True,return_type=\"both\",verbose=True)\n",
    "ens_displacements_rp_ez_scale_r = []\n",
    "\n",
    "for i in ens_displacements_rp_ez_scale[tau]:\n",
    "    ens_displacements_rp_ez_scale_r.append(np.sqrt(i[0]**2+i[1]**2))\n",
    "\n",
    "rpez_all = msd_calc(track_dict,h=None,tau_lim=None,tick_space=10,msd_fit_lim=[0,4],plot=False)\n",
    "msd_df_rp_ez = pd.DataFrame(columns=[\"MSD\"])\n",
    "msd_IO_df_rp_ez = pd.DataFrame(columns=[\"MSD\"])\n",
    "msd_IN_df_rp_ez = pd.DataFrame(columns=[\"MSD\"])\n",
    "msd_OUT_df_rp_ez = pd.DataFrame(columns=[\"MSD\"])\n",
    "msd_all = []\n",
    "msd_in = []\n",
    "for l,k in rp_ez.Movie.items():\n",
    "    for m,n in k.Cells.items():\n",
    "        for i,j in rp_ez.Movie[l].Cells[m].All_Tracjectories.items():\n",
    "            msd_all.append(j.MSD_total_um)\n",
    "            if j.Classification == \"IN\":\n",
    "            #if (j.Classification == \"OUT\") | (j.Classification == \"IN\") | (j.Classification == \"IO\") | (j.Classification == None):\n",
    "                #make a unique identifier for the dictionary consiting on l,m,i\n",
    "                identifier = str(l)+str(m)+str(i)\n",
    "                msd_df_rp_ez.loc[identifier] = j.MSD_total_um\n",
    "                #add a new column for the drop identifier which will be a string.\n",
    "                msd_df_rp_ez.loc[identifier,\"Drop\"] = j.Drop_Identifier\n",
    "                msd_in.append(j.MSD_total_um)\n",
    "            if j.Classification == \"IO\":\n",
    "                #make a unique identifier for the dictionary consiting on l,m,i\n",
    "                identifier = str(l)+str(m)+str(i)\n",
    "                msd_IO_df_rp_ez.loc[identifier] = j.MSD_total_um\n",
    "                #add a new column for the drop identifier which will be a string.\n",
    "                msd_IO_df_rp_ez.loc[identifier,\"Drop\"] = j.Drop_Identifier\n",
    "            if j.Classification == \"IN\":\n",
    "                #make a unique identifier for the dictionary consiting on l,m,i\n",
    "                identifier = str(l)+str(m)+str(i)\n",
    "                msd_IN_df_rp_ez.loc[identifier] = j.MSD_total_um\n",
    "                #add a new column for the drop identifier which will be a string.\n",
    "                msd_IN_df_rp_ez.loc[identifier,\"Drop\"] = j.Drop_Identifier\n",
    "            if j.Classification == \"OUT\":\n",
    "                #make a unique identifier for the dictionary consiting on l,m,i\n",
    "                identifier = str(l)+str(m)+str(i)\n",
    "                msd_OUT_df_rp_ez.loc[identifier] = j.MSD_total_um\n",
    "                #add a new column for the drop identifier which will be a string.\n",
    "                msd_OUT_df_rp_ez.loc[identifier,\"Drop\"] = j.Drop_Identifier\n",
    "\n",
    "plt.hist(np.array(msd_all),bins=1000,alpha =0.5)\n",
    "plt.hist(np.array(list(rpez_all[\"track_diffusion\"].values())),bins=1000,alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.hist(np.array(flatten(ens_displacements_rp_ez_scale[tau]))*130/1000,alpha =0.5,bins=100,density=True)\n",
    "plt.hist(np.array(flatten(ens_displacements_rp_ez_scale[tau+1]))*130/1000,alpha =0.5,bins=100,density=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.hist(np.array(ens_displacements_rp_ez_scale_r)*130/1000,alpha =0.5,bins=100,density=True)\n",
    "track_dict_in = rp_ez._convert_to_track_dict_bulk()[\"IN\"]\n",
    "msd_dict_rp_ez_scale_in,ens_displacements_rp_ez_scale_in = MSD_Tracks(track_dict_in,permutation=True,return_type=\"both\",verbose=True)\n",
    "ens_displacements_rp_ez_scale_r_in = []\n",
    "\n",
    "for i in ens_displacements_rp_ez_scale_in[tau]:\n",
    "    ens_displacements_rp_ez_scale_r_in.append(np.sqrt(i[0]**2+i[1]**2))\n",
    "#print the smallest displacements\n",
    "print(np.mean(np.abs(np.array(flatten(ens_displacements_rp_ez_scale_r_in[tau]))*130/1000)))\n",
    "plt.hist(np.array(ens_displacements_rp_ez_scale_r_in)*130/1000,alpha =0.5,bins=100,density=True)\n",
    "plt.show()\n",
    "\n",
    "#plot the cdf of the displacements but dont fill it in\n",
    "plt.clf()\n",
    "plt.hist(np.array(ens_displacements_rp_ez_scale_r)*130/1000,alpha =0.5,bins=100,density=True,cumulative=True,histtype=\"step\")\n",
    "plt.hist(np.array(ens_displacements_rp_ez_scale_r_in)*130/1000,alpha =0.5,bins=100,density=True,cumulative=True,histtype=\"step\")\n",
    "plt.xlabel(\"Displacement (um)\")\n",
    "plt.ylabel(\"Cumulative Probability\")\n",
    "plt.title(\"RPoc EZ\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.hist(np.log10(msd_in),bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the relationship between cell areas, density, nucleoid area, drop areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_areas = []\n",
    "nuc_areas = []\n",
    "drop_per_cell_area = []\n",
    "point_density = []\n",
    "for i,j in rp_ez_scale.Movie.items():\n",
    "    for k,l in j.Cells.items():\n",
    "        cell_areas.append(l.cell_area[0]*0.0169)\n",
    "        nuc_areas.append(l.nucleoid_area*0.0169)\n",
    "        points_in_cell = 0.0\n",
    "        for m,n in l.points_per_frame.items():\n",
    "            points_in_cell+=len(n)\n",
    "        point_density.append(float(points_in_cell)/(l.cell_area[0]*0.0169))\n",
    "\n",
    "        drops_cell = []\n",
    "        for m,n in l.Drop_Collection.items():\n",
    "            drops_cell.append((np.pi*n[-1]**2)*0.0169)\n",
    "        drop_per_cell_area.append(np.array(drops_cell))\n",
    "        \n",
    "\n",
    "NC_ratio = np.array(nuc_areas)/np.array(cell_areas)\n",
    "def line_func(x,m,c):\n",
    "    return m*x + c\n",
    "#fit a line to NC_ratio = m*cell_area_sp + c only\n",
    "line_NC = curve_fit(line_func,cell_areas,NC_ratio,bounds = (0,[1e-3,np.inf]))\n",
    "line_cell_nuc = curve_fit(line_func,cell_areas,nuc_areas,bounds = (0,[np.inf,1e-3]))\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(np.array(cell_areas),np.array(nuc_areas))\n",
    "#plot the fit line using lower and upper bound of cell area\n",
    "ax.plot(np.linspace(min(cell_areas),max(cell_areas),100),line_func(np.linspace(min(cell_areas),max(cell_areas),100),*line_cell_nuc[0]),color=\"red\")\n",
    "#annotate the slope and intercept as y=mx+c\n",
    "ax.annotate(\"y = \"+str(round(line_cell_nuc[0][0],3))+\"x + \"+str(round(line_cell_nuc[0][1],3)),(0.5,0.4),xycoords=\"axes fraction\")\n",
    "ax.set_xlabel(\"Cell Area SuperSegger(um^2)\")\n",
    "ax.set_ylabel(\"Nucleoid Area (um^2)\")\n",
    "ax.set_title(\"Nucleoid Area vs Cell Area\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(np.array(cell_areas),NC_ratio)\n",
    "#plot the fit line using lower and upper bound of cell area\n",
    "ax.plot(np.linspace(min(cell_areas),max(cell_areas),100),line_func(np.linspace(min(cell_areas),max(cell_areas),100),*line_NC[0]),color=\"red\")\n",
    "#annotate the slope and intercept as y=mx+c\n",
    "ax.annotate(\"y = \"+str(round(line_NC[0][0],3))+\"x + \"+str(round(line_NC[0][1],3)),(0.5,0.5),xycoords=\"axes fraction\")\n",
    "ax.set_xlabel(\"Cell Area SuperSegger(um^2)\")\n",
    "ax.set_ylabel(\"Nucleoid Area/Cell Area\")\n",
    "ax.set_title(\"Nucleoid Area/Cell Area vs Cell Area\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "for i in range(len(cell_areas)):\n",
    "    ax.scatter([cell_areas[i]]*len(drop_per_cell_area[i]),drop_per_cell_area[i],color=\"blue\")\n",
    "ax.set_xlabel(\"Cell Area SuperSegger(um^2)\")\n",
    "ax.set_ylabel(\"Drop Area (um^2)\")\n",
    "ax.set_title(\"Drop Area vs Cell Area\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "for i in range(len(cell_areas)):\n",
    "    ax.scatter(cell_areas[i],np.mean(np.array(drop_per_cell_area[i])),color=\"blue\")\n",
    "ax.set_xlabel(\"Cell Area SuperSegger(um^2)\")\n",
    "ax.set_ylabel(\"Average Drop Area (um^2)\")\n",
    "ax.set_title(\"Average Drop Area vs Cell Area\")\n",
    "plt.show()\n",
    "\n",
    "mean_drop_per_cell_area = []\n",
    "for i in range(len(drop_per_cell_area)):\n",
    "    mean_drop_per_cell_area.append(np.mean(np.array(drop_per_cell_area[i])))\n",
    "DC_ratio = np.array(mean_drop_per_cell_area)/np.array(cell_areas)\n",
    "DN_ratio = np.array(mean_drop_per_cell_area)/np.array(nuc_areas)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(np.array(cell_areas),DC_ratio)\n",
    "ax.set_xlabel(\"Cell Area SuperSegger(um^2)\")\n",
    "ax.set_ylabel(\"DC Ratio\")\n",
    "ax.set_title(\"DC Ratio (Drop Area/Cell Area) \\nvs Cell Area\")\n",
    "ax.set_ylim(0,0.6)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#do the same but now using nuc_area\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "for i in range(len(nuc_areas)):\n",
    "    ax.scatter([nuc_areas[i]]*len(drop_per_cell_area[i]),drop_per_cell_area[i],color=\"blue\")\n",
    "ax.set_xlabel(\"Nucleoid Area (um^2)\")\n",
    "ax.set_ylabel(\"Drop Area (um^2)\")\n",
    "ax.set_title(\"Drop Area vs Nucleoid Area\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "for i in range(len(nuc_areas)):\n",
    "    ax.scatter(nuc_areas[i],np.mean(np.array(drop_per_cell_area[i])),color=\"blue\")\n",
    "ax.set_xlabel(\"Nucleoid Area (um^2)\")\n",
    "ax.set_ylabel(\"Average Drop Area (um^2)\")\n",
    "ax.set_title(\"Average Drop Area vs Nucleoid Area\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(np.array(nuc_areas),DN_ratio)\n",
    "ax.set_xlabel(\"Nucleoid Area (um^2)\")\n",
    "ax.set_ylabel(\"DN Ratio\")\n",
    "ax.set_title(\"DN Ratio (Drop Area/Nucleoid Area) \\nvs Nucleoid Area\")\n",
    "ax.set_ylim(0,0.6)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(np.array(cell_areas),np.array(point_density))\n",
    "ax.set_xlabel(\"Cell Area SuperSegger(um^2)\")\n",
    "ax.set_ylabel(\"Point Density\")\n",
    "ax.set_title(\"Point Density vs Cell Area\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "num_drops = []\n",
    "for i in range(len(drop_per_cell_area)):\n",
    "    num_drops.append(len(drop_per_cell_area[i]))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(np.array(cell_areas),np.array(num_drops))\n",
    "ax.set_xlabel(\"Cell Area SuperSegger(um^2)\")\n",
    "ax.set_ylabel(\"Number of Drops\")\n",
    "ax.set_title(\"Number of Drops vs Cell Area\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(np.array(nuc_areas),np.array(num_drops))\n",
    "ax.set_xlabel(\"Nucleoid Area (um^2)\")\n",
    "ax.set_ylabel(\"Number of Drops\")\n",
    "ax.set_title(\"Number of Drops vs Nucleoid Area\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(np.array(NC_ratio),np.array(DN_ratio))\n",
    "ax.set_xlabel(\"NC Ratio\")\n",
    "ax.set_ylabel(\"DN Ratio\")\n",
    "ax.set_title(\"DN Ratio vs NC Ratio\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(np.array(NC_ratio),np.array(DC_ratio))\n",
    "ax.set_xlabel(\"NC Ratio\")\n",
    "ax.set_ylabel(\"DC Ratio\")\n",
    "ax.set_title(\"DC Ratio vs NC Ratio\")\n",
    "plt.show()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert tracks to dictionary\n",
    "track_dict = rp_ez._convert_to_track_dict_bulk()\n",
    "h=0.5\n",
    "a = msd_calc(track_dict[\"IN\"],h=None,tau_lim=None,tick_space=2,msd_fit_lim=5)\n",
    "\n",
    "#do a GMM fit for the track alphas \n",
    "gmm = mixture.GaussianMixture(n_components=2, covariance_type='full')\n",
    "alphas = np.array(list(a[\"track_alpha\"].values()))\n",
    "alphas = alphas.reshape(-1,1)\n",
    "gmm.fit(alphas)\n",
    "labels = gmm.predict(alphas)\n",
    "#plot the gaussian mixture model\n",
    "plt.clf()\n",
    "plt.hist(alphas, 15, density=True, alpha=0.6, color='g')\n",
    "x = np.linspace(-1., 1.5, 1000)\n",
    "logprob = gmm.score_samples(x.reshape(-1, 1))\n",
    "responsibilities = gmm.predict_proba(x.reshape(-1, 1))\n",
    "pdf = np.exp(logprob)\n",
    "pdf_individual = responsibilities * pdf[:, np.newaxis]\n",
    "plt.plot(x, pdf, '-k')\n",
    "plt.plot(x, pdf_individual, '--k')\n",
    "#annotate the two gaussians with their means only without an arrow\n",
    "plt.annotate('Gaussian 1: '+str(round(gmm.means_[0][0],2)), xy=(gmm.means_[0], 0.1), xytext=(gmm.means_[0], 0.1))\n",
    "plt.annotate('Gaussian 2: '+str(round(gmm.means_[1][0],2)), xy=(gmm.means_[1], 0.1), xytext=(gmm.means_[1], 0.2))\n",
    "\n",
    "plt.xlabel(\"Track Alpha\")\n",
    "plt.ylabel(\"Probability desity\")\n",
    "plt.show()\n",
    "\n",
    "print(gmm.means_)\n",
    "print(gmm.covariances_)\n",
    "\n",
    "\n",
    "diff_tracks_poly = con_pix_si(np.array(list(a[\"tavg_t1_msd\"].values())),which = 'msd')\n",
    "#fit a GMM to this distribution\n",
    "gmm = mixture.GaussianMixture(n_components=2, covariance_type='full')\n",
    "diff_tracks_poly = diff_tracks_poly.reshape(-1,1)\n",
    "gmm.fit(diff_tracks_poly)\n",
    "labels = gmm.predict(diff_tracks_poly)\n",
    "#plot the gaussian mixture model\n",
    "plt.clf()\n",
    "plt.hist(diff_tracks_poly, 10, density=True, alpha=0.6, color='g')\n",
    "x = np.linspace(0, 4.5, 1000)\n",
    "logprob = gmm.score_samples(x.reshape(-1, 1))\n",
    "responsibilities = gmm.predict_proba(x.reshape(-1, 1))\n",
    "pdf = np.exp(logprob)\n",
    "pdf_individual = responsibilities * pdf[:, np.newaxis]\n",
    "plt.plot(x, pdf, '-k')\n",
    "plt.plot(x, pdf_individual, '--k')\n",
    "#annotate the two gaussians with their means only without an arrow\n",
    "plt.annotate('Gaussian 1: '+str(round(gmm.means_[0][0],2)), xy=(gmm.means_[0], 0.1), xytext=(gmm.means_[0], 0.1))\n",
    "plt.annotate('Gaussian 2: '+str(round(gmm.means_[1][0],2)), xy=(gmm.means_[1], 0.1), xytext=(gmm.means_[1], 0.2))\n",
    "plt.show()\n",
    "\n",
    "#do the same GMM but for the log10 of the diffusion\n",
    "gmm_log = mixture.GaussianMixture(n_components=2, covariance_type='full')\n",
    "diff_tracks_poly = np.log10(diff_tracks_poly)\n",
    "gmm_log.fit(diff_tracks_poly)\n",
    "labels = gmm_log.predict(diff_tracks_poly)\n",
    "#plot the gaussian mixture model\n",
    "plt.clf()\n",
    "plt.hist(diff_tracks_poly, 15, density=True, alpha=0.6, color='g')\n",
    "x = np.linspace(-2, 1, 1000)\n",
    "logprob = gmm_log.score_samples(x.reshape(-1, 1))\n",
    "responsibilities = gmm_log.predict_proba(x.reshape(-1, 1))\n",
    "pdf = np.exp(logprob)\n",
    "pdf_individual = responsibilities * pdf[:, np.newaxis]\n",
    "plt.plot(x, pdf, '-k')\n",
    "plt.plot(x, pdf_individual, '--k')\n",
    "#annotate the two gaussians with their means only without an arrow\n",
    "plt.annotate('Gaussian 1: '+str(round(gmm_log.means_[0][0],2)), xy=(gmm_log.means_[0], 0.1), xytext=(gmm_log.means_[0], 0.1))\n",
    "plt.annotate('Gaussian 2: '+str(round(gmm_log.means_[1][0],2)), xy=(gmm_log.means_[1], 0.1), xytext=(gmm_log.means_[1], 0.2))\n",
    "plt.show()\n",
    "\n",
    "#do a 3 GMM for the log10 diffusion\n",
    "gmm_log3 = mixture.GaussianMixture(n_components=3, covariance_type='full')\n",
    "gmm_log3.fit(diff_tracks_poly)\n",
    "labels = gmm_log3.predict(diff_tracks_poly)\n",
    "#plot the gaussian mixture model\n",
    "plt.clf()\n",
    "plt.hist(diff_tracks_poly, 15, density=True, alpha=0.6, color='g')\n",
    "x = np.linspace(-2, 1, 1000)\n",
    "logprob = gmm_log3.score_samples(x.reshape(-1, 1))\n",
    "responsibilities = gmm_log3.predict_proba(x.reshape(-1, 1))\n",
    "pdf = np.exp(logprob)\n",
    "pdf_individual = responsibilities * pdf[:, np.newaxis]\n",
    "plt.plot(x, pdf, '-k')\n",
    "plt.plot(x, pdf_individual, '--k')\n",
    "#annotate the two gaussians with their means only without an arrow\n",
    "plt.annotate('Gaussian 1: '+str(round(gmm_log3.means_[0][0],2)), xy=(gmm_log3.means_[0], 0.1), xytext=(gmm_log3.means_[0], 0.1))\n",
    "plt.annotate('Gaussian 2: '+str(round(gmm_log3.means_[1][0],2)), xy=(gmm_log3.means_[1], 0.1), xytext=(gmm_log3.means_[1], 0.2))\n",
    "plt.annotate('Gaussian 3: '+str(round(gmm_log3.means_[2][0],2)), xy=(gmm_log3.means_[2], 0.1), xytext=(gmm_log3.means_[2], 0.3))\n",
    "plt.show()\n",
    "\n",
    "plt.hist(diff_tracks_poly, 6, alpha=0.6, color='g')\n",
    "plt.xlabel(\"Track Diffusion\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bulk msd rpoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msds = rp_ez._bulk_msd_plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the msds for \"no_drop\"\n",
    "plt.clf()\n",
    "#plt.hist(np.log10(np.array(msds[\"no_drop\"])[:,0]),bins=10,label=\"no_drop\",alpha=0.2)\n",
    "#plot the msds for \"IN\"\n",
    "plt.hist(np.log10(np.array(msds[\"in_drop\"])[:,0]),bins=10,label=\"in_drop\",alpha=0.2)\n",
    "#plot the msds for \"OUT\"\n",
    "plt.hist(np.log10(np.array(msds[\"ot_drop\"])[:,0]),bins=10,label=\"out_drop\",alpha=0.2)\n",
    "#plot the msds for io_drop\n",
    "plt.hist(np.log10(np.array(msds[\"io_drop\"])[:,0]),bins=10,label=\"io_drop\",alpha=0.2)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_radius(dataset,plot=True,bins=5):\n",
    "    radius = []\n",
    "    for i,j in dataset.Movie.items():\n",
    "        for k,l in j.Cells.items():\n",
    "            for m,n in l.Drop_Collection.items():\n",
    "                radius.append(n[2])\n",
    "    radius = np.array(radius)\n",
    "    radius = radius[radius>0]\n",
    "    radius_rp_ez = np.array(radius)*globals[\"olympus_pixel_size\"]/1000\n",
    "    rad = []\n",
    "    for i,j in dataset.Movie.items():\n",
    "        for k,l in j.Cells.items():\n",
    "            for m,n in l.All_Drop_Collection.items():\n",
    "                rad.append(n[2])\n",
    "    rad = np.array(rad)\n",
    "    rad = rad[rad>0]\n",
    "    rad = rad*globals[\"olympus_pixel_size\"]/1000\n",
    "    viable_rp_ez_mean = np.mean(radius_rp_ez)\n",
    "    viable_corr_mean = viable_rp_ez_mean/viable_rp_ez_mean\n",
    "    if plot:\n",
    "        plt.hist(radius_rp_ez,alpha = 0.1,label = \"viable\",bins=bins)#,cumulative=True,density=True)\n",
    "        plt.hist(rad,alpha = 0.1, label = \"All\",bins=bins)#,cumulative=True,density=True)\n",
    "        plt.xlabel(\"Radius in um\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        #plt.yscale(\"log\")\n",
    "        #plt.xscale(\"log\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return [radius_rp_ez,rad,viable_corr_mean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "viable_rp,all_rp,corr_rp = get_radius(rp_ez_scale,bins=20)\n",
    "viable_ll,all_ll,corr_ll = get_radius(ll_ez,bins=20)\n",
    "\n",
    "#convert the viable_rp and viable_ll to be in area instead of radius\n",
    "viable_rp = viable_rp**2*np.pi\n",
    "viable_ll = viable_ll**2*np.pi\n",
    "#plot the viable_rp and viable_ll as boxplots\n",
    "plt.clf()\n",
    "plt.boxplot([viable_rp,viable_ll],labels=[\"RPOC\",\"LL\"])\n",
    "#plot the individual points\n",
    "plt.plot(np.random.normal(1, 0.04, size=len(viable_rp)), viable_rp, 'r.', alpha=0.2)\n",
    "plt.plot(np.random.normal(2, 0.04, size=len(viable_ll)), viable_ll, 'b.', alpha=0.2)\n",
    "#perform a KS test and plot the reuslts\n",
    "ks = stats.ks_2samp(viable_rp,viable_ll)\n",
    "plt.annotate(\"KS test: \"+str(round(ks[1],3)),xy=(1.5,0.05))\n",
    "#plot a horizontal line at 0.1413\n",
    "plt.axhline(y=0.1413, color='k', linestyle='--')\n",
    "\n",
    "plt.ylabel(\"Area in um^2\")\n",
    "plt.show()\n",
    "\n",
    "#plot viable rp and ll on one plot with the same bins\n",
    "plt.clf()\n",
    "bins = np.linspace(0.01, 0.2, 20)\n",
    "plt.hist(viable_rp,alpha = 0.5,label = \"RPOC\",bins=bins)#,cumulative=True,density=True,histtype='step')\n",
    "plt.hist(viable_ll,alpha = 0.5, label = \"LL\",bins=bins)#,cumulative=True,density=True,histtype='step')\n",
    "plt.xlabel(\"Radius in um\")\n",
    "plt.ylabel(\"Count\")\n",
    "#plt.yscale(\"log\")\n",
    "#plt.xscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(len(viable_rp))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_avg_points_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_thresh = 0.5\n",
    "min_length_track = 5\n",
    "\n",
    "def _avg_points_drop(dataset):\n",
    "    drop_locs = {}\n",
    "    total_locs = {}\n",
    "    total_clusters = {}\n",
    "    total_cell_locs = {}\n",
    "    area_cell = {}\n",
    "    area_drop = {}\n",
    "    area_drop_cell_diff = {}\n",
    "    total_tracks = {}\n",
    "    drop_tracks = {}\n",
    "    total_track_counts = {}\n",
    "    for i,j in dataset.Movie.items():\n",
    "        for k,l in j.Cells.items():\n",
    "            if len(l.raw_tracks) != 0:\n",
    "                \n",
    "                sorted_tracks = dataset._convert_track_frame(np.array(l.raw_tracks),t_len_l=min_length_track,t_len_u = 100)\n",
    "                total_clusters[\"{0},{1}\".format(i,k)] = len(l.Drop_Collection)\n",
    "                loc_count = 0\n",
    "                total_track_count = 0\n",
    "                for lll in sorted_tracks[1]:\n",
    "                    total_track_count+=len(lll)\n",
    "                    for llll in lll:\n",
    "                        loc_count+=len(llll)\n",
    "\n",
    "                total_cell_locs[\"{0},{1}\".format(i,k)] = loc_count\n",
    "                area_cell[\"{0},{1}\".format(i,k)] = 0.0169*l.cell_area[0]\n",
    "                for m,n in l.Drop_Collection.items():\n",
    "                    counter = 0\n",
    "                    total_localizations = 0\n",
    "                    total_tracks_t = 0\n",
    "                    drop_tracks_t = 0\n",
    "                    x,y = sorted_tracks[1],sorted_tracks[2]\n",
    "                    for tt in range(len(x[int(m[0])])):\n",
    "                        occupancy = 0\n",
    "                        total_tracks_t+=1\n",
    "                        for ttt in range(len(x[int(m[0])][tt])):\n",
    "                            total_localizations+=1\n",
    "                            if point_inside_circle2D(n,[x[int(m[0])][tt][ttt],y[int(m[0])][tt][ttt]]):\n",
    "                                counter+=1\n",
    "                                occupancy+=1\n",
    "                        if occupancy/len(x[int(m[0])][tt]) > track_thresh:\n",
    "                            drop_tracks_t+=1\n",
    "                    total_track_counts[\"{0},{1},{2}\".format(i,k,m)] = total_track_count\n",
    "                    drop_locs[\"{0},{1},{2}\".format(i,k,m)] = counter\n",
    "                    total_locs[\"{0},{1},{2}\".format(i,k,m)] = total_localizations\n",
    "                    total_tracks[\"{0},{1},{2}\".format(i,k,m)] = total_tracks_t\n",
    "                    drop_tracks[\"{0},{1},{2}\".format(i,k,m)] = drop_tracks_t\n",
    "                    area_drop[\"{0},{1},{2}\".format(i,k,m)] = 0.0169*np.pi*n[2]**2\n",
    "                    area_drop_cell_diff[\"{0},{1},{2}\".format(i,k,m)] = 0.0169*(l.cell_area[0])# - np.pi*n[2]**2)\n",
    "            \n",
    "    return drop_locs,total_locs,total_clusters,total_cell_locs,area_cell,area_drop,area_drop_cell_diff,total_tracks,drop_tracks,total_track_counts\n",
    "\n",
    "drop_localizations,total_localizations,total_clusters,total_cell_locs,area_cell,area_drop,area_drop_cell_diff,total_tracks,drop_tracks,total_track_counts = _avg_points_drop(ll_ez)\n",
    "print(area_cell)\n",
    "print(area_drop)\n",
    "#print the average area of a cell\n",
    "print(\"Mean cell area: {}\".format(np.mean(list(area_cell.values()))))\n",
    "print(\"Mean drop area: {}\".format(np.mean(list(area_drop.values()))))\n",
    "#plot the area per cell\n",
    "plt.clf()\n",
    "plt.hist(np.array(list(area_cell.values())),bins=5)\n",
    "plt.xlabel(\"Cell Area in um^2\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Cell Area Distribution\")\n",
    "plt.show()\n",
    "\n",
    "#plot the area per drop\n",
    "plt.clf()\n",
    "plt.hist(np.array(list(area_drop.values())),bins=5)\n",
    "plt.xlabel(\"Drop Area in um^2\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Drop Area Distribution\")\n",
    "plt.show()\n",
    "\n",
    "#plot the density per drop and per cell on the same plot as a histogram\n",
    "plt.clf()\n",
    "plt.hist(np.array(list(drop_localizations.values()))/np.array(list(area_drop.values()))/1000,bins=20,alpha=0.5,label=\"Drop\")\n",
    "plt.hist(np.array(list(total_localizations.values()))/np.array(list(area_drop_cell_diff.values()))/1000,bins=20,alpha=0.5,label=\"Cell\")\n",
    "plt.xlabel(\"Point Density in um^-2\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.title(\"Points Per Area (Density) Distribution\")\n",
    "plt.show()\n",
    "\n",
    "track_density_drop = np.array(list(drop_tracks.values()))/np.array(list(area_drop.values()))/1000\n",
    "track_density_cell = np.array(list(total_tracks.values()))/np.array(list(area_drop_cell_diff.values()))/1000\n",
    "track_drop_cell_density_ratio = track_density_drop/track_density_cell\n",
    "\n",
    "#plot the track density ratio as a box plot\n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.boxplot(track_drop_cell_density_ratio)\n",
    "ax.set_ylabel(\"Track Density Ratio\")\n",
    "plt.show()\n",
    "\n",
    "track_drop_cell_density_ratio_rp = track_drop_cell_density_ratio\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.scatter(track_density_cell,track_density_drop)\n",
    "plt.xlabel(\"Cell Track Density in um^-2\")\n",
    "plt.ylabel(\"Drop Track Density in um^-2\")\n",
    "plt.title(\"Track Density Comparison\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#plot the track_drop_cell_density_ratio as a function of area_drop_cell_diff\n",
    "plt.clf()\n",
    "plt.scatter(np.array(list(area_drop_cell_diff.values())),track_drop_cell_density_ratio)\n",
    "plt.xlabel(\"Cell Area in um^2\")\n",
    "plt.ylabel(\"Track Density Ratio\")\n",
    "plt.title(\"Track Density Ratio vs Cell Area\")\n",
    "plt.show()\n",
    "\n",
    "#plot it as a function of area_drop\n",
    "plt.clf()\n",
    "plt.scatter(np.array(list(area_drop.values())),track_drop_cell_density_ratio)\n",
    "plt.xlabel(\"Drop Area in um^2\")\n",
    "plt.ylabel(\"Track Density Ratio\")\n",
    "plt.title(\"Track Density Ratio vs Drop Area\")\n",
    "plt.show()\n",
    "\n",
    "#plot it as a function of track_density_cell\n",
    "plt.clf()\n",
    "plt.scatter(track_density_cell,track_drop_cell_density_ratio)\n",
    "plt.xlabel(\"Cell Track Density in um^-2\")\n",
    "plt.ylabel(\"Track Density Ratio\")\n",
    "plt.title(\"Track Density Ratio vs Cell Track Density\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#and again as a function of track_density_drop\n",
    "plt.clf()\n",
    "plt.scatter(track_density_drop,track_drop_cell_density_ratio)\n",
    "plt.xlabel(\"Drop Track Density in um^-2\")\n",
    "plt.ylabel(\"Track Density Ratio\")\n",
    "plt.title(\"Track Density Ratio vs Drop Track Density\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#plot the track_density_drop as a function of area_drop\n",
    "plt.clf()\n",
    "plt.scatter(np.array(list(area_drop.values())),track_density_drop)\n",
    "plt.xlabel(\"Drop Area in um^2\")\n",
    "plt.ylabel(\"Track Density in um^-2\")\n",
    "plt.title(\"Track Density vs Drop Area\")\n",
    "plt.show()\n",
    "\n",
    "#plot the track_density_cell as a function of area_drop_cell_diff\n",
    "plt.clf()\n",
    "plt.scatter(np.array(list(area_drop_cell_diff.values())),track_density_cell)\n",
    "plt.xlabel(\"Cell Area in um^2\")\n",
    "plt.ylabel(\"Track Density in um^-2\")\n",
    "plt.title(\"Track Density vs Cell Area\")\n",
    "plt.show()\n",
    "\n",
    "#replot both on the same plot, also in a zoomed inset plot the plot the track_density_drop as a function of area_drop\n",
    "plt.clf()\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(np.array(list(area_drop.values())),track_density_drop,label=\"Drop\")\n",
    "ax.scatter(np.array(list(area_drop_cell_diff.values())),track_density_cell,label=\"Cell\")\n",
    "ax.set_xlabel(\"Area in um^2\")\n",
    "ax.set_ylabel(\"Track Density in um^-2\")\n",
    "ax.set_title(\"Track Density vs Area\")\n",
    "ax.legend()\n",
    "#create gridpoints for the inset plot with numbers in the range of the data\n",
    "x = np.linspace(np.min(np.array(list(area_drop.values()))),np.max(np.array(list(area_drop.values()))),100)\n",
    "y = np.linspace(np.min(track_density_drop),np.max(track_density_drop),100)\n",
    "#plot the inset plot\n",
    "axins = ax.inset_axes([0.1,0.5,0.4,0.4])\n",
    "axins.scatter(np.array(list(area_drop.values())),track_density_drop)\n",
    "axins.set_xlim(np.min(np.array(list(area_drop.values())))-1,np.max(np.array(list(area_drop.values())))+1)\n",
    "axins.set_ylim(np.min(track_density_drop)-1,np.max(track_density_drop)+1)\n",
    "#format the ticklabels to be less ugly\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "axins.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "axins.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "#plot the gridpoints\n",
    "axins.plot(x,y)\n",
    "#plot the lines that indicate the zoomed area\n",
    "ax.indicate_inset_zoom(axins)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "density_drop = np.array(list(drop_localizations.values()))/np.array(list(area_drop.values()))/1000\n",
    "density_cell = np.array(list(total_localizations.values()))/np.array(list(area_drop_cell_diff.values()))/1000\n",
    "drop_cell_density_ratio = density_drop/density_cell\n",
    "drop_area = np.array(list(area_drop.values()))\n",
    "\n",
    "unique_drop_area = {}\n",
    "unique_density_ratio_area = {}\n",
    "unique_track_density_ratio_area = {}\n",
    "unique_track_density_area = {}\n",
    "for i in range(len(drop_area)):\n",
    "    if str(drop_area[i]) not in unique_drop_area:\n",
    "        unique_drop_area[str(drop_area[i])] = []\n",
    "    else:\n",
    "        unique_drop_area[str(drop_area[i])].append(density_drop[i])\n",
    "    \n",
    "    if str(drop_area[i]) not in unique_density_ratio_area:\n",
    "        unique_density_ratio_area[str(drop_area[i])] = []\n",
    "    else:\n",
    "        unique_density_ratio_area[str(drop_area[i])].append(drop_cell_density_ratio[i])\n",
    "    \n",
    "    if str(drop_area[i]) not in unique_track_density_ratio_area:\n",
    "        unique_track_density_ratio_area[str(drop_area[i])] = []\n",
    "    else:\n",
    "        unique_track_density_ratio_area[str(drop_area[i])].append(track_drop_cell_density_ratio[i])\n",
    "    \n",
    "    if str(drop_area[i]) not in unique_track_density_area:\n",
    "        unique_track_density_area[str(drop_area[i])] = []\n",
    "    else:\n",
    "        unique_track_density_area[str(drop_area[i])].append(track_density_drop[i])\n",
    "\n",
    "#for each element in unique drop area find the mean of its values\n",
    "unique_drop_area_list = []\n",
    "unique_drop_area_mean = []\n",
    "unique_drop_area_SE = []\n",
    "#do the same for the density ratio\n",
    "unique_density_ratio_area_mean = []\n",
    "unique_density_ratio_area_SE = []\n",
    "#do the same for the track density ratio\n",
    "unique_track_density_ratio_area_mean = []\n",
    "unique_track_density_ratio_area_SE = []\n",
    "#do the same for the track density\n",
    "unique_track_density_area_mean = []\n",
    "unique_track_density_area_SE = []\n",
    "\n",
    "\n",
    "for i,j in unique_track_density_ratio_area.items():\n",
    "    unique_track_density_ratio_area_mean.append(np.mean(j))\n",
    "    unique_track_density_ratio_area_SE.append(np.std(j)/np.sqrt(len(j)))\n",
    "\n",
    "\n",
    "for i,j in unique_density_ratio_area.items():\n",
    "    unique_density_ratio_area_mean.append(np.mean(j))\n",
    "    unique_density_ratio_area_SE.append(np.std(j)/np.sqrt(len(j)))\n",
    "\n",
    "\n",
    "for i,j in unique_drop_area.items():\n",
    "    unique_drop_area_list.append(float(i))\n",
    "    unique_drop_area_mean.append(np.mean(j))\n",
    "    unique_drop_area_SE.append(np.std(j)/np.sqrt(len(j)))\n",
    "\n",
    "for i,j in unique_track_density_area.items():\n",
    "    unique_track_density_area_mean.append(np.mean(j))\n",
    "    unique_track_density_area_SE.append(np.std(j)/np.sqrt(len(j)))\n",
    "\n",
    "plt.clf()\n",
    "plt.errorbar(unique_drop_area_list,unique_track_density_area_mean,yerr=unique_track_density_area_SE,fmt='o')\n",
    "plt.xlabel(\"Drop Area in um^2\")\n",
    "plt.ylabel(\"Track Density (Molecules) in um^-2\")\n",
    "plt.title(\"Track Density (Molecules) vs Drop Area\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.errorbar(unique_drop_area_list,unique_drop_area_mean,yerr=unique_drop_area_SE,fmt='o')\n",
    "plt.xlabel(\"Drop Area in um^2\")\n",
    "plt.ylabel(\"Point Density in um^-2\")\n",
    "plt.title(\"Density vs Drop Area\")\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.errorbar(unique_drop_area_list,unique_density_ratio_area_mean,yerr=unique_density_ratio_area_SE,fmt='o')\n",
    "plt.xlabel(\"Drop Area in um^2\")\n",
    "plt.ylabel(\"Drop/Cell Density Ratio\")\n",
    "plt.title(\"Drop/Cell Density Ratio vs Drop Area\")\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.errorbar(unique_drop_area_list,unique_track_density_ratio_area_mean,yerr=unique_track_density_ratio_area_SE,fmt='o')\n",
    "plt.xlabel(\"Drop Area in um^2\")\n",
    "plt.ylabel(\"Drop/Cell Track Density Ratio\")\n",
    "plt.title(\"Drop/Cell Track Density Ratio vs Drop Area\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#plot the density_drop as a function of drop_area\n",
    "#plot an exponential decay fit to the data\n",
    "\n",
    "def exp_decay(x,a,b,c):\n",
    "    return a*np.exp(-b*x) + c\n",
    "\n",
    "popt,pcov = curve_fit(exp_decay,drop_area,density_drop)\n",
    "print(\"Fit parameters: {}\".format(popt))\n",
    "print(\"Fit covariance: {}\".format(pcov))\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.scatter(drop_area,density_drop)\n",
    "plt.plot(np.linspace(0.1,0.6,100),exp_decay(np.linspace(0.1,1.5,100),*popt),label=\"Fit\")\n",
    "plt.xlabel(\"Drop Area in um^2\")\n",
    "plt.ylabel(\"Point Density in um^-2\")\n",
    "plt.title(\"Density vs Drop Area\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(drop_area,np.array(list(drop_localizations.values())))\n",
    "plt.xlabel(\"Drop Area in um^2\")\n",
    "plt.ylabel(\"Number of Localizations in Drop\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#print the average density per drop\n",
    "print(\"Mean density per drop: {}\".format(np.mean(np.array(list(drop_localizations.values()))/np.array(list(area_drop.values())))))\n",
    "#print the average density per cell\n",
    "print(\"Mean density per cell: {}\".format(np.mean(np.array(list(total_localizations.values()))/np.array(list(area_drop_cell_diff.values())))))\n",
    "\n",
    "\n",
    "print(drop_localizations)\n",
    "print(total_localizations)\n",
    "print(total_clusters)\n",
    "print(total_cell_locs)\n",
    "avg_drop_localizations = np.mean(list(drop_localizations.values()))\n",
    "std_drop_localizations = np.std(list(drop_localizations.values()))\n",
    "print(avg_drop_localizations,std_drop_localizations,np.mean(list(total_localizations.values())),np.std(list(total_localizations.values())))  \n",
    "print(\"Mean number of clusters per cell: {}\".format(np.mean(list(total_clusters.values()))))   \n",
    "#plot the distribution of localizations per drop\n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.hist(list(total_clusters.values()),alpha=0.5,label=\"Viable Drops per cell\",bins=5)\n",
    "ax.set_xlabel(\"Number of viable drops per cell\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "#add a annotation with the mean and std\n",
    "ax.annotate(\"Mean: {0:.2f}\\nStd: {1:.2f}\".format(np.mean(list(total_clusters.values())),np.std(list(total_clusters.values()))),xy=(0.5,0.5),xycoords=\"axes fraction\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "#plot the distribution of localizations per drop\n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.hist(list(drop_localizations.values()),alpha=0.5,label=\"# localizations per drop\",bins=5)\n",
    "ax.set_xlabel(\"Number of localizations per drop\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "#plot the total_localizations per cell \n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.hist(list(total_cell_locs.values()),alpha=0.5,label=\"# localizations per cell\",bins=5)\n",
    "ax.set_xlabel(\"Number of localizations per cell\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "#plot the rp track_drop_cell_density_ratio and ll as a box plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.boxplot([track_drop_cell_density_ratio_rp,track_drop_cell_density_ratio_ll],labels=[\"RPOC\",\"LL\"])\n",
    "#plot the individual points\n",
    "ax.scatter(np.random.normal(1,0.1,len(track_drop_cell_density_ratio_rp)),track_drop_cell_density_ratio_rp,alpha=0.5,color='r')\n",
    "ax.scatter(np.random.normal(2,0.1,len(track_drop_cell_density_ratio_ll)),track_drop_cell_density_ratio_ll,alpha=0.5,color='b')\n",
    "#plot a horizontal line at 1\n",
    "ax.axhline(1,color='k',linestyle='--')\n",
    "\n",
    "ax.set_ylabel(\"Track/Drop Cell Density Ratio\")\n",
    "ax.set_title(\"Track/Drop Cell Density Ratio\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plt.imshow(rp_ez.Movie[\"0\"].Cells[\"0\"].cell_mask, cmap='gray', interpolation='none')\n",
    "mask = rp_ez.Movie[\"0\"].Cells[\"0\"].cell_mask\n",
    "cell = read_file(rp_ez._get_movie_path('0',1))\n",
    "masked = cell*mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ID = '1'\n",
    "a = rp_ez_scale.Movie[movie_ID].Movie_nucleoid\n",
    "b = blob_detection(path = a,\\\n",
    "                    threshold=5e-2,\n",
    "                    overlap=0.9,\n",
    "                    min_sigma=1/np.sqrt(2),\n",
    "                    max_sigma=10/np.sqrt(2),\n",
    "                    num_sigma=500,\n",
    "                    median = False,\n",
    "                    verbose=True)\n",
    "b._update_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                    \"plot_fit\":False,\n",
    "                                    \"fitting_image\":\"Original\",\n",
    "                                    \"radius_func\":identity,\n",
    "                                    \"residual_func\":residuals_gaus2d,\n",
    "                                    \"sigma_range\":0.5,\n",
    "                                    \"centroid_range\":0.5,\n",
    "                                    \"height_range\":2})\n",
    "c = b.detection(type='bp')\n",
    "aa = read_file(a)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.imshow(aa,cmap = 'Greys')\n",
    "# for i in rp_ez.Movie['1'].Cells['1'].All_Drop_Collection.values():\n",
    "#     cir = plt.Circle((i[0],i[1]),radius=i[2], fill = False)\n",
    "#     ax.add_artist(cir)\n",
    "for i in c[\"Scale\"]:\n",
    "    cir = plt.Circle((i[1],i[0]),radius=i[2], fill = False)\n",
    "    ax.add_artist(cir)\n",
    "plt.show()\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rp_ez.Movie[movie_ID].Movie_nucleoid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import filters\n",
    "#fig,ax = run_analysis_plotting.plot_img(rp_ez,1,movie_ID,1,int(\"0,2\"[0]))\n",
    "a =rp_ez_scale._get_movie_path('1',3)\n",
    "b = blob_detection(path = a,\\\n",
    "                    median= False,\\\n",
    "                    threshold= 1e-3, \\\n",
    "                    min_sigma= 1/np.sqrt(2), \\\n",
    "                    max_sigma = 10/np.sqrt(2), \\\n",
    "                    num_sigma= 500, \\\n",
    "                    overlap = 0,\n",
    "                    verbose=True)\n",
    "c = b.detection(type = 'bp')\n",
    "aa = read_file(a)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "#ax.imshow(filters.median_filter(aa,size =1),cmap = 'Greys')\n",
    "ax.imshow(aa,cmap = 'Greys')\n",
    "# for i in rp_ez.Movie['1'].Cells['7'].All_Drop_Collection.values():\n",
    "#     cir = plt.Circle((i[0],i[1]),radius=i[2], fill = False)\n",
    "#     ax.add_artist(cir)\n",
    "for i in c[\"Scale\"]:\n",
    "    cir = plt.Circle((i[1],i[0]),radius=i[2], fill = False, color = 'r', linewidth = 0.2)\n",
    "    ax.add_artist(cir)\n",
    "plt.show()\n",
    "print(rp_ez.Movie['1'].Cells['1'].All_Drop_Collection)\n",
    "print(rp_ez.Movie['1'].Cells['1'].All_Drop_Verbose)\n",
    "print(report_fit(rp_ez.Movie['1'].Cells['1'].All_Drop_Verbose['0,0'][\"Fit\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [5,7,10,15,20,30]\n",
    "axis_x = 1\n",
    "axis_y = 1\n",
    "x,y,fig,ax=run_analysis_plotting.draw_item(rp_ez_scale,(axis_x,axis_y),all_tracks=True,cell_ID = '4',movie_ID='3',movie_frame_index=4)\n",
    "print(rp_ez_scale.Movie['0'].Cells['3'].Drop_Collection)\n",
    "print(rp_ez_scale.Movie['0'].Cells['3'].All_Drop_Collection)\n",
    "# ax[0,0].set_xlim(160,185)\n",
    "# ax[0,0].set_ylim(135,163)\n",
    "#invert the y axis\n",
    "ax[0,0].invert_yaxis()\n",
    "#make the x,y axis without ticks\n",
    "#ax[0,0].set_xticks([])\n",
    "#ax[0,0].set_yticks([])\n",
    "x_y = np.array([[a,b] for a,b in zip(x,y)])\n",
    "for i in range(axis_x):\n",
    "    for j in range(axis_y):\n",
    "        clustering = OPTICS(min_samples=20).fit(x_y)\n",
    "        a = ax[i,j].scatter(x,y,s= 0.5,c = clustering.labels_)\n",
    "        pass\n",
    "#fig.colorbar(a)\n",
    "fig.tight_layout()\n",
    "for i,j in rp_ez_scale.Movie['0'].Cells['3'].All_Drop_Collection.items():\n",
    "    if i[0] == '0':\n",
    "        #make a circle for each drop\n",
    "        cir = plt.Circle((j[0],j[1]),radius=j[2], fill = False)\n",
    "        #add the circle to the plot\n",
    "        ax[0,0].add_artist(cir)\n",
    "\n",
    "#set the x,y lims to the min and max of the points\n",
    "ax[0,0].set_xlim(np.min(x),np.max(x))\n",
    "ax[0,0].set_ylim(np.min(y),np.max(y))\n",
    "# ax[0,0].set_xlim(120,145)\n",
    "# ax[0,0].set_ylim(105,125)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [5,7,10,15,20,30]\n",
    "axis_x = 1\n",
    "axis_y = 1\n",
    "x,y,fig,ax=run_analysis_plotting.draw_item(ll_ez,(axis_x,axis_y),all_tracks=True,cell_ID = '0',movie_ID='0',movie_frame_index=0)\n",
    "ax[0,0].set_xlim(150,200)\n",
    "ax[0,0].set_ylim(160,205)\n",
    "#invert the y axis\n",
    "ax[0,0].invert_yaxis()\n",
    "#make the x,y axis without ticks\n",
    "ax[0,0].set_xticks([])\n",
    "ax[0,0].set_yticks([])\n",
    "x_y = np.array([[a,b] for a,b in zip(x,y)])\n",
    "for i in range(axis_x):\n",
    "    for j in range(axis_y):\n",
    "        clustering = OPTICS(min_samples=samples[i*axis_x + j]).fit(x_y)\n",
    "        a = ax[i,j].scatter(x,y,s= 0.5,c='r')#c = clustering.labels_)\n",
    "        pass\n",
    "#fig.colorbar(a)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clustering = DBSCAN(eps=0.4, min_samples=4).fit(x_y)\n",
    "\n",
    "a = plt.scatter(x_y[:,0],x_y[:,1],s= 2,c = 'r')\n",
    "#remove the noise points\n",
    "x_y_cls = x_y[clustering.labels_ != -1]\n",
    "cluster_lab = clustering.labels_[clustering.labels_ != -1]\n",
    "#plot without the noise points\n",
    "plt.scatter(x_y_cls[:,0],x_y_cls[:,1],s= 2,c = cluster_lab)\n",
    "\n",
    "\n",
    "plt.colorbar(a)\n",
    "#make the two axis equal\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.tight_layout()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ID = '1'\n",
    "cell_ID = ['5']\n",
    "x = []\n",
    "y = []\n",
    "for k in cell_ID:\n",
    "    arr_1 = np.array(rp_ez.Movie[movie_ID].Cells[k].raw_tracks)\n",
    "    x+=list(arr_1[:,2])\n",
    "    y+=list(arr_1[:,3])\n",
    "\n",
    "#plot the x,y coordinates\n",
    "plt.scatter(x,y,s= 1)\n",
    "#perform clustering on thisnusing DBSCAN\n",
    "clustering = DBSCAN(eps=0.4, min_samples=20).fit(np.array([[a,b] for a,b in zip(x,y)]))\n",
    "#plot the clustering result\n",
    "a = plt.scatter(x,y,s= 2,c = clustering.labels_)\n",
    "plt.colorbar(a)\n",
    "\n",
    "\n",
    "#make the two axis equal\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.tight_layout()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "#find the area of each cluster using the convex hull\n",
    "from scipy.spatial import ConvexHull\n",
    "#find the unique labels\n",
    "unique_labels = set(clustering.labels_)\n",
    "#find the convex hull for each cluster\n",
    "for k in unique_labels:\n",
    "    if k == -1:\n",
    "        continue\n",
    "    #find the points in the cluster\n",
    "    points = np.array([[a,b] for a,b,c in zip(x,y,clustering.labels_) if c == k])\n",
    "    #find the convex hull\n",
    "    hull = ConvexHull(points)\n",
    "    #plot the convex hull\n",
    "    for simplex in hull.simplices:\n",
    "        plt.plot(points[simplex, 0], points[simplex, 1], 'r-')\n",
    "    area = hull.volume\n",
    "    #plot the area\n",
    "    plt.text(np.mean(points[:,0]),np.mean(points[:,1]),\"{0:.2f}\".format(area),fontsize=20,color='r')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Find the density of the points using 2d histogram\n",
    "hist, xedges, yedges = np.histogram2d(x, y, bins=100)\n",
    "extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "a = plt.imshow(hist.T, extent=extent, origin='lower')\n",
    "#make the two axis equal\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.tight_layout()\n",
    "plt.gca().invert_yaxis()\n",
    "#make a colorbar title\n",
    "plt.colorbar(a).set_label('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = simulate_foci.sim_foci(max_x = 200,\n",
    "                            min_x = 0,\n",
    "                            radius = 50,\n",
    "                            center = [100,100],\n",
    "                            total_points = 500,\n",
    "                            density_dif = 5.0,\n",
    "                            pdf = simulate_foci.tophat_function_2d)\n",
    "sim_xy = sim._makePoints()\n",
    "for i in samples:\n",
    "    clustering = OPTICS(min_samples=i).fit(sim_xy)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    a = ax.scatter(sim_xy[:,0],sim_xy[:,1],s= 2,c = clustering.labels_)\n",
    "    plt.colorbar(a)\n",
    "    cir = plt.Circle(sim.center,radius = sim.radius,fill = False)\n",
    "    ax.set_title(\"min_samples = {0}\".format(i))\n",
    "    ax.add_artist(cir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib qt \n",
    "#train data on the best of above: samples = 30\n",
    "clustering_fit = DBSCAN(min_samples=30,eps = 1.0).fit(sim_xy)\n",
    "print(rp_ez.Movie['1'].Cells['1'].Drop_Collection)\n",
    "print(rp_ez.Movie['1'].Cells['4'].Drop_Collection)\n",
    "x,y,fig,ax=run_analysis_plotting.draw_item(rp_ez,cell_ID = ['1'],movie_ID='1',movie_frame_index = 0,all_tracks = 1)\n",
    "x_y = np.array([[a,b] for a,b in zip(x,y)])\n",
    "clustering = clustering_fit.fit_predict(x_y)\n",
    "a = ax.scatter(x,y,s= 1,c = clustering)\n",
    "fig.colorbar(a)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "print(len(x))\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "a = ax.scatter(x,y,s= 20,c = clustering,cmap = \"Greys\")\n",
    "fig.colorbar(a)\n",
    "plt.gca().invert_yaxis()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_fit = DBSCAN(min_samples=30,eps = 1.0).fit(sim_xy)\n",
    "x,y,fig,ax=run_analysis_plotting.draw_item(rp_ez,cell_ID = ['1'],movie_ID='1',movie_frame_index = 0,all_tracks = 1)\n",
    "x_y = np.array([[a,b] for a,b in zip(x,y)])\n",
    "clustering = clustering_fit.fit_predict(x_y)\n",
    "a = ax.scatter(x,y,s= 1,c = clustering)\n",
    "fig.colorbar(a)\n",
    "plt.gca().invert_yaxis()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "non_cluster = np.where((np.asarray(clustering) >= 0))[0]\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot()\n",
    "# a = read_file(rp_ez.Movie['1'].Movie_nucleoid)\n",
    "# ax.imshow(a,cmap = 'Greys')\n",
    "# a = ax.scatter(np.asarray(x)[non_cluster],np.asarray(y)[non_cluster],s= 20,c = clustering[non_cluster],cmap = \"Greys\")\n",
    "# fig.colorbar(a)\n",
    "# ax.set_xlim((120,190))\n",
    "# ax.set_ylim((55,100))\n",
    "# plt.gca().invert_yaxis()\n",
    "# fig.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "a = read_file(rp_ez.Movie['1'].Movie_nucleoid)\n",
    "ax.imshow(a,cmap = 'Greys')\n",
    "a = ax.scatter(np.asarray(x)[non_cluster],np.asarray(y)[non_cluster],s= 20,c = clustering[non_cluster],cmap = \"Greys\")\n",
    "fig.colorbar(a)\n",
    "ax.set_xlim((170,240))\n",
    "ax.set_ylim((140,200))\n",
    "plt.gca().invert_yaxis()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "a = read_file(rp_ez.Movie['1'].Movie_nucleoid)\n",
    "ax.imshow(a,cmap = 'Greys')\n",
    "#a = ax.scatter(np.asarray(x)[non_cluster],np.asarray(y)[non_cluster],s= 20,c = clustering[non_cluster],cmap = \"Greys\")\n",
    "#fig.colorbar(a)\n",
    "ax.set_xlim((170,240))\n",
    "ax.set_ylim((140,200))\n",
    "plt.gca().invert_yaxis()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only take points in cluster one at a time\n",
    "unique_clusters = np.unique(clustering)[np.unique(clustering) > -1]\n",
    "cluster_xy = []\n",
    "for i in range(len(unique_clusters)):\n",
    "    indx_i = clustering == unique_clusters[i]\n",
    "    indx_i = np.array(indx_i)\n",
    "    x_indx = np.array(x)[indx_i]\n",
    "    y_indx = np.array(y)[indx_i]\n",
    "    #make pair \n",
    "    x_y_indx = np.array([[a,b] for a,b in zip(x_indx,y_indx)])\n",
    "    cluster_xy.append(x_y_indx)\n",
    "\n",
    "cluster_circles = []\n",
    "for i in cluster_xy:\n",
    "    circle = smallestenclosingcircle.make_circle(i)\n",
    "    cluster_circles.append(circle)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "a = ax.scatter(x,y,s= 20,c = clustering,cmap = \"Reds\")\n",
    "\n",
    "for i in cluster_circles:\n",
    "    Drawing_uncolored_circle = create_circle_obj(i,fill = False)\n",
    "    ax.add_artist(Drawing_uncolored_circle)\n",
    "\n",
    "\n",
    "fig.colorbar(a)\n",
    "plt.gca().invert_yaxis()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "print(cluster_circles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import plot\n",
    "\n",
    "which_object = rp_ez\n",
    "movie_ID = '0'\n",
    "cell_ID = '3'\n",
    "drop_ID = '0,0'\n",
    "copies = 1\n",
    "print(rp_ez.Movie[movie_ID].Cells[cell_ID].Drop_Collection)\n",
    "drop = which_object.Movie[movie_ID].Cells[cell_ID].Drop_Collection[drop_ID]\n",
    "cir = create_circle_obj(drop)\n",
    "fig,ax = run_analysis_plotting.plot_img(rp_ez,copies,movie_ID,cell_ID,int(drop_ID[0]))\n",
    "\n",
    "x_y = []\n",
    "\n",
    "def plot_lines(dic,color,fig,a,color_first = None,color_last = None):\n",
    "    len_tracks = []\n",
    "    for i,j in dic.items():\n",
    "        ax.plot(j.X,j.Y,color = color)\n",
    "        len_tracks.append(len(j.X))\n",
    "        if color_first != None:\n",
    "            ax.plot(j.X[0],j.Y[0],color = color_first,markersize = 2,marker = 'o')\n",
    "        if color_last != None:\n",
    "            ax.plot(j.X[-1],j.Y[-1],color = color_last,markersize = 2,marker = 'o')\n",
    "    return [len_tracks,fig,ax]\n",
    "    \n",
    "def plot_lines_bulk(movie_ID, cell_ID, drop_ID, fig, ax, plot_lines,**kwargs):\n",
    "    if kwargs.get(\"IN\",False) == True:\n",
    "        len_intracks = plot_lines(rp_ez.Movie[movie_ID].Cells[cell_ID].Trajectory_Collection[drop_ID].IN_Trajectory_Collection,\n",
    "                            'red',fig,ax,color_first='black',color_last='grey')[0]\n",
    "    else:\n",
    "        len_intracks = None\n",
    "    if kwargs.get(\"Io\",False) == True:\n",
    "        len_iotracks = plot_lines(rp_ez.Movie[movie_ID].Cells[cell_ID].Trajectory_Collection[drop_ID].IO_Trajectory_Collection,\n",
    "                            'orange',fig,ax,color_first='black',color_last='grey')[0]\n",
    "    else:\n",
    "        len_iotracks = None\n",
    "    if kwargs.get(\"OT\",False) == True:\n",
    "        len_ottracks = plot_lines(rp_ez.Movie[movie_ID].Cells[cell_ID].Trajectory_Collection[drop_ID].OUT_Trajectory_Collection,\n",
    "                            'green',fig,ax,color_first='black',color_last='grey')[0]\n",
    "    else:\n",
    "        len_ottracks = None                            \n",
    "    return len_intracks,len_iotracks,len_ottracks\n",
    "\n",
    "len_intracks, len_iotracks, len_ottracks = plot_lines_bulk(movie_ID, cell_ID, drop_ID, fig, ax, plot_lines, IN = False, IO = False, OT = False)\n",
    "\n",
    "#use bounding box of the cell to define the limits of viewing\n",
    "bounding_box = which_object.Movie[movie_ID].Cells[cell_ID].bounding_box\n",
    "extended_view = 20\n",
    "ax.set_xlim((np.min(np.asarray(bounding_box[:,0]))-extended_view,np.max(np.asarray(bounding_box[:,0]))+extended_view))\n",
    "ax.set_ylim((np.min(np.asarray(bounding_box[:,1]))-extended_view,np.max(np.asarray(bounding_box[:,1]))+extended_view))\n",
    "ax.add_artist(cir)\n",
    "for i,j in which_object.Movie[movie_ID].Cells[cell_ID].All_Drop_Collection.items():\n",
    "    print(j)\n",
    "    cir = create_circle_obj(j)\n",
    "    ax.add_artist(cir)\n",
    "ax.set_title(\"Track Info: IN = {0}, IO = {1}, OT = {2}\".format(len_intracks,len_iotracks,len_ottracks))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "print(which_object.Movie[movie_ID].Cells[cell_ID].cell_area, which_object.Movie[movie_ID].Cells[cell_ID].cell_axis_lengths)\n",
    "print(bounding_box)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import plot\n",
    "\n",
    "which_object = rp_ez\n",
    "movie_ID = '1'\n",
    "cell_ID = '0'\n",
    "drop_ID = '0,1'\n",
    "copies = 1\n",
    "print(rp_ez.Movie[movie_ID].Cells[cell_ID].Drop_Collection)\n",
    "drop = which_object.Movie[movie_ID].Cells[cell_ID].Drop_Collection[drop_ID]\n",
    "cir = create_circle_obj(drop)\n",
    "fig,ax = run_analysis_plotting.plot_img(rp_ez,copies,movie_ID,cell_ID,int(drop_ID[0]))\n",
    "\n",
    "x_y = []\n",
    "\n",
    "def plot_lines(dic,color,fig,a,color_first = None,color_last = None):\n",
    "    len_tracks = []\n",
    "    for i,j in dic.items():\n",
    "        ax.plot(j.X,j.Y,color = color)\n",
    "        len_tracks.append(len(j.X))\n",
    "        if color_first != None:\n",
    "            ax.plot(j.X[0],j.Y[0],color = color_first,markersize = 2,marker = 'o')\n",
    "        if color_last != None:\n",
    "            ax.plot(j.X[-1],j.Y[-1],color = color_last,markersize = 2,marker = 'o')\n",
    "    return [len_tracks,fig,ax]\n",
    "\n",
    "len_intracks, len_iotracks, len_ottracks = plot_lines_bulk(movie_ID, cell_ID, drop_ID, fig, ax, plot_lines, IN = True, IO = False, OT = False)\n",
    "\n",
    "#use bounding box of the cell to define the limits of viewing\n",
    "bounding_box = which_object.Movie[movie_ID].Cells[cell_ID].bounding_box\n",
    "extended_view = 10\n",
    "ax.set_xlim((np.min(np.asarray(bounding_box[:,0]))-extended_view,np.max(np.asarray(bounding_box[:,0]))+extended_view))\n",
    "ax.set_ylim((np.min(np.asarray(bounding_box[:,1]))-extended_view,np.max(np.asarray(bounding_box[:,1]))+extended_view))\n",
    "ax.add_artist(cir)\n",
    "ax.set_title(\"Track Info: IN = {0}, IO = {1}, OT = {2}\".format(len_intracks,len_iotracks,len_ottracks))\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "print(which_object.Movie[movie_ID].Cells[cell_ID].cell_area, which_object.Movie[movie_ID].Cells[cell_ID].cell_axis_lengths)\n",
    "print(bounding_box)\n",
    "\n",
    "def get_track_Tmsd(movie_ID, cell_ID, drop_ID):\n",
    "    for i,j in rp_ez.Movie[movie_ID].Cells[cell_ID].Trajectory_Collection[drop_ID].IN_Trajectory_Collection.items():\n",
    "        print(\"IN: \", j.MSD_total_um, \" Distance from out \", j.distance_from_drop)\n",
    "    for i,j in rp_ez.Movie[movie_ID].Cells[cell_ID].Trajectory_Collection[drop_ID].IO_Trajectory_Collection.items():\n",
    "        print(\"IO: \", j.MSD_total_um, \" Distance from out \", j.distance_from_drop)\n",
    "    for i,j in rp_ez.Movie[movie_ID].Cells[cell_ID].Trajectory_Collection[drop_ID].OUT_Trajectory_Collection.items():\n",
    "        print(\"OT: \", j.MSD_total_um, \" Distance from out \", j.distance_from_drop)\n",
    "\n",
    "get_track_Tmsd(movie_ID, cell_ID, drop_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "viable_rp,all_rp = get_radius(rp_ez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "rp_ez_ba = boundary_analysis(dataset = rp_ez.Movie)\n",
    "rp_ez_aa = rp_ez_ba.directional_displacement_bulk(IN = True,IO = True, OT = True)\n",
    "a = rp_ez_ba.plot_directional_displacements(dir_displacements = rp_ez_aa[0],dist_center = rp_ez_aa[1],angles = rp_ez_aa[2])\n",
    "def adjust_axis(a,lim = 1.5):\n",
    "    a[2].set_xlim((-lim,lim))\n",
    "    a[2].set_ylim((-lim,lim))\n",
    "    a[2].set_aspect(1, adjustable='box')\n",
    "    plt.show()\n",
    "adjust_axis(a,lim = 3)\n",
    "\n",
    "def plot_pairCorrelation(aa,**kwargs):\n",
    "    x,y = rt_to_xy(np.array(aa[1]),aa[2])\n",
    "    g_r, radii, interior_points = centered_pairCorrelation_2D(x = x,\n",
    "                                                        y = y,\n",
    "                                                        center = kwargs.get('center',[0,0]),\n",
    "                                                        rMax = kwargs.get('rMax',1.1),\n",
    "                                                        dr = kwargs.get('dr',0.1))\n",
    "    if kwargs.get(\"fig\", None) == None:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot()\n",
    "        ax.plot(radii,g_r)\n",
    "    else:\n",
    "        kwargs.get(\"ax\").plot(radii,g_r)\n",
    "    return [g_r, radii, interior_points,fig,ax]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = []\n",
    "centers = []\n",
    "disps = []\n",
    "for i,j in rp_ez.Movie.items():\n",
    "    for k,l in j.Cells.items():\n",
    "        sorted_tracks = rp_ez._convert_track_frame(track_set=np.array(l.raw_tracks),t_len_l = 1)\n",
    "        drops = l._convert_viableDrop_list()\n",
    "        for sf in range(len(sorted_tracks[0])):\n",
    "            x = sorted_tracks[1][sf]\n",
    "            y = sorted_tracks[2][sf]\n",
    "            drop = drops[sf]\n",
    "            if len(drop) > 0:\n",
    "                angle,center,disp = boundary_analysis._directional_variableTracks(x,y,drop)\n",
    "                angles+=angle\n",
    "                centers+=center\n",
    "                disps+=disp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.scatter(*rt_to_xy(np.array(rp_ez_aa[1]),rp_ez_aa[2]),s = 0.1)\n",
    "cir = plt.Circle((0,0),1,fill = False)\n",
    "ax.add_artist(cir)\n",
    "plt.xlim((-3,3))\n",
    "plt.ylim((-3,3))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.scatter(*rt_to_xy(np.array(centers),angles),s = 0.1)\n",
    "cir = plt.Circle((0,0),1,fill = False)\n",
    "ax.add_artist(cir)\n",
    "plt.xlim((-3,3))\n",
    "plt.ylim((-3,3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulate drops of different radius \n",
    "def sim_drop(radius = 1,center = [100,100],**kwargs):\n",
    "    sim = simulate_foci.sim_foci(max_x = kwargs.get(\"max_x\",400),\n",
    "                            min_x = 0,\n",
    "                            radius = radius,\n",
    "                            center = center,\n",
    "                            total_points = kwargs.get(\"points\",1000),\n",
    "                            density_dif = kwargs.get(\"density\",50.0),\n",
    "                            pdf = simulate_foci.tophat_function_2d)\n",
    "    sim_xy = sim._makePoints()\n",
    "    return sim_xy\n",
    "radi = np.arange(10,50,3)\n",
    "centers = np.random.randint(50,150,size = (2,len(radi)))\n",
    "\n",
    "mapped = []\n",
    "for i in range(len(radi)):\n",
    "    xy = sim_drop(radius=radi[i],center=centers[:,i])\n",
    "    mapped.append([xy[:,0],xy[:,1],[centers[:,i][0],centers[:,i][1],radi[i]]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "angle,center,disp = boundary_analysis._directional_displacement_utility(mapped)\n",
    "%matplotlib widget\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.scatter(*rt_to_xy(np.array(center),angle),s = 0.1)\n",
    "cir = plt.Circle((0,0),1,fill = False)\n",
    "ax.add_artist(cir)\n",
    "plt.xlim((-3,3))\n",
    "plt.ylim((-3,3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(mapped)):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    a = ax.scatter(mapped[i][0],mapped[i][1],s= 2)\n",
    "    cir = plt.Circle((mapped[i][2][0],mapped[i][2][1]),mapped[i][2][2],fill = False)\n",
    "    ax.add_artist(cir)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ID = '1'\n",
    "cell_ID = '1'\n",
    "frame = 0\n",
    "\n",
    "seg_frame = rp_ez._get_movie_path(movie_ID=movie_ID,frame=frame)\n",
    "gfp_img = rp_ez._get_nucleoid_path(movie_ID=movie_ID,cell_ID=cell_ID,full_path=False)\n",
    "shape = np.shape(gfp_img)\n",
    "x,y = np.meshgrid(np.arange(0,shape[0],1),np.arange(0,shape[1],1),indexing = 'ij')\n",
    "\n",
    "print(rp_ez.Movie[movie_ID].Cells[cell_ID].bounding_box)\n",
    "print(rp_ez.Movie[movie_ID].Cells[cell_ID].r_offset)\n",
    "print(rp_ez.Movie[movie_ID].Cells[cell_ID].Drop_Collection)\n",
    "\n",
    "\n",
    "def rescale_range(x,min_x,max_x,a,b):\n",
    "    '''https://stats.stackexchange.com/questions/281162/scale-a-number-between-a-range'''\n",
    "    return ((b-a)*(x - min_x)/(max_x - min_x)) + a\n",
    "\n",
    "def _gaussian_mesh_helper(mesh_2d,initial_xy,sub_arr = [3,3]):\n",
    "    ''' \n",
    "    takes a 2d_mesh (image data) and a bounding box to return a list of (x,y,z) in that bounding box\n",
    "    box is implimented from the center point of the pixel.\n",
    "    '''\n",
    "    #make x,y,z list from mesh data\n",
    "    #find dims\n",
    "    sub_arr = np.array(sub_arr)\n",
    "    initial_xy = np.array(initial_xy)\n",
    "    minx,miny = initial_xy - sub_arr\n",
    "    maxx,maxy = initial_xy + sub_arr\n",
    "    minx,miny = int(minx),int(miny)\n",
    "    maxx,maxy =int(maxx),int(maxy)\n",
    "    centers = [rescale_range(initial_xy[0],minx,maxx,0,-2*sub_arr[1]+1),rescale_range(initial_xy[1],miny,maxy,0,-2*sub_arr[0]+1)]\n",
    "    x,y = np.meshgrid(np.arange(minx,maxx,1),np.arange(miny,maxy,1))\n",
    "    mesh_view = mesh_2d[minx:maxx,miny:maxy]\n",
    "    \n",
    "    return [x-maxx+1,y-maxy+1,mesh_view,centers]\n",
    "\n",
    "xk,yk,zk,centers = _gaussian_mesh_helper(gfp_img,rp_ez.Movie[movie_ID].Cells[cell_ID].Drop_Collection['0,1'][:-1],sub_arr=[10,10])\n",
    "x_cent = centers[0]\n",
    "y_cent = centers[1]\n",
    "# plt.clf()\n",
    "# plt.imshow(zk)\n",
    "# plt.show()\n",
    "import lmfit\n",
    "from lmfit import Parameters, minimize, report_fit\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "\n",
    "def sim_foci_pdf(max_x,radius,center,density_dif,points,sigma):\n",
    "    min_x = 0\n",
    "    max_x = max_x\n",
    "    x1,y1 = tf.cast(tf.linspace(min_x,max_x,max_x),tf.float64), tf.cast(tf.linspace(min_x,max_x,max_x),tf.float64)\n",
    "    a = sim_drop(radius=radius,center=center,density_dif = density_dif,max_x = max_x, points = points)\n",
    "    x = a[:,0]\n",
    "    y = a[:,1]\n",
    "    sigma = np.array([sigma,sigma],dtype = type(a[0][0]))\n",
    "    z = 0\n",
    "    for i in range(len(a)):\n",
    "        z += np.array(simulate_foci.get_gaussian(a[i], sigma,domain = [x1,x1]))\n",
    "    return x1,y1,z\n",
    "\n",
    "x1, y1, z = sim_foci_pdf(max_x=200,radius=2.5,center=[100,100],density_dif=100.,points=1000,sigma=1)\n",
    "\n",
    "def gaussian2D(x, y, cen_x, cen_y, sig_x, sig_y, offset):\n",
    "    return np.exp(-(((cen_x-x)/sig_x)**2 + ((cen_y-y)/sig_y)**2)/2.0) + offset\n",
    "\n",
    "def gaus_constrained(x,y,sig_x,offset,kwargs = {}):\n",
    "    return gaussian2D(x, y, cen_x = kwargs.get(\"cen_x\",100), cen_y = kwargs.get(\"cen_y\",100), sig_x = sig_x, sig_y = kwargs.get(\"sig_y\",sig_x),offset = offset)\n",
    "\n",
    "\n",
    "def residuals(p, x, y, z,**kwargs):\n",
    "    height = p[\"height\"].value\n",
    "    #cen_x = p[\"centroid_x\"].value\n",
    "    #cen_y = p[\"centroid_y\"].value\n",
    "    sigma_x = p[\"sigma_x\"].value\n",
    "    #sigma_y = p[\"sigma_y\"].value\n",
    "    offset = p[\"background\"].value\n",
    "    return (z - height*gaus_constrained(x,y,sigma_x,offset,kwargs=kwargs))#gaussian2D(x,y, cen_x = cen_x, cen_y = cen_y, sig_x = sigma_x, sig_y = sigma_y, offset = offset))\n",
    "\n",
    "def initalize_2dgaus(**kwargs):\n",
    "    initial = Parameters()\n",
    "    for i,j in kwargs.items():\n",
    "        initial.add(i,value = j)\n",
    "    # initial.add(\"height\",value=.3)\n",
    "    # #initial.add(\"centroid_x\",value=100.)\n",
    "    # #initial.add(\"centroid_y\",value=100.)\n",
    "    # initial.add(\"sigma_x\",value=20.)\n",
    "    # #initial.add(\"sigma_y\",value=20.)\n",
    "    # initial.add(\"background\",value=0.015)\n",
    "    return initial\n",
    "\n",
    "initial = initalize_2dgaus(height = 0.3,sigma_x = 20.,background = 0.015)\n",
    "\n",
    "xx,yy = np.meshgrid(x1,y1,indexing=\"xy\")\n",
    "fit = minimize(residuals, initial, args=(yy, xx, z), kws = {\"cen_x\":100,\"cen_y\":100})\n",
    "print(report_fit(fit))\n",
    "#z1 = fit.params[\"height\"]*gaussian2D(xx,yy, cen_x = fit.params[\"centroid_x\"], cen_y = fit.params[\"centroid_y\"], sig_x = fit.params[\"sigma_x\"], sig_y = fit.params[\"sigma_y\"], offset = fit.params[\"background\"])\n",
    "z1 = fit.params[\"height\"]*gaus_constrained(xx,yy,fit.params[\"sigma_x\"],fit.params[\"background\"])\n",
    "# plt.clf() \n",
    "# #Change the Size of Graph using Figsize\n",
    "# fig = plt.figure(figsize=(10,10))\n",
    " \n",
    "\n",
    "# #Generating a 3D sine wave\n",
    "# ax = plt.axes(projection='3d')\n",
    "\n",
    "# x,y = np.meshgrid(x1,y1)\n",
    "# ax.plot_wireframe(x,y,z)\n",
    "# ax.plot_wireframe(x,y,z1,color = \"green\")\n",
    "# #plt.imshow(gfp_img,cmap = \"Greys\")\n",
    "# plt.show()\n",
    "initial = initalize_2dgaus(height = 10000,sigma_x = 2.,background = 100)\n",
    "fit = minimize(residuals, initial, args=(yk, xk, zk), kws = {\"cen_x\":x_cent,\"cen_y\":y_cent})\n",
    "print(report_fit(fit))\n",
    "z1 = fit.params[\"height\"]*gaus_constrained(xk,yk,fit.params[\"sigma_x\"],fit.params[\"background\"],kwargs = {\"cen_x\":x_cent,\"cen_y\":y_cent})\n",
    "plt.clf() \n",
    "\n",
    "#Change the Size of Graph using Figsize\n",
    "fig = plt.figure(figsize=(10,10))\n",
    " \n",
    "\n",
    "#Generating a 3D sine wave\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.plot_wireframe(xk,yk,zk)\n",
    "ax.scatter3D(x_cent,y_cent,10000)\n",
    "ax.plot_wireframe(xk,yk,z1,color = \"green\")\n",
    "#plt.imshow(gfp_img,cmap = \"Greys\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_ez.Movie['0'].Cells['0'].Trajectory_Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = rp_ez_ba.directional_displacement(rp_ez.Movie['1'].Cells['0'].Trajectory_Collection['0,1'].IN_Trajectory_Collection,rp_ez.Movie['1'].Cells['0'])\n",
    "b = rp_ez_ba.plot_directional_displacements(dir_displacements = a[2],dist_center = a[1],angles = a[0])\n",
    "adjust_axis(b)\n",
    "print(len(a[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LACI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_ez= run_analysis(\"/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/new_days/20190527/ll_ez\",\"laco_laci_ez\")\n",
    "ll_ez.read_parameters(minimum_percent_per_drop_in = 0.5, \n",
    "                    t_len_u = 25, \n",
    "                    t_len_l = 4, \n",
    "                    minimum_tracks_per_drop = 3)\n",
    " \n",
    "ll_ez.get_blob_parameters(threshold=1e-4,\n",
    "                        overlap=0,\n",
    "                        detection_name='bp',\n",
    "                        min_sigma=1/np.sqrt(2),\n",
    "                        max_sigma=5/np.sqrt(2),\n",
    "                        num_sigma=500,median = False)\n",
    "\n",
    "ll_ez.get_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                    \"plot_fit\":False,\n",
    "                                    \"fitting_image\":\"Original\",\n",
    "                                    \"radius_func\":np.mean,#identity,\n",
    "                                    \"residual_func\":residuals_gaus2d,\n",
    "                                    \"sigma_range\":0.5,\n",
    "                                    \"centroid_range\":0.5,\n",
    "                                    \"height_range\":2})\n",
    "ll_ez.type_of_blob =\"SCALE_SPACE_PLUS\"#ll_ez.type_of_blob = \"TRACKMATE\"\n",
    "ll_ez.a_file_style = \"new\"\n",
    "ll_ez.run_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_durations = ll_ez.track_durations\n",
    "print(temp_durations)\n",
    "#lets plot the histogram for the IN track durations\n",
    "plt.clf()\n",
    "plt.hist(temp_durations[\"IO\"],bins=20,density=True,label=\"IO\",alpha=0.5)\n",
    "plt.hist(temp_durations[\"IN\"],bins=20,density=True,label=\"IN\",alpha=0.5)\n",
    "plt.hist(temp_durations[\"OUT\"],bins=20,density=True,label=\"OO\",alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Track Duration (frames)\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Histogram of Track Durations for IN Tracks\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "print(len(temp_durations[\"IO\"]),len(temp_durations[\"IN\"]),len(temp_durations[\"OUT\"]))\n",
    "print(len(temp_durations[\"IN\"]),len(rp_ez_scale.track_collection[\"IN\"]))\n",
    "all_track_lengths = temp_durations[\"IN\"]\n",
    "all_track_diff = []\n",
    "for i in ll_ez.track_collection[\"IN\"]:\n",
    "    all_track_diff.append(i.MSD_total_um)\n",
    "\n",
    "plt.scatter(all_track_lengths,all_track_diff)\n",
    "plt.show()\n",
    "#print the means and std\n",
    "print(\"Mean of IN track durations: \",np.mean(temp_durations[\"IN\"]))\n",
    "print(\"Std of IN track durations: \",np.std(temp_durations[\"IN\"]))\n",
    "print(\"Mean of IO track durations: \",np.mean(temp_durations[\"IO\"]))\n",
    "print(\"Std of IO track durations: \",np.std(temp_durations[\"IO\"]))\n",
    "print(\"Mean of OT track durations: \",np.mean(temp_durations[\"OUT\"]))\n",
    "print(\"Std of OT track durations: \",np.std(temp_durations[\"OUT\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_ez1= run_analysis(\"/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/17/ll_ez\",\"ll_ez\")\n",
    "ll_ez1.read_parameters(minimum_percent_per_drop_in = 0.5, \n",
    "                    t_len_u = 100, \n",
    "                    t_len_l = 5, \n",
    "                    minimum_tracks_per_drop = 3)\n",
    " \n",
    "ll_ez1.get_blob_parameters(threshold=1e-4,\n",
    "                        overlap=0.5,\n",
    "                        detection_name='bp',\n",
    "                        min_sigma=1/np.sqrt(2),\n",
    "                        max_sigma=5/np.sqrt(2),\n",
    "                        num_sigma=500,median = False)\n",
    "\n",
    "ll_ez1.get_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                    \"plot_fit\":False,\n",
    "                                    \"fitting_image\":\"Original\",\n",
    "                                    \"radius_func\":np.mean,#identity,\n",
    "                                    \"residual_func\":residuals_gaus2d,\n",
    "                                    \"sigma_range\":0.5,\n",
    "                                    \"centroid_range\":0.5,\n",
    "                                    \"height_range\":2})\n",
    "ll_ez1.type_of_blob = \"TRACKMATE\"\n",
    "ll_ez1.run_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_ez._make_SMAUG_files()\n",
    "ll_ez._make_NOBIAS_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ID = '2'\n",
    "cell_ID = ['1']\n",
    "x = []\n",
    "y = []\n",
    "for k in cell_ID:\n",
    "    arr_1 = np.array(ll_ez.Movie[movie_ID].Cells[k].raw_tracks)\n",
    "    x+=list(arr_1[:,2])\n",
    "    y+=list(arr_1[:,3])\n",
    "\n",
    "#plot the x,y coordinates\n",
    "plt.scatter(x,y,s= 1)\n",
    "#perform clustering on thisnusing DBSCAN\n",
    "clustering = DBSCAN(eps=0.4, min_samples=3).fit(np.array([[a,b] for a,b in zip(x,y)]))\n",
    "#plot the clustering result\n",
    "a = plt.scatter(x,y,s= 2,c = clustering.labels_)\n",
    "plt.colorbar(a)\n",
    "\n",
    "\n",
    "#make the two axis equal\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.tight_layout()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "#find the area of each cluster using the convex hull\n",
    "from scipy.spatial import ConvexHull\n",
    "#find the unique labels\n",
    "unique_labels = set(clustering.labels_)\n",
    "#find the convex hull for each cluster\n",
    "for k in unique_labels:\n",
    "    if k == -1:\n",
    "        continue\n",
    "    #find the points in the cluster\n",
    "    points = np.array([[a,b] for a,b,c in zip(x,y,clustering.labels_) if c == k])\n",
    "    #find the convex hull\n",
    "    hull = ConvexHull(points)\n",
    "    #plot the convex hull\n",
    "    for simplex in hull.simplices:\n",
    "        plt.plot(points[simplex, 0], points[simplex, 1], 'r-')\n",
    "    area = hull.area\n",
    "    #plot the area\n",
    "    plt.text(np.mean(points[:,0]),np.mean(points[:,1]),\"{0:.2f}\".format(area),fontsize=20,color='r')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Find the density of the points using 2d histogram\n",
    "hist, xedges, yedges = np.histogram2d(x, y, bins=100)\n",
    "extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "a = plt.imshow(hist.T, extent=extent, origin='lower')\n",
    "#make the two axis equal\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.tight_layout()\n",
    "plt.gca().invert_yaxis()\n",
    "#make a colorbar title\n",
    "plt.colorbar(a).set_label('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_ez1= run_analysis(\"/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/20200215/ll_m9\",\"ll_m9\")\n",
    "ll_ez1.read_parameters(minimum_percent_per_drop_in = 0.9, \n",
    "                    t_len_u = 100, \n",
    "                    t_len_l=10, \n",
    "                    minimum_tracks_per_drop = 3)\n",
    "\n",
    "ll_ez1.get_blob_parameters(threshold=1e-4,\n",
    "                        overlap=0,\n",
    "                        detection_name='bp',\n",
    "                        min_sigma=1/np.sqrt(2),\n",
    "                        max_sigma=6/np.sqrt(2),\n",
    "                        num_sigma=500,median = False)\n",
    "\n",
    "ll_ez1.get_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                    \"plot_fit\":False,\n",
    "                                    \"fitting_image\":\"Original\",\n",
    "                                    \"radius_func\":identity,\n",
    "                                    \"residual_func\":residuals_gaus2d,\n",
    "                                    \"sigma_range\":2,\n",
    "                                    \"centroid_range\":0.5,\n",
    "                                    \"height_range\":2})\n",
    "ll_ez1.run_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_ez2= run_analysis(\"/Users/baljyot/Documents/CODE/GitHub_t2/Baljyot_EXP_RPOC/DATA/17/ll_ez\",\"ll_ez\")\n",
    "ll_ez2.read_parameters(minimum_percent_per_drop_in = 0.9, \n",
    "                    t_len_u = 100, \n",
    "                    t_len_l=10, \n",
    "                    minimum_tracks_per_drop = 3)\n",
    "\n",
    "ll_ez2.get_blob_parameters(threshold=1e-3,\n",
    "                        overlap=0,\n",
    "                        detection_name='bp',\n",
    "                        min_sigma=1/np.sqrt(2),\n",
    "                        max_sigma=3/np.sqrt(2),\n",
    "                        num_sigma=500,median = False)\n",
    "\n",
    "ll_ez2.get_fitting_parameters(kwargs={\"mask_size\":3,\n",
    "                                    \"plot_fit\":False,\n",
    "                                    \"fitting_image\":\"Original\",\n",
    "                                    \"radius_func\":identity,\n",
    "                                    \"residual_func\":residuals_gaus2d,\n",
    "                                    \"sigma_range\":2,\n",
    "                                    \"centroid_range\":0.5,\n",
    "                                    \"height_range\":2})\n",
    "ll_ez2.type_of_blob = \"Scale\"\n",
    "ll_ez2.run_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert tracks to dictionary\n",
    "track_dict = ll_ez._convert_to_track_dict_bulk()\n",
    "print(len(track_dict[\"ALL\"]))\n",
    "h=0.5\n",
    "a = msd_calc(track_dict[\"ALL\"],h=None,tau_lim=None,tick_space=10,msd_fit_lim=4)\n",
    "msds = np.array(list(a[\"track_diffusion\"].values()))\n",
    "#plot the log10 of the msds as a histogram\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist(np.log10(con_pix_si(msds,which = 'msd')),bins=7)\n",
    "ax.set_xlabel(\"log10(msd)\")\n",
    "ax.set_ylabel(\"count\")\n",
    "ax.set_title(\"log10(msd) histogram\")\n",
    "plt.show()\n",
    "print(\"mean msd: \",np.mean(con_pix_si(msds,which = 'msd')))\n",
    "\n",
    "#for a range of msd_fit_lim values, plot the alpha values \n",
    "msd_fit_lim_range = np.arange(3,40,1)\n",
    "alphas = []\n",
    "for msd_fit_lim in msd_fit_lim_range:\n",
    "    aa = msd_calc(track_dict[\"ALL\"],h=None,tau_lim=None,tick_space=3,msd_fit_lim=int(msd_fit_lim),plot=False)\n",
    "    alphas.append(aa[\"fit_ens\"][0][0])\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(msd_fit_lim_range,alphas)\n",
    "ax.set_xlabel(\"msd_fit_lim\")\n",
    "ax.set_ylabel(\"alpha\")\n",
    "ax.set_title(\"alpha vs msd_fit_lim\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#do a GMM fit for the track alphas \n",
    "gmm = mixture.GaussianMixture(n_components=2, covariance_type='full')\n",
    "alphas = np.array(list(a[\"track_alpha\"].values()))\n",
    "alphas = alphas.reshape(-1,1)\n",
    "gmm.fit(alphas)\n",
    "labels = gmm.predict(alphas)\n",
    "#plot the gaussian mixture model\n",
    "plt.clf()\n",
    "plt.hist(alphas[(alphas>-1)&(alphas<2)], 15, density=True, alpha=0.6, color='g')\n",
    "# x = np.linspace(-1., 1.5, 1000)\n",
    "# logprob = gmm.score_samples(x.reshape(-1, 1))\n",
    "# responsibilities = gmm.predict_proba(x.reshape(-1, 1))\n",
    "# pdf = np.exp(logprob)\n",
    "# pdf_individual = responsibilities * pdf[:, np.newaxis]\n",
    "# plt.plot(x, pdf, '-k')\n",
    "# plt.plot(x, pdf_individual, '--k')\n",
    "#annotate the plot with the means only \n",
    "plt.text(gmm.means_[1],0.1,\"gaussian 1: %.2f\"%gmm.means_[0])\n",
    "plt.text(gmm.means_[1],0.5,\"gaussian 2: %.2f\"%gmm.means_[1])\n",
    "plt.xlabel(\"Track Alpha\")\n",
    "plt.ylabel(\"Probability desity\")\n",
    "plt.show()\n",
    "\n",
    "print(gmm.means_)\n",
    "print(gmm.covariances_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = ll_ez.Movie['0'].Cells[\"0\"].All_Tracjectories['0,4'].Y\n",
    "x_test = ll_ez.Movie['0'].Cells[\"0\"].All_Tracjectories['0,4'].X\n",
    "msd = ll_ez.Movie['0'].Cells[\"0\"].All_Tracjectories['0,4'].MSD_total_um\n",
    "print(msd)\n",
    "print(con_pix_si(MSD_tavg1(x_test,y_test,1),which = 'msd'))\n",
    "\n",
    "def MSD_tavg111(x,y,f,f_inc = False):\n",
    "    \n",
    "    dists = np.zeros(len(x)-1)\n",
    "    for i in range(len(x)-1):\n",
    "        dists[i] = dist(x[i],y[i],x[i+1],y[i+1])\n",
    "    if f_inc == True:\n",
    "        return np.mean((dists/np.diff(f))**2)/4.\n",
    "    else:\n",
    "        return np.mean(dists**2)/4.\n",
    "    \n",
    "print(con_pix_si(MSD_tavg111(x_test,y_test,1,f_inc = False),which = 'msd'))\n",
    "\n",
    "#make a df for the tracks with each row being a track_ID and the columns being X,Y,Frames\n",
    "track_df = {}\n",
    "\n",
    "\n",
    "#make a df for the msds\n",
    "msd_df_ll = pd.DataFrame(columns=[\"MSD\"])\n",
    "msd_dif_new = pd.DataFrame(columns=[\"MSD\"])\n",
    "counter = 0\n",
    "for k,l in ll_ez.Movie.items():\n",
    "    for m,n in l.Cells.items():\n",
    "        for i,j in n.All_Tracjectories.items():\n",
    "            msd_df_ll.loc[i] = j.MSD_total_um\n",
    "            msd_dif_new.loc[i] = con_pix_si(MSD_tavg111(j.X,j.Y,1,f_inc = False),which = 'msd')\n",
    "            track_df[str(counter)] = [j.X,j.Y,j.Frames]\n",
    "            counter+=1\n",
    "print(len(x_test),len(y_test),msd)\n",
    "print(msd_df_ll.head())\n",
    "\n",
    "#create a df from the track_df dictionary with each row being a track of Track_ID = \"key\", and each column being X,Y,Frames\n",
    "track_dfs = pd.DataFrame.from_dict(track_df,orient='index',columns=[\"X\",\"Y\",\"Frames\"])\n",
    "print(track_dfs.head())\n",
    "print(len(track_dfs))\n",
    "\n",
    "#save the track_df as a csv in my downloads folder\n",
    "track_dfs.to_csv(\"/Users/baljyot/Desktop/track_df.csv\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#plot the MSDs of the tracks\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist(np.log10(msd_df_ll[\"MSD\"]),bins=14)\n",
    "ax.set_xlabel(\"log10(msd)\")\n",
    "ax.set_ylabel(\"count\")\n",
    "ax.set_title(\"log10(msd) histogram\")\n",
    "plt.show()\n",
    "\n",
    "#plot the MSDs of the tracks\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist(np.log10(msd_dif_new[\"MSD\"]),bins=14)\n",
    "ax.set_xlabel(\"log10(msd)\")\n",
    "ax.set_ylabel(\"count\")\n",
    "ax.set_title(\"log10(msd) histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_dfs.to_csv(\"track_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the histogram of msds\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,5))\n",
    "ax.hist(np.log10(msds_df[\"msds\"]),bins=14,label=\"Fixed\",alpha=0.5)\n",
    "ax.hist(np.log10(msd_df_ll[\"MSD\"]),bins=14,label=\"LACI/O\",alpha=0.5)\n",
    "#make the legend\n",
    "ax.legend()\n",
    "#put the x and y labels\n",
    "ax.set_xlabel(\"msds (um^2)\")\n",
    "ax.set_ylabel(\"count\")\n",
    "#make a title\n",
    "ax.set_title(\"Histogram of msds\")\n",
    "plt.show()\n",
    "\n",
    "#plot violin plots of the msds with y being msd and x being the method\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,5))\n",
    "ax.violinplot([np.log10(msds_df[\"msds\"]),np.log10(msd_df_ll[\"MSD\"])])\n",
    "#make the legend\n",
    "ax.set_xticks([1,2])\n",
    "ax.set_xticklabels([\"Fixed\",\"LACI/O\"])\n",
    "#put the x and y labels\n",
    "ax.set_ylabel(\"log10(msds) (um^2)\")\n",
    "#make a title\n",
    "ax.set_title(\"Violin plot of msds\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert tracks to dictionary\n",
    "track_dict = ll_ez1._convert_to_track_dict_bulk()\n",
    "h=0.5\n",
    "a = msd_calc(track_dict[\"IN\"],h=None,tau_lim=35,tick_space=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert tracks to dictionary\n",
    "track_dict = ll_ez2._convert_to_track_dict_bulk()\n",
    "h=0.5\n",
    "a = msd_calc(track_dict[\"OUT\"],h=None,tau_lim=50,tick_space=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msd2 = ll_ez2._bulk_msd_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_areas = []\n",
    "nuc_areas = []\n",
    "drop_per_cell_area = []\n",
    "point_density = []\n",
    "for i,j in ll_ez.Movie.items():\n",
    "    for k,l in j.Cells.items():\n",
    "        cell_areas.append(l.cell_area[0]*0.0169)\n",
    "        nuc_areas.append(l.nucleoid_area*0.0169)\n",
    "        points_in_cell = 0.0\n",
    "        # for m,n in l.points_per_frame.items():\n",
    "        #     points_in_cell+=len(n)\n",
    "        point_density.append(float(points_in_cell)/(l.cell_area[0]*0.0169))\n",
    "\n",
    "        drops_cell = []\n",
    "        for m,n in l.Drop_Collection.items():\n",
    "            drops_cell.append((np.pi*n[-1]**2)*0.0169)\n",
    "        drop_per_cell_area.append(np.array(drops_cell))\n",
    "        \n",
    "\n",
    "NC_ratio = np.array(nuc_areas)/np.array(cell_areas)\n",
    "def line_func(x,m,c):\n",
    "    return m*x + c\n",
    "#fit a line to NC_ratio = m*cell_area_sp + c only\n",
    "line_NC = curve_fit(line_func,cell_areas,NC_ratio,bounds = (0,[1e-3,np.inf]))\n",
    "line_cell_nuc = curve_fit(line_func,cell_areas,nuc_areas,bounds = (0,[np.inf,1e-3]))\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(np.array(cell_areas),np.array(nuc_areas))\n",
    "#plot the fit line using lower and upper bound of cell area\n",
    "ax.plot(np.linspace(min(cell_areas),max(cell_areas),100),line_func(np.linspace(min(cell_areas),max(cell_areas),100),*line_cell_nuc[0]),color=\"red\")\n",
    "#annotate the slope and intercept as y=mx+c\n",
    "ax.annotate(\"y = \"+str(round(line_cell_nuc[0][0],3))+\"x + \"+str(round(line_cell_nuc[0][1],3)),(0.5,0.4),xycoords=\"axes fraction\")\n",
    "ax.set_xlabel(\"Cell Area SuperSegger(um^2)\")\n",
    "ax.set_ylabel(\"Nucleoid Area (um^2)\")\n",
    "ax.set_title(\"Nucleoid Area vs Cell Area\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(np.array(cell_areas),NC_ratio)\n",
    "#plot the fit line using lower and upper bound of cell area\n",
    "ax.plot(np.linspace(min(cell_areas),max(cell_areas),100),line_func(np.linspace(min(cell_areas),max(cell_areas),100),*line_NC[0]),color=\"red\")\n",
    "#annotate the slope and intercept as y=mx+c\n",
    "ax.annotate(\"y = \"+str(round(line_NC[0][0],3))+\"x + \"+str(round(line_NC[0][1],3)),(0.5,0.5),xycoords=\"axes fraction\")\n",
    "ax.set_xlabel(\"Cell Area SuperSegger(um^2)\")\n",
    "ax.set_ylabel(\"Nucleoid Area/Cell Area\")\n",
    "ax.set_title(\"Nucleoid Area/Cell Area vs Cell Area\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "for i in range(len(cell_areas)):\n",
    "    ax.scatter([cell_areas[i]]*len(drop_per_cell_area[i]),drop_per_cell_area[i],color=\"blue\")\n",
    "ax.set_xlabel(\"Cell Area SuperSegger(um^2)\")\n",
    "ax.set_ylabel(\"Drop Area (um^2)\")\n",
    "ax.set_title(\"Drop Area vs Cell Area\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "for i in range(len(cell_areas)):\n",
    "    ax.scatter(cell_areas[i],np.mean(np.array(drop_per_cell_area[i])),color=\"blue\")\n",
    "ax.set_xlabel(\"Cell Area SuperSegger(um^2)\")\n",
    "ax.set_ylabel(\"Average Drop Area (um^2)\")\n",
    "ax.set_title(\"Average Drop Area vs Cell Area\")\n",
    "plt.show()\n",
    "\n",
    "mean_drop_per_cell_area = []\n",
    "for i in range(len(drop_per_cell_area)):\n",
    "    mean_drop_per_cell_area.append(np.mean(np.array(drop_per_cell_area[i])))\n",
    "DC_ratio = np.array(mean_drop_per_cell_area)/np.array(cell_areas)\n",
    "DN_ratio = np.array(mean_drop_per_cell_area)/np.array(nuc_areas)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(np.array(cell_areas),DC_ratio)\n",
    "ax.set_xlabel(\"Cell Area SuperSegger(um^2)\")\n",
    "ax.set_ylabel(\"DC Ratio\")\n",
    "ax.set_title(\"DC Ratio (Drop Area/Cell Area) \\nvs Cell Area\")\n",
    "ax.set_ylim(0,0.6)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#do the same but now using nuc_area\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "for i in range(len(nuc_areas)):\n",
    "    ax.scatter([nuc_areas[i]]*len(drop_per_cell_area[i]),drop_per_cell_area[i],color=\"blue\")\n",
    "ax.set_xlabel(\"Nucleoid Area (um^2)\")\n",
    "ax.set_ylabel(\"Drop Area (um^2)\")\n",
    "ax.set_title(\"Drop Area vs Nucleoid Area\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "for i in range(len(nuc_areas)):\n",
    "    ax.scatter(nuc_areas[i],np.mean(np.array(drop_per_cell_area[i])),color=\"blue\")\n",
    "ax.set_xlabel(\"Nucleoid Area (um^2)\")\n",
    "ax.set_ylabel(\"Average Drop Area (um^2)\")\n",
    "ax.set_title(\"Average Drop Area vs Nucleoid Area\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(np.array(nuc_areas),DN_ratio)\n",
    "ax.set_xlabel(\"Nucleoid Area (um^2)\")\n",
    "ax.set_ylabel(\"DN Ratio\")\n",
    "ax.set_title(\"DN Ratio (Drop Area/Nucleoid Area) \\nvs Nucleoid Area\")\n",
    "ax.set_ylim(0,0.6)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(np.array(cell_areas),np.array(point_density))\n",
    "ax.set_xlabel(\"Cell Area SuperSegger(um^2)\")\n",
    "ax.set_ylabel(\"Point Density\")\n",
    "ax.set_title(\"Point Density vs Cell Area\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "num_drops = []\n",
    "for i in range(len(drop_per_cell_area)):\n",
    "    num_drops.append(len(drop_per_cell_area[i]))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(np.array(cell_areas),np.array(num_drops))\n",
    "ax.set_xlabel(\"Cell Area SuperSegger(um^2)\")\n",
    "ax.set_ylabel(\"Number of Drops\")\n",
    "ax.set_title(\"Number of Drops vs Cell Area\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(np.array(nuc_areas),np.array(num_drops))\n",
    "ax.set_xlabel(\"Nucleoid Area (um^2)\")\n",
    "ax.set_ylabel(\"Number of Drops\")\n",
    "ax.set_title(\"Number of Drops vs Nucleoid Area\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(np.array(NC_ratio),np.array(DN_ratio))\n",
    "ax.set_xlabel(\"NC Ratio\")\n",
    "ax.set_ylabel(\"DN Ratio\")\n",
    "ax.set_title(\"DN Ratio vs NC Ratio\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(np.array(NC_ratio),np.array(DC_ratio))\n",
    "ax.set_xlabel(\"NC Ratio\")\n",
    "ax.set_ylabel(\"DC Ratio\")\n",
    "ax.set_title(\"DC Ratio vs NC Ratio\")\n",
    "plt.show()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_thresh = 0.5\n",
    "\n",
    "drop_localizations,total_localizations,total_clusters,total_cell_locs,area_cell,area_drop,area_drop_cell_diff,total_tracks,drop_tracks,total_track_counts = _avg_points_drop(ll_ez)\n",
    "print(area_cell)\n",
    "print(area_drop)\n",
    "#print the average area of a cell\n",
    "print(\"Mean cell area: {}\".format(np.mean(list(area_cell.values()))))\n",
    "print(\"Mean drop area: {}\".format(np.mean(list(area_drop.values()))))\n",
    "#plot the area per cell\n",
    "plt.clf()\n",
    "plt.hist(np.array(list(area_cell.values())),bins=5)\n",
    "plt.xlabel(\"Cell Area in um^2\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Cell Area Distribution\")\n",
    "plt.show()\n",
    "\n",
    "#plot the area per drop\n",
    "plt.clf()\n",
    "plt.hist(np.array(list(area_drop.values())),bins=5)\n",
    "plt.xlabel(\"Drop Area in um^2\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Drop Area Distribution\")\n",
    "plt.show()\n",
    "\n",
    "#plot the density per drop and per cell on the same plot as a histogram\n",
    "plt.clf()\n",
    "plt.hist(np.array(list(drop_localizations.values()))/np.array(list(area_drop.values()))/1000,bins=20,alpha=0.5,label=\"Drop\")\n",
    "plt.hist(np.array(list(total_localizations.values()))/np.array(list(area_drop_cell_diff.values()))/1000,bins=20,alpha=0.5,label=\"Cell\")\n",
    "plt.xlabel(\"Point Density in um^-2\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.title(\"Points Per Area (Density) Distribution\")\n",
    "plt.show()\n",
    "\n",
    "track_density_drop = np.array(list(drop_tracks.values()))/np.array(list(area_drop.values()))/1000\n",
    "track_density_cell = np.array(list(total_tracks.values()))/np.array(list(area_drop_cell_diff.values()))/1000\n",
    "track_drop_cell_density_ratio = track_density_drop/track_density_cell\n",
    "track_drop_cell_density_ratio_ll = track_drop_cell_density_ratio\n",
    "\n",
    "#plot the track_drop_cell_density_ratio as a function of area_drop_cell_diff\n",
    "plt.clf()\n",
    "plt.scatter(np.array(list(area_drop_cell_diff.values())),track_drop_cell_density_ratio)\n",
    "plt.xlabel(\"Cell Area in um^2\")\n",
    "plt.ylabel(\"Track Density Ratio\")\n",
    "plt.title(\"Track Density Ratio vs Cell Area\")\n",
    "plt.show()\n",
    "\n",
    "#plot it as a function of area_drop\n",
    "plt.clf()\n",
    "plt.scatter(np.array(list(area_drop.values())),track_drop_cell_density_ratio)\n",
    "plt.xlabel(\"Drop Area in um^2\")\n",
    "plt.ylabel(\"Track Density Ratio\")\n",
    "plt.title(\"Track Density Ratio vs Drop Area\")\n",
    "plt.show()\n",
    "\n",
    "#plot it as a function of track_density_cell\n",
    "plt.clf()\n",
    "plt.scatter(track_density_cell,track_drop_cell_density_ratio)\n",
    "plt.xlabel(\"Cell Track Density in um^-2\")\n",
    "plt.ylabel(\"Track Density Ratio\")\n",
    "plt.title(\"Track Density Ratio vs Cell Track Density\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#and again as a function of track_density_drop\n",
    "plt.clf()\n",
    "plt.scatter(track_density_drop,track_drop_cell_density_ratio)\n",
    "plt.xlabel(\"Drop Track Density in um^-2\")\n",
    "plt.ylabel(\"Track Density Ratio\")\n",
    "plt.title(\"Track Density Ratio vs Drop Track Density\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#plot the track_density_drop as a function of area_drop\n",
    "plt.clf()\n",
    "plt.scatter(np.array(list(area_drop.values())),track_density_drop)\n",
    "plt.xlabel(\"Drop Area in um^2\")\n",
    "plt.ylabel(\"Track Density in um^-2\")\n",
    "plt.title(\"Track Density vs Drop Area\")\n",
    "plt.show()\n",
    "\n",
    "#plot the track_density_cell as a function of area_drop_cell_diff\n",
    "plt.clf()\n",
    "plt.scatter(np.array(list(area_drop_cell_diff.values())),track_density_cell)\n",
    "plt.xlabel(\"Cell Area in um^2\")\n",
    "plt.ylabel(\"Track Density in um^-2\")\n",
    "plt.title(\"Track Density vs Cell Area\")\n",
    "plt.show()\n",
    "\n",
    "#replot both on the same plot, also in a zoomed inset plot the plot the track_density_drop as a function of area_drop\n",
    "plt.clf()\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(np.array(list(area_drop.values())),track_density_drop,label=\"Drop\")\n",
    "ax.scatter(np.array(list(area_drop_cell_diff.values())),track_density_cell,label=\"Cell\")\n",
    "ax.set_xlabel(\"Area in um^2\")\n",
    "ax.set_ylabel(\"Track Density in um^-2\")\n",
    "ax.set_title(\"Track Density vs Area\")\n",
    "ax.legend()\n",
    "#create gridpoints for the inset plot with numbers in the range of the data\n",
    "x = np.linspace(np.min(np.array(list(area_drop.values()))),np.max(np.array(list(area_drop.values()))),100)\n",
    "y = np.linspace(np.min(track_density_drop),np.max(track_density_drop),100)\n",
    "#plot the inset plot\n",
    "axins = ax.inset_axes([0.1,0.5,0.4,0.4])\n",
    "axins.scatter(np.array(list(area_drop.values())),track_density_drop)\n",
    "axins.set_xlim(np.min(np.array(list(area_drop.values())))-1,np.max(np.array(list(area_drop.values())))+1)\n",
    "axins.set_ylim(np.min(track_density_drop)-1,np.max(track_density_drop)+1)\n",
    "#format the ticklabels to be less ugly\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "axins.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "axins.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "#plot the gridpoints\n",
    "axins.plot(x,y)\n",
    "#plot the lines that indicate the zoomed area\n",
    "ax.indicate_inset_zoom(axins)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "density_drop = np.array(list(drop_localizations.values()))/np.array(list(area_drop.values()))/1000\n",
    "density_cell = np.array(list(total_localizations.values()))/np.array(list(area_drop_cell_diff.values()))/1000\n",
    "drop_cell_density_ratio = density_drop/density_cell\n",
    "drop_area = np.array(list(area_drop.values()))\n",
    "\n",
    "unique_drop_area = {}\n",
    "unique_density_ratio_area = {}\n",
    "unique_track_density_ratio_area = {}\n",
    "unique_track_density_area = {}\n",
    "for i in range(len(drop_area)):\n",
    "    if str(drop_area[i]) not in unique_drop_area:\n",
    "        unique_drop_area[str(drop_area[i])] = []\n",
    "    else:\n",
    "        unique_drop_area[str(drop_area[i])].append(density_drop[i])\n",
    "    \n",
    "    if str(drop_area[i]) not in unique_density_ratio_area:\n",
    "        unique_density_ratio_area[str(drop_area[i])] = []\n",
    "    else:\n",
    "        unique_density_ratio_area[str(drop_area[i])].append(drop_cell_density_ratio[i])\n",
    "    \n",
    "    if str(drop_area[i]) not in unique_track_density_ratio_area:\n",
    "        unique_track_density_ratio_area[str(drop_area[i])] = []\n",
    "    else:\n",
    "        unique_track_density_ratio_area[str(drop_area[i])].append(track_drop_cell_density_ratio[i])\n",
    "    \n",
    "    if str(drop_area[i]) not in unique_track_density_area:\n",
    "        unique_track_density_area[str(drop_area[i])] = []\n",
    "    else:\n",
    "        unique_track_density_area[str(drop_area[i])].append(track_density_drop[i])\n",
    "\n",
    "#for each element in unique drop area find the mean of its values\n",
    "unique_drop_area_list = []\n",
    "unique_drop_area_mean = []\n",
    "unique_drop_area_SE = []\n",
    "#do the same for the density ratio\n",
    "unique_density_ratio_area_mean = []\n",
    "unique_density_ratio_area_SE = []\n",
    "#do the same for the track density ratio\n",
    "unique_track_density_ratio_area_mean = []\n",
    "unique_track_density_ratio_area_SE = []\n",
    "#do the same for the track density\n",
    "unique_track_density_area_mean = []\n",
    "unique_track_density_area_SE = []\n",
    "\n",
    "\n",
    "for i,j in unique_track_density_ratio_area.items():\n",
    "    unique_track_density_ratio_area_mean.append(np.mean(j))\n",
    "    unique_track_density_ratio_area_SE.append(np.std(j)/np.sqrt(len(j)))\n",
    "\n",
    "\n",
    "for i,j in unique_density_ratio_area.items():\n",
    "    unique_density_ratio_area_mean.append(np.mean(j))\n",
    "    unique_density_ratio_area_SE.append(np.std(j)/np.sqrt(len(j)))\n",
    "\n",
    "\n",
    "for i,j in unique_drop_area.items():\n",
    "    unique_drop_area_list.append(float(i))\n",
    "    unique_drop_area_mean.append(np.mean(j))\n",
    "    unique_drop_area_SE.append(np.std(j)/np.sqrt(len(j)))\n",
    "\n",
    "for i,j in unique_track_density_area.items():\n",
    "    unique_track_density_area_mean.append(np.mean(j))\n",
    "    unique_track_density_area_SE.append(np.std(j)/np.sqrt(len(j)))\n",
    "\n",
    "plt.clf()\n",
    "plt.errorbar(unique_drop_area_list,unique_track_density_area_mean,yerr=unique_track_density_area_SE,fmt='o')\n",
    "plt.xlabel(\"Drop Area in um^2\")\n",
    "plt.ylabel(\"Track Density (Molecules) in um^-2\")\n",
    "plt.title(\"Track Density (Molecules) vs Drop Area\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.errorbar(unique_drop_area_list,unique_drop_area_mean,yerr=unique_drop_area_SE,fmt='o')\n",
    "plt.xlabel(\"Drop Area in um^2\")\n",
    "plt.ylabel(\"Point Density in um^-2\")\n",
    "plt.title(\"Density vs Drop Area\")\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.errorbar(unique_drop_area_list,unique_density_ratio_area_mean,yerr=unique_density_ratio_area_SE,fmt='o')\n",
    "plt.xlabel(\"Drop Area in um^2\")\n",
    "plt.ylabel(\"Drop/Cell Density Ratio\")\n",
    "plt.title(\"Drop/Cell Density Ratio vs Drop Area\")\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.errorbar(unique_drop_area_list,unique_track_density_ratio_area_mean,yerr=unique_track_density_ratio_area_SE,fmt='o')\n",
    "plt.xlabel(\"Drop Area in um^2\")\n",
    "plt.ylabel(\"Drop/Cell Track Density Ratio\")\n",
    "plt.title(\"Drop/Cell Track Density Ratio vs Drop Area\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.scatter(drop_area,density_drop)\n",
    "plt.xlabel(\"Drop Area in um^2\")\n",
    "plt.ylabel(\"Point Density in um^-2\")\n",
    "plt.title(\"Density vs Drop Area\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(drop_area,np.array(list(drop_localizations.values())))\n",
    "plt.xlabel(\"Drop Area in um^2\")\n",
    "plt.ylabel(\"Number of Localizations in Drop\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#print the average density per drop\n",
    "print(\"Mean density per drop: {}\".format(np.mean(np.array(list(drop_localizations.values()))/np.array(list(area_drop.values())))))\n",
    "#print the average density per cell\n",
    "print(\"Mean density per cell: {}\".format(np.mean(np.array(list(total_localizations.values()))/np.array(list(area_drop_cell_diff.values())))))\n",
    "\n",
    "\n",
    "print(drop_localizations)\n",
    "print(total_localizations)\n",
    "print(total_clusters)\n",
    "print(total_cell_locs)\n",
    "avg_drop_localizations = np.mean(list(drop_localizations.values()))\n",
    "std_drop_localizations = np.std(list(drop_localizations.values()))\n",
    "print(avg_drop_localizations,std_drop_localizations,np.mean(list(total_localizations.values())),np.std(list(total_localizations.values())))  \n",
    "print(\"Mean number of clusters per cell: {}\".format(np.mean(list(total_clusters.values()))))   \n",
    "#plot the distribution of localizations per drop\n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.hist(list(total_clusters.values()),alpha=0.5,label=\"Viable Drops per cell\",bins=5)\n",
    "ax.set_xlabel(\"Number of viable drops per cell\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "#add a annotation with the mean and std\n",
    "ax.annotate(\"Mean: {0:.2f}\\nStd: {1:.2f}\".format(np.mean(list(total_clusters.values())),np.std(list(total_clusters.values()))),xy=(0.5,0.5),xycoords=\"axes fraction\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "#plot the distribution of localizations per drop\n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.hist(list(drop_localizations.values()),alpha=0.5,label=\"# localizations per drop\",bins=5)\n",
    "ax.set_xlabel(\"Number of localizations per drop\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "#plot the total_localizations per cell \n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.hist(list(total_cell_locs.values()),alpha=0.5,label=\"# localizations per cell\",bins=5)\n",
    "ax.set_xlabel(\"Number of localizations per cell\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "viable_ll=get_radius(ll_ez,bins = 50)\n",
    "print(viable_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ID = '2'\n",
    "cell_ID = ['0']\n",
    "x = []\n",
    "y = []\n",
    "for k in cell_ID:\n",
    "    arr_1 = np.array(ll_ez.Movie[movie_ID].Cells[k].raw_tracks)\n",
    "    x+=list(arr_1[:,2])\n",
    "    y+=list(arr_1[:,3])\n",
    "\n",
    "#plot the x,y coordinates\n",
    "plt.scatter(x,y,s= 1)\n",
    "#perform clustering on thisnusing DBSCAN\n",
    "clustering = DBSCAN(eps=0.4, min_samples=30).fit(np.array([[a,b] for a,b in zip(x,y)]))\n",
    "#plot the clustering result\n",
    "a = plt.scatter(x,y,s= 2,c = clustering.labels_)\n",
    "plt.colorbar(a)\n",
    "\n",
    "\n",
    "#make the two axis equal\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.tight_layout()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "#find the area of each cluster using the convex hull\n",
    "from scipy.spatial import ConvexHull\n",
    "#find the unique labels\n",
    "unique_labels = set(clustering.labels_)\n",
    "#find the convex hull for each cluster\n",
    "for k in unique_labels:\n",
    "    if k == -1:\n",
    "        continue\n",
    "    #find the points in the cluster\n",
    "    points = np.array([[a,b] for a,b,c in zip(x,y,clustering.labels_) if c == k])\n",
    "    #find the convex hull\n",
    "    hull = ConvexHull(points)\n",
    "    #plot the convex hull\n",
    "    for simplex in hull.simplices:\n",
    "        plt.plot(points[simplex, 0], points[simplex, 1], 'r-')\n",
    "    area = hull.area\n",
    "    #plot the area\n",
    "    plt.text(np.mean(points[:,0]),np.mean(points[:,1]),\"{0:.2f}\".format(area),fontsize=20,color='r')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Find the density of the points using 2d histogram\n",
    "hist, xedges, yedges = np.histogram2d(x, y, bins=40)\n",
    "extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "a = plt.imshow(hist.T, extent=extent, origin='lower')\n",
    "#make the two axis equal\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.tight_layout()\n",
    "plt.gca().invert_yaxis()\n",
    "#make a colorbar title\n",
    "plt.colorbar(a).set_label('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import filters\n",
    "#fig,ax = run_analysis_plotting.plot_img(rp_ez,1,movie_ID,1,int(\"0,2\"[0]))\n",
    "a =ll_ez._get_movie_path('0',1)\n",
    "b = blob_detection(path = a,\\\n",
    "                    median= False,\\\n",
    "                    threshold= 1e-4, \\\n",
    "                    min_sigma= 1, \\\n",
    "                    max_sigma = 3.5, \\\n",
    "                    num_sigma= 500, \\\n",
    "                    overlap = 0)\n",
    "c = b.detection(type = 'bp')\n",
    "aa = read_file(a)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "#ax.imshow(filters.median_filter(aa,size =1),cmap = 'Greys')\n",
    "ax.imshow(aa,cmap = 'Greys')\n",
    "# for i in rp_ez.Movie['1'].Cells['7'].All_Drop_Collection.values():\n",
    "#     cir = plt.Circle((i[0],i[1]),radius=i[2], fill = False)\n",
    "#     ax.add_artist(cir)\n",
    "for i in c:\n",
    "    cir = plt.Circle((i[1],i[0]),radius=i[2], fill = False)\n",
    "    ax.add_artist(cir)\n",
    "plt.show()\n",
    "print(ll_ez.Movie['0'].Cells['0'].All_Drop_Collection)\n",
    "print(ll_ez.Movie['0'].Cells['0'].All_Drop_Verbose)\n",
    "print(report_fit(ll_ez.Movie['1'].Cells['1'].All_Drop_Verbose['0,0'][\"Fit\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ID = '5'\n",
    "a = ll_ez.Movie[movie_ID].Movie_nucleoid\n",
    "b = blob_detection(path = a,\\\n",
    "                    median= False,\\\n",
    "                    threshold= 0.015, \\\n",
    "                    min_sigma= 1, \\\n",
    "                    max_sigma = 2, \\\n",
    "                    num_sigma= 200, \\\n",
    "                    overlap = 0)\n",
    "c = b.detection()\n",
    "aa = read_file(a)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.imshow(aa,cmap = 'Greys')\n",
    "for i in c:\n",
    "    cir = plt.Circle((i[1],i[0]),radius=i[2], fill = False)\n",
    "    ax.add_artist(cir)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ll_ez_ba = boundary_analysis(dataset = ll_ez.Movie)\n",
    "ll_ez_aa = ll_ez_ba.directional_displacement_bulk(IN = True,IO = True, OT = True)\n",
    "b = ll_ez_ba.plot_directional_displacements(dir_displacements = ll_ez_aa[0],dist_center = ll_ez_aa[1],angles = ll_ez_aa[2])\n",
    "\n",
    "adjust_axis(b)\n",
    "\n",
    "# radius = []\n",
    "# for i,j in ll_ez.Movie.items():\n",
    "#     for k,l in j.Cells.items():\n",
    "#         for m,n in l.Drop_Collection.items():\n",
    "#             radius.append(n[2])\n",
    "# radius_ll_ez = np.array(radius)*0.13\n",
    "# plt.hist(radius_ll_ez,alpha = 0.1,label = \"viable\")\n",
    "# plt.hist(np.array(ll_ez.radius)[:,2]*0.13,alpha = 0.1, label = \"All\")\n",
    "# plt.xlabel(\"Radius in um\")\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.hist(radius_ll_ez,alpha = 0.1,label = \"ll\")\n",
    "# plt.hist(radius_rp_ez,alpha = 0.1,label = \"rp\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# plt.plot(ll_ez_aa[0],ll_ez_aa[2],'b.')\n",
    "# plt.show()\n",
    "print(np.sum(np.asarray(rp_ez_aa[1])>5))\n",
    "a = plot_pairCorrelation(ll_ez_aa, dr = 0.05, rMax = 1.)\n",
    "b = plot_pairCorrelation(rp_ez_aa, ax = a[4], dr = 0.05, rMax = 1.)\n",
    "plt.show()\n",
    "plt.plot(a[1],a[0],label = \"ll_ez\")\n",
    "plt.plot(b[1],b[0],label = \"rp_ez\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"radius\")\n",
    "plt.ylabel(\"g(r)\")\n",
    "plt.hlines(y = 1, xmin = 0, xmax = 1.5)\n",
    "\n",
    "\n",
    "rMax = 1\n",
    "dr = 0.05\n",
    "edges = np.arange(0,rMax,dr)\n",
    "resl,binsl = np.histogram(ll_ez_aa[1],bins = edges)\n",
    "resr,binsr = np.histogram(rp_ez_aa[1],bins = edges)\n",
    "num_radiusl = len(np.where(np.asarray(ll_ez_aa[1]) <= rMax)[0])\n",
    "num_radiusr = len(np.where(np.asarray(rp_ez_aa[1]) <= rMax)[0])\n",
    "space_density = np.zeros(len(edges)-1)\n",
    "radius = np.zeros(len(edges)-1)\n",
    "for i in range(len(edges)-1):\n",
    "    space_density[i] = np.pi*(edges[i+1]**2 - edges[i]**2)\n",
    "    radius[i] = (edges[i+1] - edges[i])/2\n",
    "# plt.clf()\n",
    "# plt.plot(binsl[:-1],resl)\n",
    "# plt.plot(binsr[:-1],resr)\n",
    "# plt.show()\n",
    "# plt.plot(radius,((resl/num_radiusl)/(np.pi*rMax*rMax)) / space_density)\n",
    "# plt.plot(radius,((resr/num_radiusr)/(np.pi*rMax*rMax)) / space_density)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which = ll_ez\n",
    "y_collection = []\n",
    "x_collection = []\n",
    "in_msd = []\n",
    "io_msd = []\n",
    "radius_col = []\n",
    "\n",
    "#cm of track to boundary vs diff\n",
    "cm_boundary = []\n",
    "cm_diff = []\n",
    "track_recidency_in_drop = []\n",
    "cm_error = []\n",
    "end_to_end = []\n",
    "radius_gyration = []\n",
    "\n",
    "IO_inside_start = []\n",
    "IO_inside_start_dist = []\n",
    "IO_outside_start = []\n",
    "IO_outside_start_dist = []\n",
    "#number of tracks that start inside and end inside\n",
    "tracks_in_in = 0\n",
    "len_ii = []\n",
    "tracks_out_out = 0\n",
    "len_oo = []\n",
    "tracks_in_out = 0\n",
    "len_io = []\n",
    "tracks_out_in = 0\n",
    "len_oi = []\n",
    "directional_displacement = []\n",
    "dist_center = []\n",
    "long_axis_angle = []\n",
    "\n",
    "#take notice of tracks which have displacements away from the condensate (in/out only) of >0.2 um\n",
    "track_xy = []\n",
    "track_drop = []\n",
    "track_movie = []\n",
    "track_cell = []\n",
    "track_cell_e1_e2 = []\n",
    "displacement_aligned_long = []\n",
    "track_drop_loc = []\n",
    "track_id = []\n",
    "angles = []\n",
    "for k,v in which.Movie.items():\n",
    "   for o,oo in which.Movie[k].Cells.items():\n",
    "      for kk,vv in which.Movie[k].Cells[o].Trajectory_Collection.items():\n",
    "          \n",
    "          for kkk,vvv in which.Movie[k].Cells[o].Trajectory_Collection[kk].IN_Trajectory_Collection.items():\n",
    "              track = which.Movie[k].Cells[o].Trajectory_Collection[kk].IN_Trajectory_Collection[kkk]\n",
    "              x_val = track.X\n",
    "              y_val = track.Y\n",
    "              drop_data = which.Movie[k].Cells[o].Drop_Collection[track.Drop_Identifier]\n",
    "\n",
    "              diff_dist_temp = con_pix_si(dif_dis(x_val,y_val),which = 'um')\n",
    "              drop_center_dist = (dist(x_val,y_val,drop_data[0],drop_data[1]))-drop_data[2]\n",
    "              angles += list(angle_dist(x_val,y_val,drop_data[0],drop_data[1]))\n",
    "\n",
    "              #direction of the trajectory\n",
    "              #r2 -r1 > 0 moving out, r2 - r1 < 0 moving in\n",
    "              directional = con_pix_si(np.diff(dist(x_val,y_val,drop_data[0],drop_data[1])),which = 'um')\n",
    "              directional_displacement+=list(directional)\n",
    "              dist_center+=list(drop_center_dist)[:-1]\n",
    "\n",
    "              radius_col.append(drop_data[2])\n",
    "              y_collection+=list(diff_dist_temp)\n",
    "              x_collection+=list(con_pix_si(drop_center_dist[:-1], which = 'um'))\n",
    "\n",
    "              in_msd.append(track.MSD_total_um)\n",
    "\n",
    "              #center of mass of track relative to boundary vs. diffusion of track\n",
    "              cm = cm_normal(x_val,y_val)\n",
    "              cm_dist_boundary = dist(cm[0],cm[1],drop_data[0],drop_data[1]) - drop_data[2]\n",
    "              cm_boundary.append(con_pix_si(cm_dist_boundary,which = 'um'))\n",
    "              cm_diff.append(track.MSD_total_um)\n",
    "              cm_error.append(np.sqrt(np.std(x_val)**2 + np.std(y_val)**2)/np.sqrt(len(x_val)))\n",
    "              track_recidency_in_drop.append(np.sum(drop_center_dist<0.0)/len(x_val))\n",
    "              #end ot end distance of trajectory:\n",
    "              end_to_end.append(end_distance(x_val,y_val))\n",
    "\n",
    "              #radius of gyration\n",
    "              radius_gyration.append(radius_of_gyration(x_val,y_val))\n",
    "\n",
    "\n",
    "\n",
    "              #how aligned is the displacement vector for each displacement to each axis of the cell. \n",
    "              #differences in x,y\n",
    "              dif_x = np.diff(x_val)\n",
    "              dif_y = np.diff(y_val)\n",
    "              long_axis_vec = which.Movie[k].Cells[o].cell_long_axis\n",
    "              angle_xy = []\n",
    "              for i in range(len(dif_x)):\n",
    "                  termer = np.arccos(np.dot(long_axis_vec.T[0],[dif_x[i],dif_y[i]])/(np.linalg.norm(long_axis_vec.T[0])*np.linalg.norm([dif_x[i],dif_y[i]])))*180/np.pi\n",
    "                  angle_xy.append(termer)\n",
    "              long_axis_angle+=angle_xy\n",
    "\n",
    "\n",
    "          for kkk,vvv in which.Movie[k].Cells[o].Trajectory_Collection[kk].IO_Trajectory_Collection.items():\n",
    "              track = which.Movie[k].Cells[o].Trajectory_Collection[kk].IO_Trajectory_Collection[kkk]\n",
    "              x_val = track.X\n",
    "              y_val = track.Y\n",
    "              drop_data = which.Movie[k].Cells[o].Drop_Collection[track.Drop_Identifier]\n",
    "                  \n",
    "              diff_dist_temp = con_pix_si(dif_dis(x_val,y_val),which = 'um')\n",
    "              drop_center_dist = (dist(x_val,y_val,drop_data[0],drop_data[1]) - drop_data[2])/drop_data[2]\n",
    "              angles += list(angle_dist(x_val,y_val,drop_data[0],drop_data[1])[:-1])\n",
    "              #direction of the trajectory\n",
    "              #r2 -r1 > 0 moving out, r2 - r1 < 0 moving in\n",
    "              directional = con_pix_si(np.diff(dist(x_val,y_val,drop_data[0],drop_data[1])),which = 'um')\n",
    "              directional_displacement+=list(directional)\n",
    "              dist_center+=list(drop_center_dist[:-1])\n",
    "\n",
    "              radius_col.append(drop_data[2])\n",
    "              y_collection+=list(diff_dist_temp)\n",
    "              x_collection+=list(con_pix_si(drop_center_dist[:-1], which = 'um'))\n",
    "              io_msd.append(track.MSD_total_um)\n",
    "\n",
    "              #center of mass of track relative to boundary vs. diffusion of track\n",
    "              cm = cm_normal(x_val,y_val)\n",
    "              cm_dist_boundary = dist(cm[0],cm[1],drop_data[0],drop_data[1]) - drop_data[2]\n",
    "              cm_boundary.append(con_pix_si(cm_dist_boundary,which = 'um'))\n",
    "              cm_diff.append(track.MSD_total_um)\n",
    "              cm_error.append(np.sqrt(np.std(x_val)**2 + np.std(y_val)**2))\n",
    "              track_recidency_in_drop.append(np.sum(drop_center_dist<0.0)/len(x_val))\n",
    "              #end ot end distance of trajectory:\n",
    "              end_to_end.append(end_distance(x_val,y_val))\n",
    "\n",
    "              #radius of gyration\n",
    "              radius_gyration.append(radius_of_gyration(x_val,y_val))\n",
    "\n",
    "              #how aligned is the displacement vector for each displacement to each axis of the cell. \n",
    "              #differences in x,y\n",
    "              dif_x = np.diff(x_val)\n",
    "              dif_y = np.diff(y_val)\n",
    "              long_axis_vec = which.Movie[k].Cells[o].cell_long_axis\n",
    "              angle_xy = []\n",
    "              for i in range(len(dif_x)):\n",
    "                  termer = np.arccos(np.dot(long_axis_vec.T[0],[dif_x[i],dif_y[i]])/(np.linalg.norm(long_axis_vec.T[0])*np.linalg.norm([dif_x[i],dif_y[i]])))*180/np.pi\n",
    "                  angle_xy.append(termer)\n",
    "              long_axis_angle+=angle_xy\n",
    "              #check the tracks which have displacements way outside the condensate and ask how are they oriented relative to the cell axis and where the condensate is\n",
    "              if np.sum(np.array(con_pix_si(drop_center_dist[:-1], which = 'um'))>-0.4) != 0:\n",
    "                  track_xy.append([x_val,y_val])\n",
    "                  track_drop.append(which.Movie[k].Cells[o].Drop_Collection[track.Drop_Identifier])\n",
    "                  track_drop_loc.append(track.Drop_Identifier)\n",
    "                  track_id.append(kkk)\n",
    "                  track_movie.append(k)\n",
    "                  track_cell.append(o)\n",
    "                  track_cell_e1_e2.append([which.Movie[k].Cells[o].cell_long_axis,which.Movie[k].Cells[o].cell_short_axis])\n",
    "                  #how aligned is the displacement vector for each displacement to each axis of the cell. \n",
    "                  #differences in x,y\n",
    "                  dif_x = np.diff(x_val)\n",
    "                  dif_y = np.diff(y_val)\n",
    "                  long_axis_vec = which.Movie[k].Cells[o].cell_long_axis\n",
    "                  angle_xy = []\n",
    "                  for i in range(len(dif_x)):\n",
    "                      termer = np.arccos(np.dot(long_axis_vec.T[0],[dif_x[i],dif_y[i]])/(np.linalg.norm(long_axis_vec.T[0])*np.linalg.norm([dif_x[i],dif_y[i]])))*180/np.pi\n",
    "                      angle_xy.append(termer)\n",
    "\n",
    "\n",
    "\n",
    "              #for IO trajectories that start in the inside of condensates how do they behave?\n",
    "              distances_center = dist(x_val,y_val,drop_data[0],drop_data[1]) \n",
    "              index_radius = distances_center<drop_data[2]\n",
    "              # index_index = 0\n",
    "              # for i in range(len(index_radius)):\n",
    "              #     if i==0:\n",
    "              #         index_index = index_radius[i]\n",
    "              #     else:\n",
    "\n",
    "\n",
    "              if (index_radius[0] == True) and (index_radius[-1] == True):\n",
    "                  IO_inside_start+=list(diff_dist_temp)\n",
    "                  IO_inside_start_dist+=list(con_pix_si(drop_center_dist[:-1], which = 'um'))\n",
    "                  tracks_in_in+=1\n",
    "                  len_ii.append(len(index_radius))\n",
    "              elif (index_radius[0] == False) and (index_radius[-1] == False):\n",
    "                  IO_outside_start+=list(diff_dist_temp)\n",
    "                  IO_outside_start_dist+=list(con_pix_si(drop_center_dist[:-1], which = 'um'))\n",
    "                  tracks_out_out+=1\n",
    "                  len_oo.append(len(index_radius))\n",
    "              if (index_radius[0] == False) and (index_radius[-1] == True):\n",
    "                  tracks_out_in+=1\n",
    "                  len_oi.append(len(index_radius))\n",
    "              if (index_radius[0] == True) and (index_radius[-1] == False):\n",
    "                  tracks_in_out+=1\n",
    "                  len_io.append(len(index_radius))\n",
    "\n",
    "          # for kkk,vvv in which.Movie[k].Cells[o].Trajectory_Collection[kk].OUT_Trajectory_Collection.items():\n",
    "          #     track = which.Movie[k].Cells[o].Trajectory_Collection[kk].OUT_Trajectory_Collection[kkk]\n",
    "          #     x_val = track.X\n",
    "          #     y_val = track.Y\n",
    "          #     drop_data = which.Movie[k].Cells[o].Drop_Collection[track.Drop_Identifier]\n",
    "                  \n",
    "          #     diff_dist_temp = con_pix_si(dif_dis(x_val,y_val),which = 'um')\n",
    "          #     drop_center_dist = dist(x_val,y_val,drop_data[0],drop_data[1]) - drop_data[2]\n",
    "\n",
    "          #     #direction of the trajectory\n",
    "          #     #r2 -r1 > 0 moving out, r2 - r1 < 0 moving in\n",
    "          #     directional = con_pix_si(np.diff(dist(x_val,y_val,drop_data[0],drop_data[1])),which = 'um')\n",
    "          #     directional_displacement+=list(directional)\n",
    "          #     dist_center+=list(con_pix_si(drop_center_dist[:-1], which = 'um'))\n",
    "\n",
    "          #     radius_col.append(drop_data[2])\n",
    "          #     y_collection+=list(diff_dist_temp)\n",
    "          #     x_collection+=list(con_pix_si(drop_center_dist[:-1], which = 'um'))\n",
    "          #     io_msd.append(track.MSD_total_um)\n",
    "\n",
    "          #     #center of mass of track relative to boundary vs. diffusion of track\n",
    "          #     cm = cm_normal(x_val,y_val)\n",
    "          #     cm_dist_boundary = dist(cm[0],cm[1],drop_data[0],drop_data[1]) - drop_data[2]\n",
    "          #     cm_boundary.append(con_pix_si(cm_dist_boundary,which = 'um'))\n",
    "          #     cm_diff.append(track.MSD_total_um)\n",
    "          #     cm_error.append(np.sqrt(np.std(x_val)**2 + np.std(y_val)**2))\n",
    "          #     track_recidency_in_drop.append(np.sum(drop_center_dist<0.0)/len(x_val))\n",
    "          #     #end ot end distance of trajectory:\n",
    "          #     end_to_end.append(end_distance(x_val,y_val))\n",
    "\n",
    "          #     #radius of gyration\n",
    "          #     radius_gyration.append(radius_of_gyration(x_val,y_val))\n",
    "\n",
    "\n",
    "#plotting tracks on cells\n",
    "\n",
    "\n",
    "#get the '2' movie:\n",
    "movie_selc = '7'\n",
    "\n",
    "ind_m = np.array(track_movie) == movie_selc\n",
    "cells_m = np.array(track_cell)[ind_m]\n",
    "drops_m = np.array(track_drop)[ind_m]\n",
    "tracks_m = np.array(track_xy)[ind_m]\n",
    "drop_loc_m = np.array(track_drop_loc)[ind_m]\n",
    "track_idm = np.array(track_id)[ind_m]\n",
    "\n",
    "\n",
    "\n",
    "cmap_all=plt.get_cmap('gray')\n",
    "\n",
    "\n",
    "'''\n",
    "for i in range(len(tracks_m)):\n",
    "    img = mpimg.imread(which.Movie[movie_selc].Movie_location[int(drop_loc_m[i][0])])\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    pimg = ax.imshow(img,cmap=cmap_all)\n",
    "    ax.plot(tracks_m[i][0],tracks_m[i][1],'-') \n",
    "    cir = Circle([drops_m[i][0],drops_m[i][1]], radius =drops_m[i][2], fill = False, color = 'red')\n",
    "    ax.add_artist(cir)\n",
    "    for k,l in which.Movie[movie_selc].Cells[cells_m[i]].Drop_Collection.items():\n",
    "\n",
    "        print(drops_m[i])\n",
    "        if k[0] == track_idm[i][0]:\n",
    "            cir = Circle([l[0],l[1]], radius =l[2], fill = False, color = \"black\")\n",
    "            ax.add_artist(cir)\n",
    "    plt.xlim((50,120))\n",
    "    plt.ylim((180,240))\n",
    "    plt.show()\n",
    "'''\n",
    "for i in range(len(tracks_m)):\n",
    "    if i == 0:\n",
    "      img = mpimg.imread(which.Movie[movie_selc].Movie_location[int(drop_loc_m[i][0])])\n",
    "      fig = plt.figure()\n",
    "      ax = plt.axes(projection='3d')\n",
    "      nx,ny = np.shape(img)\n",
    "      x = range(nx)\n",
    "      y = range(ny)\n",
    "      X, Y = np.meshgrid(x, y)\n",
    "      ax.plot_surface(X[50:120,190:240], Y[50:120,190:240], img[190:240,50:120].T, rstride=1, cstride=1,cmap='viridis', edgecolor='none')\n",
    "      cir = Circle([drops_m[i][1],drops_m[i][0]], radius =drops_m[i][2], fill = False, color = 'red')\n",
    "      ax.add_patch(cir)\n",
    "      art3d.pathpatch_2d_to_3d(cir, z=200)\n",
    "      plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#directional_displacement\n",
    "x = np.array(dist_center)\n",
    "y = np.array(directional_displacement)\n",
    "xy = np.vstack([dist_center,directional_displacement])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "idx = z.argsort()\n",
    "x, y, z = x[idx], y[idx], z[idx]\n",
    "\n",
    "\n",
    "# angles = np.array(angles)\n",
    "\n",
    "# def rt_to_xy(r,theta):\n",
    "#     y = r*np.sin(theta)\n",
    "#     x = r*np.cos(theta)\n",
    "#     return np.array([x,y])\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot()\n",
    "# ax.scatter(*rt_to_xy(x,angles),c = long_axis_angle,s = 0.1)\n",
    "# cir = plt.Circle( (0,0) ,1,fill = False )\n",
    "# ax.plot(0,0,'bo',markersize = 2)\n",
    "# plt.colorbar()\n",
    "# ax.add_artist(cir)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import binned_statistic_2d\n",
    "a,b,c,d = binned_statistic_2d(x,y,None,'count',bins = 50, expand_binnumbers = True)\n",
    "\n",
    "weights2 = np.ones_like(y[(d[0]==5) | (d[0]==6) | (d[0]==7) | (d[0]==8)]) / (len(y[(d[0]==5) | (d[0]==6) | (d[0]==7) | (d[0]==8)]))\n",
    "weights1 = np.ones_like(y[(d[0]==9) | (d[0]==10) | (d[0]==11) | (d[0]==12)]) / (len(y[(d[0]==9) | (d[0]==10) | (d[0]==11) | (d[0]==12)]))\n",
    "plt.hist(y[(d[0]==9) | (d[0]==10) | (d[0]==11) | (d[0]==12)],alpha = 0.3,label = \"Boundary\",weights=weights1)\n",
    "plt.hist(y[(d[0]==5) | (d[0]==6) | (d[0]==7) | (d[0]==8)],alpha = 0.3,label = \"Droplet Phase\",weights=weights2)\n",
    "plt.xlabel(\"Directional Displacements (um)\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.hist(abs(y[(d[0]==9) | (d[0]==10) | (d[0]==11) | (d[0]==12)]),alpha = 0.3,label = \"Boundary\",weights=weights1)\n",
    "plt.hist(abs(y[(d[0]==5) | (d[0]==6) | (d[0]==7) | (d[0]==8)]),alpha = 0.3,label = \"Droplet Phase\",weights=weights2)\n",
    "plt.show()\n",
    "\n",
    "weights1 = np.ones_like(y[(d[0]==2) | (d[0]==3) | (d[0]==4)]) / (len(y[(d[0]==2) | (d[0]==3) | (d[0]==4)]))\n",
    "weights2 = np.ones_like(y[(d[0]==5) | (d[0]==6) | (d[0]==7)]) / (len(y[(d[0]==5) | (d[0]==6) | (d[0]==7)]))\n",
    "plt.hist(y[(d[0]==2) | (d[0]==3) | (d[0]==4)],alpha = 0.3,label = \"Boundary\",weights=weights1)\n",
    "plt.hist(y[(d[0]==5) | (d[0]==6) | (d[0]==7)],alpha = 0.3,label = \"Droplet Phase\",weights=weights2)\n",
    "plt.xlabel(\"Directional Displacements (um)\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.hist(abs(y[(d[0]==9) | (d[0]==10) | (d[0]==11) | (d[0]==12)]),alpha = 0.3,label = \"Boundary\",weights=weights1)\n",
    "# plt.hist(abs(y[(d[0]==5) | (d[0]==6) | (d[0]==7) | (d[0]==8)]),alpha = 0.3,label = \"Droplet Phase\",weights=weights2)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "weights1 = np.ones_like(y[(d[0]==16) | (d[0]==17) | (d[0]==18)]) / (len(y[(d[0]==16) | (d[0]==17) | (d[0]==18)]))\n",
    "weights2 = np.ones_like(y[(d[0]==19) | (d[0]==20) | (d[0]==21)]) / (len(y[(d[0]==19) | (d[0]==20) | (d[0]==21)]))\n",
    "plt.hist(y[(d[0]==16) | (d[0]==17) | (d[0]==18)],alpha = 0.3,label = \"Outside Boundary\",weights=weights1)\n",
    "plt.hist(y[(d[0]==19) | (d[0]==20) | (d[0]==21)],alpha = 0.3,label = \"Non-Droplet/Boundary Phase\",weights=weights2)\n",
    "plt.xlabel(\"Directional Displacements (um)\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "weights3 = np.ones_like(y[(d[0]==1)]) / (len(y[(d[0]==1)]))\n",
    "plt.hist(y[(d[0]==1)],alpha = 1,label = \"Center of Condensate\",weights=weights3)\n",
    "plt.xlabel(\"Directional Displacements (um)\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n, _ = np.histogram(x,bins = 20)\n",
    "sy, _ = np.histogram(x,bins = 20,weights = y)\n",
    "sy2, _ = np.histogram(x,bins = 20,weights = y*y)\n",
    "h, x_bins, y_bins = np.histogram2d(x,y,bins = 20)\n",
    "\n",
    "mean = sy/n\n",
    "std = np.sqrt(sy2/n - mean*mean)\n",
    "plt.scatter(x,y,c = z, s = 50)\n",
    "plt.plot((_[1:] + _[:-1])/2,mean, 'r-')\n",
    "#plt.plot((_[1:] + _[:-1])/2,np.sum(h.T,axis = 1)/(np.sum(np.sum(h.T,axis = 1))))\n",
    "#plt.axvline(x=2.5*0.globals[pixel_size],linestyle = 'dashed')\n",
    "plt.errorbar((_[1:] + _[:-1])/2, mean,yerr = std/np.sqrt(len(mean)),fmt = 'r-')\n",
    "plt.xlabel(\"Distance of Localization to Boundary (um)\")\n",
    "plt.ylabel(\"Displacements (um)\")\n",
    "#plt.ylabel(\"Dapp (um^2/s)\")\n",
    "# plt.ylim((-0.2,1.25))\n",
    "# plt.xlim((-0.35,1))\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "n, _ = np.histogram(x,bins = 20)\n",
    "sy, _ = np.histogram(x,bins = 20,weights = y)\n",
    "sy2, _ = np.histogram(x,bins = 20,weights = y*y)\n",
    "h, x_bins, y_bins = np.histogram2d(x,y,bins = 20)\n",
    "\n",
    "mean = sy/n\n",
    "std = np.sqrt(sy2/n - mean*mean)\n",
    "plt.scatter(x,y,c = long_axis_angle, s = 50)\n",
    "plt.plot((_[1:] + _[:-1])/2,mean, 'r-')\n",
    "#plt.plot((_[1:] + _[:-1])/2,np.sum(h.T,axis = 1)/(np.sum(np.sum(h.T,axis = 1))))\n",
    "#plt.axvline(x=2.5*0.globals[pixel_size],linestyle = 'dashed')\n",
    "plt.errorbar((_[1:] + _[:-1])/2, mean,yerr = std/np.sqrt(len(mean)),fmt = 'r-')\n",
    "plt.xlabel(\"Distance of Localization to Boundary (um)\")\n",
    "plt.ylabel(\"Displacements (um)\")\n",
    "#plt.ylabel(\"Dapp (um^2/s)\")\n",
    "# plt.ylim((-0.2,1.25))\n",
    "# plt.xlim((-0.35,1))\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#IO_Crossing inside\n",
    "x = np.array(IO_inside_start_dist)\n",
    "y = np.array(IO_inside_start)\n",
    "xy = np.vstack([IO_inside_start_dist,IO_inside_start])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "idx = z.argsort()\n",
    "x, y, z = x[idx], y[idx], z[idx]\n",
    "\n",
    "\n",
    "\n",
    "n, _ = np.histogram(x,bins = 20)\n",
    "sy, _ = np.histogram(x,bins = 20,weights = y)\n",
    "sy2, _ = np.histogram(x,bins = 20,weights = y*y)\n",
    "h, x_bins, y_bins = np.histogram2d(x,y,bins = 20)\n",
    "\n",
    "mean = sy/n\n",
    "std = np.sqrt(sy2/n - mean*mean)\n",
    "plt.scatter(x,y,c = z, s = 50)\n",
    "plt.plot((_[1:] + _[:-1])/2,mean, 'r-')\n",
    "#plt.plot((_[1:] + _[:-1])/2,np.sum(h.T,axis = 1)/(np.sum(np.sum(h.T,axis = 1))))\n",
    "#plt.axvline(x=2.5*0.globals[pixel_size],linestyle = 'dashed')\n",
    "plt.errorbar((_[1:] + _[:-1])/2, mean,yerr = std/np.sqrt(len(mean)),fmt = 'r-')\n",
    "plt.xlabel(\"Distance of Localization to Boundary (um)\")\n",
    "plt.ylabel(\"Displacements (um)\")\n",
    "#plt.ylabel(\"Dapp (um^2/s)\")\n",
    "# plt.ylim((-0.2,1.25))\n",
    "# plt.xlim((-0.35,1))\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#IO_Crossing outside\n",
    "x = np.array(IO_outside_start_dist)\n",
    "y = np.array(IO_outside_start)\n",
    "xy = np.vstack([IO_outside_start_dist,IO_outside_start])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "idx = z.argsort()\n",
    "x, y, z = x[idx], y[idx], z[idx]\n",
    "\n",
    "\n",
    "\n",
    "n, _ = np.histogram(x,bins = 20)\n",
    "sy, _ = np.histogram(x,bins = 20,weights = y)\n",
    "sy2, _ = np.histogram(x,bins = 20,weights = y*y)\n",
    "h, x_bins, y_bins = np.histogram2d(x,y,bins = 20)\n",
    "\n",
    "mean = sy/n\n",
    "std = np.sqrt(sy2/n - mean*mean)\n",
    "plt.scatter(x,y,c = z, s = 50)\n",
    "plt.plot((_[1:] + _[:-1])/2,mean, 'r-')\n",
    "#plt.plot((_[1:] + _[:-1])/2,np.sum(h.T,axis = 1)/(np.sum(np.sum(h.T,axis = 1))))\n",
    "#plt.axvline(x=2.5*0.globals[pixel_size],linestyle = 'dashed')\n",
    "plt.errorbar((_[1:] + _[:-1])/2, mean,yerr = std/np.sqrt(len(mean)),fmt = 'r-')\n",
    "plt.xlabel(\"Distance of Localization to Boundary (um)\")\n",
    "plt.ylabel(\"Displacements (um)\")\n",
    "#plt.ylabel(\"Dapp (um^2/s)\")\n",
    "# plt.ylim((-0.2,1.25))\n",
    "# plt.xlim((-0.35,1))\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ID = '2'\n",
    "cell_ID = '0'\n",
    "total_sorted_tracks = 0\n",
    "for i in range(len(rp_ez.Movie[movie_ID].Cells[cell_ID].sorted_tracks_frame[0])):\n",
    "    total_sorted_tracks += len(rp_ez.Movie[movie_ID].Cells[cell_ID].sorted_tracks_frame[0][i])\n",
    "total_sorted_tracks\n",
    "total_seg = 0\n",
    "in_tracks = 0\n",
    "out_tracks = 0\n",
    "io_tracks = 0\n",
    "for i,j in rp_ez.Movie[movie_ID].Cells[cell_ID].Trajectory_Collection.items():\n",
    "    total_seg += len(j.IN_Trajectory_Collection)\n",
    "    in_tracks += len(j.IN_Trajectory_Collection)\n",
    "    total_seg += len(j.IO_Trajectory_Collection)\n",
    "    io_tracks += len(j.IO_Trajectory_Collection)\n",
    "    total_seg += len(j.OUT_Trajectory_Collection)\n",
    "    out_tracks += len(j.OUT_Trajectory_Collection)\n",
    "a = len(rp_ez.Movie[movie_ID].Cells[cell_ID].No_Drops_Trajectory_Collection)\n",
    "print(a)\n",
    "print(total_seg)\n",
    "print(total_sorted_tracks)\n",
    "print(total_seg+a)\n",
    "\n",
    "print(in_tracks,io_tracks,out_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7cac227c059137e80483b20a8726c6d0152843f1c5edcbf7ca06b975e08a95d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
